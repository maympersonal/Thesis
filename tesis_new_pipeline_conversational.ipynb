{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tahoTv9x9QVz",
        "outputId": "43529b22-5ed8-4f38-f95a-d5c68c1cf008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Ejecuta la funci√≥n de montaje\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9JAaMnUyMZr",
        "outputId": "d7890052-57c3-491f-c57b-4006004080bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "liboctave-dev is already the newest version (6.4.0-2).\n",
            "octave is already the newest version (6.4.0-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Octave 6.4.0\n"
          ]
        }
      ],
      "source": [
        "# === Sistema (Octave) ===\n",
        "!sudo DEBIAN_FRONTEND=noninteractive apt-get update -yq\n",
        "!sudo DEBIAN_FRONTEND=noninteractive apt-get install -yq octave liboctave-dev\n",
        "\n",
        "# Verificar Octave:\n",
        "!octave --quiet --eval \"printf('Octave %s\\n', version());\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqGtHdwEySjE"
      },
      "outputs": [],
      "source": [
        "# === Python (libs) ===\n",
        "# Sugerencias:\n",
        "# - --quiet para menos ruido\n",
        "# - hf-transfer acelera descargas de modelos\n",
        "# - 'transformers' y 'accelerate' ayudan con flujos HF/vLLM y utilidades\n",
        "# - 'sentencepiece' para tokenizadores (Qwen, Llama, etc.)\n",
        "# - 'pandas' √∫til para CSVs de resultados; 'openpyxl' para Excel\n",
        "\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q datasets huggingface_hub transformers accelerate \\\n",
        "  sentencepiece pandas openpyxl hf-transfer \\\n",
        "  vllm math-verify\n",
        "\n",
        "# Activar transferencia acelerada en HF (m√°s r√°pido al bajar modelos)\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLZxZECTBZaX",
        "outputId": "d2e3377b-ed2f-478f-b1fc-e93d04145945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus[milvus-lite] in /usr/local/lib/python3.12/dist-packages (2.6.3)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (79.0.1)\n",
            "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (1.76.0)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (3.11.4)\n",
            "Requirement already satisfied: protobuf>=5.27.2 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (5.29.5)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (1.2.1)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (2.5.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus[milvus-lite]) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from milvus-lite>=2.4.0->pymilvus[milvus-lite]) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus[milvus-lite]) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymilvus[milvus-lite]\n",
        "!pip -q install tqdm\n",
        "!pip -q install math-verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpDNtDp6yWQ7",
        "outputId": "7dfdc14c-094a-41fc-eac7-0bda719df52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "HF_HUB_ENABLE_HF_TRANSFER: None\n",
            "GPU(s):\n",
            " NVIDIA L4, 23034 MiB, 550.54.15\n",
            "OK import datasets\n",
            "OK import huggingface_hub\n",
            "OK import transformers\n",
            "OK import accelerate\n",
            "OK import vllm\n",
            "OK import math_verify\n",
            "OK import openpyxl\n",
            "OK import pandas\n",
            "vLLM: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "# === Chequeos r√°pidos de entorno ===\n",
        "\n",
        "import os, sys, subprocess\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"HF_HUB_ENABLE_HF_TRANSFER:\", os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\"))\n",
        "\n",
        "# GPU y CUDA visibles\n",
        "try:\n",
        "    out = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"])\n",
        "    print(\"GPU(s):\\n\", out.decode().strip())\n",
        "except Exception as e:\n",
        "    print(\"nvidia-smi no disponible:\", e)\n",
        "\n",
        "# Importes cr√≠ticos\n",
        "for pkg in [\"datasets\",\"huggingface_hub\",\"transformers\",\"accelerate\",\"vllm\",\"math_verify\",\"openpyxl\",\"pandas\"]:\n",
        "    try:\n",
        "        __import__(pkg.replace(\"-\", \"_\"))\n",
        "        print(f\"OK import {pkg}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Fallo import {pkg} -> {e}\")\n",
        "\n",
        "# vLLM sanity (no lanza servidor, solo import y versi√≥n)\n",
        "try:\n",
        "    import vllm\n",
        "    print(\"vLLM:\", getattr(vllm, \"__version__\", \"unknown\"))\n",
        "except Exception as e:\n",
        "    print(\"Error vLLM:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2W8-DjSTGzb",
        "outputId": "4a068d85-2da5-4607-b03f-78b09437bd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/octave\n",
            "octave: X11 DISPLAY environment variable not set\n",
            "octave: disabling GUI features\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "!which octave\n",
        "!octave --eval \"disp(2 + 2)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZYB9blksgln"
      },
      "source": [
        "# Definiciones de clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lUgOvCnyhCL",
        "outputId": "c541021a-476c-4966-ec0d-c084cefb3ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-02 17:09:30 [__init__.py:216] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Union\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Opcional: transformers solo para chat_template\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    _HAS_TRANSFORMERS = True\n",
        "except Exception:\n",
        "    _HAS_TRANSFORMERS = False\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "logger = logging.getLogger(\"VLLMGenerator\")\n",
        "\n",
        "\n",
        "class VLLMGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        download: bool = False,\n",
        "        local_dir: Optional[str] = None,\n",
        "        hf_token: Optional[str] = None,\n",
        "        temperature: float = 0.0,\n",
        "        top_p: float = 1.0,\n",
        "        max_tokens: int = 1024,\n",
        "        seed: int = 42,\n",
        "        log_prompts: bool = False,\n",
        "        # Ajustes de vLLM / GPU\n",
        "        dtype: str = \"auto\",                    # \"auto\" | \"float16\" | \"bfloat16\" | \"float32\"\n",
        "        tensor_parallel_size: int = 1,\n",
        "        gpu_memory_utilization: float = 0.92,   # Colab L4 va bien con 0.90‚Äì0.95\n",
        "        max_model_len: Optional[int] = None,    # e.g., 8192 para limitar si hace falta\n",
        "        trust_remote_code: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Wrapper de vLLM con:\n",
        "        - Descarga opcional desde HF.\n",
        "        - Chat template autom√°tico si el tokenizador lo soporta.\n",
        "        - Par√°metros de muestreo y carga configurables.\n",
        "        \"\"\"\n",
        "        self.repo_or_path = model_name\n",
        "        self.log_prompts = log_prompts\n",
        "        self._tokenizer = None\n",
        "        self._use_chat_template = False\n",
        "        self._eos_token = None\n",
        "        self._stop_tokens = None\n",
        "\n",
        "        # Descarga opcional del modelo (√∫til para repetir pruebas sin red/latencia)\n",
        "        if download:\n",
        "            folder = local_dir or model_name.split(\"/\")[-1]\n",
        "            logger.info(\"Descargando modelo %s a ./%s ...\", model_name, folder)\n",
        "            self.repo_or_path = snapshot_download(\n",
        "                repo_id=model_name,\n",
        "                local_dir=folder,\n",
        "                local_dir_use_symlinks=False,  # evita symlinks problem√°ticos en Colab/Drive\n",
        "                token=hf_token\n",
        "            )\n",
        "            logger.info(\"Modelo descargado en: %s\", self.repo_or_path)\n",
        "\n",
        "        # Cargar tokenizer para chat templates (si est√° disponible)\n",
        "        if _HAS_TRANSFORMERS:\n",
        "            try:\n",
        "                self._tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    self.repo_or_path,\n",
        "                    use_fast=True,\n",
        "                    token=hf_token,\n",
        "                    trust_remote_code=trust_remote_code\n",
        "                )\n",
        "                # Detectar si el tokenizer tiene plantilla de chat\n",
        "                if hasattr(self._tokenizer, \"apply_chat_template\"):\n",
        "                    self._use_chat_template = True\n",
        "                    logger.info(\"Chat template detectado para este modelo.\")\n",
        "                # eos/stop\n",
        "                self._eos_token = getattr(self._tokenizer, \"eos_token\", None)\n",
        "            except Exception as e:\n",
        "                logger.warning(\"No se pudo cargar tokenizer (%s). Se usar√° formateo fallback.\", e)\n",
        "        else:\n",
        "            logger.info(\"transformers no disponible; usando formateo fallback.\")\n",
        "\n",
        "        # Inicializar LLM de vLLM\n",
        "        load_start = time.time()\n",
        "        self.llm = LLM(\n",
        "            model=self.repo_or_path,\n",
        "            dtype=dtype,\n",
        "            tensor_parallel_size=tensor_parallel_size,\n",
        "            gpu_memory_utilization=gpu_memory_utilization,\n",
        "            max_model_len=max_model_len,\n",
        "            trust_remote_code=trust_remote_code,\n",
        "            # se puede a√±adir: enforce_eager=True para depuraci√≥n (m√°s lento)\n",
        "        )\n",
        "        logger.info(\"Modelo cargado en %.2f s\", time.time() - load_start)\n",
        "\n",
        "        # Sampling por defecto (sobreescribir al llamar a generate)\n",
        "        stops = []\n",
        "        if self._eos_token:\n",
        "            stops.append(self._eos_token)\n",
        "        # Algunos modelos usan tokens especiales de fin:\n",
        "        # Qwen2: \"<|im_end|>\", Mistral/LLaMA: \"</s>\", etc.\n",
        "        # a√±adirlos expl√≠citamente:\n",
        "        # if \"qwen\" in model_name.lower(): stops.append(\"<|im_end|>\")\n",
        "\n",
        "        self.sampling_params = SamplingParams(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_tokens=max_tokens,\n",
        "            seed=seed,\n",
        "            stop=stops or None\n",
        "        )\n",
        "        logger.info(\"Sampling por defecto: %s\", self.sampling_params)\n",
        "\n",
        "    # ---------- Helpers de formateo ----------\n",
        "    def _format_messages(self, messages: List[Dict[str, str]]) -> str:\n",
        "        \"\"\"\n",
        "        Convierte una lista de mensajes [{'role': 'system'|'user'|'assistant', 'content': str}, ...]\n",
        "        a un prompt seg√∫n chat_template si existe; si no, usa el fallback original.\n",
        "        \"\"\"\n",
        "        # Normalizar roles\n",
        "        norm = []\n",
        "        for m in messages:\n",
        "            role = m[\"role\"].strip().lower()\n",
        "            if role not in {\"system\", \"user\", \"assistant\"}:\n",
        "                role = \"user\"\n",
        "            norm.append({\"role\": role, \"content\": m[\"content\"].strip()})\n",
        "\n",
        "        if self._use_chat_template:\n",
        "            try:\n",
        "                # add_generation_prompt=True agrega el turno del assistant al final\n",
        "                prompt = self._tokenizer.apply_chat_template(\n",
        "                    norm,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=True\n",
        "                )\n",
        "                return prompt\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Fallo apply_chat_template (%s). Usando fallback.\", e)\n",
        "\n",
        "        # Fallback (formato original)\n",
        "        prompt = []\n",
        "        for m in norm:\n",
        "            if m[\"role\"] == \"system\":\n",
        "                prompt.append(\"<|system|>\\n\" + m[\"content\"] + \"\\n\")\n",
        "            elif m[\"role\"] == \"user\":\n",
        "                prompt.append(\"<|user|>\\n\" + m[\"content\"] + \"\\n\")\n",
        "            else:\n",
        "                prompt.append(\"<|assistant|>\\n\" + m[\"content\"] + \"\\n\")\n",
        "        prompt.append(\"<|assistant|>\\n\")\n",
        "        return \"\".join(prompt)\n",
        "\n",
        "    # ---------- API p√∫blica ----------\n",
        "    def generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        sampling: Optional[SamplingParams] = None\n",
        "    ) -> List[str]:\n",
        "        sp = sampling or self.sampling_params\n",
        "        start = time.time()\n",
        "        outputs = self.llm.generate(prompts, sp)\n",
        "        logger.info(\"Generaci√≥n completada en %.2f s\", time.time() - start)\n",
        "\n",
        "        texts = []\n",
        "        for out in outputs:\n",
        "            if out.outputs:\n",
        "                texts.append(out.outputs[0].text.strip())\n",
        "            else:\n",
        "                texts.append(\"\")\n",
        "        return texts\n",
        "\n",
        "    def chat(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        debug: Optional[bool] = None,\n",
        "        sampling: Optional[SamplingParams] = None\n",
        "    ) -> str:\n",
        "        prompt = self._format_messages(messages)\n",
        "        should_log = self.log_prompts if debug is None else debug\n",
        "        if should_log:\n",
        "            print(\"üì§ Prompt enviado al modelo\\n\" + \"-\"*40 + f\"\\n{prompt}\\n\" + \"-\"*40)\n",
        "        resp = self.generate([prompt], sampling=sampling)\n",
        "        return resp[0] if resp else \"\"\n",
        "\n",
        "    def batch_chat(\n",
        "        self,\n",
        "        conversations: List[List[Dict[str, str]]],\n",
        "        debug: bool = False,\n",
        "        sampling: Optional[SamplingParams] = None\n",
        "    ) -> List[str]:\n",
        "        prompts = [self._format_messages(msgs) for msgs in conversations]\n",
        "        if debug or self.log_prompts:\n",
        "            for i, p in enumerate(prompts, 1):\n",
        "                print(f\"\\nüì§ Prompt #{i}\\n\" + \"-\"*40 + f\"\\n{p}\\n\" + \"-\"*40)\n",
        "        results = self.generate(prompts, sampling=sampling)\n",
        "        return [r.strip() for r in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unO4FLGRDlDY"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "class ModelRegistry:\n",
        "    \"\"\"\n",
        "    Registry of supported Hugging Face models for vLLMGenerator.\n",
        "\n",
        "    Contains a fixed list of model identifiers and their metadata.\n",
        "    \"\"\"\n",
        "    # Mapping from model key to huggingface repo id\n",
        "    MODEL_REPOS: Dict[str, str] = {\n",
        "        \"llama3-8b-instruct\": \"nreHieW/Llama-3.1-8B-Instruct\",\n",
        "        \"deepseek-math-7b\": \"deepseek-ai/deepseek-math-7b-instruct\",\n",
        "        \"qwen2-7b\": \"Qwen/Qwen2-7B\",\n",
        "        \"qwen2-7b-instruct\": \"Qwen/Qwen2-7B-Instruct\",\n",
        "        \"qwen2-math-7b-instruct\": \"Qwen/Qwen2-Math-7B-Instruct\",\n",
        "        \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        \"mathstral-7b\": \"mistralai/Mathstral-7B-v0.1\",\n",
        "        \"deepseek-coder-7b\": \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n",
        "        \"mathcoder-l-7b\": \"MathLLMs/MathCoder-L-7B\",\n",
        "        \"open-reasoner-zero-7b\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-7B\",\n",
        "    }\n",
        "\n",
        "    # Additional details for each model\n",
        "    MODEL_DETAILS: Dict[str, Dict[str, str]] = {\n",
        "        \"llama3-8b-instruct\": {\n",
        "            \"description\": \"LLaMA-3 8B instruct-tuned model\",\n",
        "            \"parameters\": \"8 billion\",\n",
        "            \"context_length\": \"8192 tokens\",\n",
        "            \"suitable_for\": \"Instruction following tasks with improved reasoning\",\n",
        "        },\n",
        "        \"deepseek-math-7b\": {\n",
        "            \"description\": \"DeepSeek Math 7B instruct model\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"4096 tokens\",\n",
        "            \"suitable_for\": \"Mathematical problem solving and proofs\",\n",
        "        },\n",
        "        \"qwen2-7b\": {\n",
        "            \"description\": \"Qwen 2 7B base model\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"4096 tokens\",\n",
        "            \"suitable_for\": \"General purpose text generation\",\n",
        "        },\n",
        "        \"qwen2-7b-instruct\": {\n",
        "            \"description\": \"Qwen 2 7B instruct-tuned model\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"4096 tokens\",\n",
        "            \"suitable_for\": \"Instruction-based tasks and chat\",\n",
        "        },\n",
        "        \"qwen2-math-7b-instruct\": {\n",
        "            \"description\": \"Qwen 2 Math 7B instruct model\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"4096 tokens\",\n",
        "            \"suitable_for\": \"Mathematical reasoning and code generation\",\n",
        "        },\n",
        "        \"mistral-7b-instruct\": {\n",
        "            \"description\": \"Mistral 7B Instruct v0.3\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"8192 tokens\",\n",
        "            \"suitable_for\": \"Instruction following with long context\",\n",
        "        },\n",
        "        \"mathstral-7b\": {\n",
        "            \"description\": \"Mathstral 7B\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"8192 tokens\",\n",
        "            \"suitable_for\": \"Mathematics-specific tasks\",\n",
        "        },\n",
        "        \"deepseek-coder-7b\": {\n",
        "            \"description\": \"DeepSeek Coder 7B instruct model\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"4096 tokens\",\n",
        "            \"suitable_for\": \"Code generation and debugging\",\n",
        "        },\n",
        "        \"mathcoder-l-7b\": {\n",
        "            \"description\": \"MathCoder-L-7B: open-source 7B model tailored for mathematical reasoning & code generation\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"unknown (use safe 8192 tokens)\",\n",
        "            \"suitable_for\": \"Mathematical problem solving with code generation\"\n",
        "        },\n",
        "        \"open-reasoner-zero-7b\": {\n",
        "            \"description\": \"Open-Reasoner-Zero-7B: open-source 7B reasoning-oriented model (RL trained) for math/logic tasks\",\n",
        "            \"parameters\": \"7 billion\",\n",
        "            \"context_length\": \"safe ~8192 tokens\",\n",
        "            \"suitable_for\": \"Logical & mathematical reasoning (chain-of-thought)\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def get_supported_models(cls) -> List[str]:\n",
        "        \"\"\"Return list of supported model keys.\"\"\"\n",
        "        return list(cls.MODEL_REPOS.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_repo(cls, model_key: str) -> str:\n",
        "        \"\"\"Given a model key, return the HF repository identifier.\"\"\"\n",
        "        return cls.MODEL_REPOS[model_key]\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_details(cls, model_key: str) -> Dict[str, str]:\n",
        "        \"\"\"Return metadata for a given model key.\"\"\"\n",
        "        return cls.MODEL_DETAILS.get(model_key, {})\n",
        "\n",
        "    @classmethod\n",
        "    def validate_model_key(cls, model_key: str) -> bool:\n",
        "        \"\"\"Check if a model key is registered.\"\"\"\n",
        "        return model_key in cls.MODEL_REPOS\n",
        "\n",
        "    @classmethod\n",
        "    def as_list_of_repos(cls) -> List[str]:\n",
        "        \"\"\"Return list of HF repo strings for all supported models.\"\"\"\n",
        "        return list(cls.MODEL_REPOS.values())\n",
        "\n",
        "\n",
        "    # --- Helpers ---\n",
        "    @classmethod\n",
        "    def canonical_key(cls, model_key: str) -> str:\n",
        "        return model_key.strip().lower()\n",
        "\n",
        "    @classmethod\n",
        "    def safe_get_model_repo(cls, model_key: str) -> str:\n",
        "        key = cls.canonical_key(model_key)\n",
        "        if key in cls.MODEL_REPOS:\n",
        "            return cls.MODEL_REPOS[key]\n",
        "        candidates = difflib.get_close_matches(key, cls.MODEL_REPOS.keys(), n=3, cutoff=0.5)\n",
        "        if candidates:\n",
        "            raise KeyError(f\"Modelo '{model_key}' no registrado. ¬øQuisiste decir: {', '.join(candidates)}?\")\n",
        "        raise KeyError(f\"Modelo '{model_key}' no registrado. Usa uno de: {', '.join(cls.MODEL_REPOS.keys())}\")\n",
        "\n",
        "    @classmethod\n",
        "    def get_family(cls, model_key: str) -> str:\n",
        "        key = cls.canonical_key(model_key)\n",
        "        repo = cls.MODEL_REPOS.get(key, \"\")\n",
        "        low = f\"{key} {repo}\".lower()\n",
        "        if \"llama\" in low: return \"llama\"\n",
        "        if \"qwen\" in low: return \"qwen\"\n",
        "        if \"mistral\" in low or \"mathstral\" in low: return \"mistral\"\n",
        "        if \"deepseek\" in low: return \"deepseek\"\n",
        "        return \"generic\"\n",
        "\n",
        "    @classmethod\n",
        "    def default_stops_for(cls, model_key: str):\n",
        "        fam = cls.get_family(model_key)\n",
        "        if fam == \"qwen\": return [\"<|im_end|>\"]\n",
        "        if fam in (\"llama\", \"mistral\"): return [\"</s>\"]\n",
        "        if fam == \"deepseek\": return [\"<|EOT|>\", \"</s>\"]\n",
        "        return None\n",
        "\n",
        "    @classmethod\n",
        "    def sampling_defaults_for(cls, model_key: str):\n",
        "        fam = cls.get_family(model_key)\n",
        "        if fam in (\"llama\", \"mistral\"): return {\"temperature\": 0.2, \"top_p\": 0.9}\n",
        "        if fam in (\"qwen\", \"deepseek\"): return {\"temperature\": 0.1, \"top_p\": 0.95}\n",
        "        return {\"temperature\": 0.0, \"top_p\": 1.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVlkBrFG6WcJ"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import tempfile\n",
        "import os\n",
        "import shutil\n",
        "import signal\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "class OctaveExecutionError(Exception): ...\n",
        "class OctaveTimeoutError(Exception): ...\n",
        "\n",
        "class OctaveCodeExecutor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        timeout: int = 10,\n",
        "        octave_cmd: str = \"octave\",\n",
        "        max_output_size: int = 10000,\n",
        "        workdir: Optional[str] = None,\n",
        "        packages: Optional[List[str]] = None,\n",
        "        env: Optional[Dict[str, str]] = None,\n",
        "    ):\n",
        "        self.timeout = int(timeout)\n",
        "        self.octave_cmd = octave_cmd\n",
        "        self.max_output_size = int(max_output_size)\n",
        "        self.workdir = workdir\n",
        "        self.packages = packages or []\n",
        "        self.env = {\"OMP_NUM_THREADS\": \"1\", \"OPENBLAS_NUM_THREADS\": \"1\", **(env or {})}\n",
        "\n",
        "        if shutil.which(self.octave_cmd) is None:\n",
        "            raise FileNotFoundError(\n",
        "                f\"El binario '{self.octave_cmd}' no se encontr√≥.\\n\"\n",
        "                f\"Instala Octave con:\\n!apt-get update && apt-get install -y octave\"\n",
        "            )\n",
        "\n",
        "    def _safe_path_for_octave(self, path: str) -> str:\n",
        "        # Octave usa '...' para strings; escapamos comillas simples dobl√°ndolas\n",
        "        return path.replace(\"'\", \"''\")\n",
        "\n",
        "    def _write_temp_file(self, code: str, dir_path: Optional[str]) -> str:\n",
        "        fd, path = tempfile.mkstemp(suffix=\".m\", dir=dir_path)\n",
        "        with os.fdopen(fd, \"w\") as f:\n",
        "            f.write(code)\n",
        "        return path\n",
        "\n",
        "    def _build_eval(self, mfile_path: str) -> str:\n",
        "        # Prelude: opciones r√°pidas y cargas de paquetes\n",
        "        prelude_lines = [\n",
        "            \"more off;\",\n",
        "            \"warning('off','all');\",\n",
        "        ]\n",
        "        for p in self.packages:\n",
        "            prelude_lines.append(f\"try; pkg load {p}; catch; end;\")\n",
        "        prelude = \" \".join(prelude_lines)\n",
        "\n",
        "        spath = self._safe_path_for_octave(mfile_path)\n",
        "        # Ejecuta y controla error con getReport; devuelve c√≥digo de salida\n",
        "        eval_str = (\n",
        "            f\"{prelude} \"\n",
        "            f\"try, run('{spath}'); exit(0); \"\n",
        "            f\"catch err, fprintf(2, '%s\\\\n', getReport(err, 'extended', 'hyperlinks', 'off')); exit(2); end;\"\n",
        "        )\n",
        "        return eval_str\n",
        "\n",
        "    def execute_with_timeout(self, code: str, keep_temp: bool = False) -> Tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"\n",
        "        API compatible:\n",
        "        - Devuelve (stdout, stderr or None)\n",
        "        - Lanza OctaveTimeoutError si se excede timeout\n",
        "        - Lanza OctaveExecutionError si Octave devuelve c√≥digo != 0\n",
        "        \"\"\"\n",
        "        # Usar workdir temporal si no se especifica\n",
        "        temp_dir = None\n",
        "        cwd = self.workdir\n",
        "        if cwd is None:\n",
        "            temp_dir = tempfile.mkdtemp(prefix=\"oct_\")\n",
        "            cwd = temp_dir\n",
        "\n",
        "        mfile_path = self._write_temp_file(code, dir_path=cwd)\n",
        "        try:\n",
        "            cmd = [\n",
        "                self.octave_cmd,\n",
        "                \"-qf\",            # quiet + no init files\n",
        "                \"--no-gui\",\n",
        "                \"--eval\",\n",
        "                self._build_eval(mfile_path),\n",
        "            ]\n",
        "\n",
        "            # Crear un grupo de procesos para poder matar hijos en timeout\n",
        "            proc = subprocess.Popen(\n",
        "                cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True,\n",
        "                cwd=cwd,\n",
        "                env={**os.environ, **self.env},\n",
        "                preexec_fn=os.setsid  # Linux/Colab: nuevo process group\n",
        "            )\n",
        "            try:\n",
        "                stdout, stderr = proc.communicate(timeout=self.timeout)\n",
        "            except subprocess.TimeoutExpired:\n",
        "                # Matar todo el grupo\n",
        "                try:\n",
        "                    os.killpg(proc.pid, signal.SIGKILL)\n",
        "                except Exception:\n",
        "                    proc.kill()\n",
        "                raise OctaveTimeoutError(\"Tiempo de ejecuci√≥n excedido para el c√≥digo Octave.\")\n",
        "\n",
        "            # Truncar salidas\n",
        "            stdout = (stdout or \"\")[:self.max_output_size].strip()\n",
        "            stderr = (stderr or \"\").strip()\n",
        "\n",
        "            if proc.returncode != 0:\n",
        "                # Si hubo error, levanta excepci√≥n con el reporte extendido\n",
        "                raise OctaveExecutionError(stderr or \"Ejecuci√≥n de Octave fallida (sin stderr).\")\n",
        "\n",
        "            return stdout, (stderr if stderr else None)\n",
        "\n",
        "        finally:\n",
        "            # Limpieza\n",
        "            if not keep_temp:\n",
        "                try:\n",
        "                    if os.path.exists(mfile_path):\n",
        "                        os.remove(mfile_path)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                if temp_dir and os.path.isdir(temp_dir):\n",
        "                    shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "    # M√©todo extra (opcional): devuelve metadatos completos\n",
        "    def execute(self, code: str, keep_temp: bool = False) -> Dict[str, Optional[str]]:\n",
        "        try:\n",
        "            out, err = self.execute_with_timeout(code, keep_temp=keep_temp)\n",
        "            return {\"stdout\": out, \"stderr\": err, \"returncode\": 0, \"timed_out\": False}\n",
        "        except OctaveTimeoutError as e:\n",
        "            return {\"stdout\": None, \"stderr\": str(e), \"returncode\": None, \"timed_out\": True}\n",
        "        except OctaveExecutionError as e:\n",
        "            return {\"stdout\": None, \"stderr\": str(e), \"returncode\": 2, \"timed_out\": False}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Eu6fZYY7suY"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pymilvus import MilvusClient\n",
        "\n",
        "class MilvusRetriever:\n",
        "    def __init__(self, df, encoder, db_path, collection_name, rebuild: bool = False):\n",
        "        \"\"\"\n",
        "        df: DataFrame con columnas: question, octave_code, embedding (string \"[f1 f2 ...]\")\n",
        "        encoder: SentenceTransformer (para consultas)\n",
        "        db_path: ruta Milvus Lite, e.g. '/content/.../milvus.db'\n",
        "        collection_name: nombre de la colecci√≥n\n",
        "        rebuild: si True, fuerza reindexado desde cero\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.encoder = encoder\n",
        "        self.client = MilvusClient(uri=db_path)\n",
        "        self.collection_name = collection_name\n",
        "        self.dimension = 384  # debe coincidir con los embeddings\n",
        "\n",
        "        # Crear colecci√≥n si no existe\n",
        "        if self.collection_name not in self.client.list_collections():\n",
        "            self.client.create_collection(\n",
        "                self.collection_name,\n",
        "                dimension=self.dimension,\n",
        "                consistency_level=\"Eventually\",\n",
        "                auto_id=True\n",
        "            )\n",
        "            self._index_data()\n",
        "        else:\n",
        "            # Si existe, decidir si reindexar o no\n",
        "            if rebuild or self._row_count() == 0:\n",
        "                try:\n",
        "                    self.client.drop_collection(self.collection_name)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                self.client.create_collection(\n",
        "                    self.collection_name,\n",
        "                    dimension=self.dimension,\n",
        "                    consistency_level=\"Eventually\",\n",
        "                    auto_id=True\n",
        "                )\n",
        "                self._index_data()\n",
        "            else:\n",
        "                print(f\"Milvus collection '{self.collection_name}' ya existe con datos.\")\n",
        "\n",
        "    def _row_count(self) -> int:\n",
        "        \"\"\"\n",
        "        Devuelve el n√∫mero de entidades en la colecci√≥n usando APIs disponibles.\n",
        "        \"\"\"\n",
        "        # 1) Intentar get_collection_stats (MilvusClient simple API)\n",
        "        try:\n",
        "            stats = self.client.get_collection_stats(self.collection_name)\n",
        "            # row_count puede venir como str\n",
        "            return int(stats.get(\"row_count\", 0))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # 2) Intentar describe_collection (algunas versiones)\n",
        "        try:\n",
        "            info = self.client.describe_collection(self.collection_name)\n",
        "            # distintos nombres posibles\n",
        "            for key in (\"row_count\", \"num_entities\", \"count\"):\n",
        "                if key in info:\n",
        "                    return int(info[key])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # 3) Fallback: hacer un search con vector dummy y estimar si hay algo\n",
        "        try:\n",
        "            import numpy as np\n",
        "            dummy = np.zeros((1, self.dimension), dtype=np.float32)\n",
        "            res = self.client.search(\n",
        "                self.collection_name,\n",
        "                data=dummy,\n",
        "                output_fields=[\"question\"],\n",
        "                limit=1,\n",
        "                consistency_level=\"Eventually\"\n",
        "            )\n",
        "            # si devuelve hits sin error, asumimos > 0\n",
        "            return 1 if (isinstance(res, list) and len(res) > 0 and len(res[0]) > 0) else 0\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    def _index_data(self):\n",
        "        print(\"üì• Indexando vectores en Milvus...\")\n",
        "        import numpy as np\n",
        "        vectors = np.stack(\n",
        "            self.df[\"embedding\"].apply(lambda x: np.fromstring(str(x).strip(\"[]\"), sep=\" \"))\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        # Normaliza si √≠ndice/consulta lo requiere (opcional)\n",
        "        # from numpy.linalg import norm\n",
        "        # vectors = (vectors / (norm(vectors, axis=1, keepdims=True) + 1e-12)).astype(np.float32)\n",
        "\n",
        "        entities = [\n",
        "            {\"question\": row[\"question\"], \"octave_code\": row[\"octave_code\"], \"vector\": vec}\n",
        "            for row, vec in zip(self.df.to_dict(orient=\"records\"), vectors)\n",
        "        ]\n",
        "        self.client.insert(self.collection_name, data=entities, progress_bar=True)\n",
        "        print(\"Indexaci√≥n completada.\")\n",
        "\n",
        "    def retrieve(self, query, top_k=3):\n",
        "        import torch\n",
        "        import torch.nn.functional as F\n",
        "        import numpy as np\n",
        "\n",
        "        q_emb = self.encoder.encode([query], convert_to_numpy=True)\n",
        "        q_emb = F.normalize(torch.tensor(q_emb), p=2, dim=1).numpy().astype(np.float32)\n",
        "\n",
        "        results = self.client.search(\n",
        "            self.collection_name,\n",
        "            data=q_emb,\n",
        "            output_fields=[\"question\", \"octave_code\"],\n",
        "            limit=top_k + 1,\n",
        "            consistency_level=\"Eventually\"\n",
        "        )\n",
        "\n",
        "        seen = set()\n",
        "        retrieved = []\n",
        "        for hit in results[0]:\n",
        "            q_text = hit.entity[\"question\"].strip()\n",
        "            if q_text == query.strip():\n",
        "                continue\n",
        "            if q_text in seen:\n",
        "                continue\n",
        "            seen.add(q_text)\n",
        "            retrieved.append(f\"Problem: {q_text}\\nCode: {hit.entity['octave_code']}\")\n",
        "            if len(retrieved) == top_k:\n",
        "                break\n",
        "\n",
        "        return \"\\n\".join(retrieved)\n",
        "\n",
        "\n",
        "    def list_all_problems(self) -> List[str]:\n",
        "        return self.df[\"question\"].drop_duplicates().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDpA_YrQ-_WK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "class FewShotRetriever:\n",
        "    def __init__(self, csv_path: str, seed: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        Inicializa el retriever cargando el .csv y clasificando ejemplos por problem_type.\n",
        "\n",
        "        Requiere columnas: 'question', 'octave_code', 'problem_type'.\n",
        "        \"\"\"\n",
        "        self.examples_by_type: Dict[str, List[Tuple[str, str]]] = defaultdict(list)\n",
        "        self.seed = seed\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Verificaci√≥n b√°sica de columnas\n",
        "        required_cols = {\"question\", \"octave_code\", \"problem_type\"}\n",
        "        missing = required_cols - set(df.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Faltan columnas en el CSV: {', '.join(sorted(missing))}\")\n",
        "\n",
        "        # Limpieza y carga\n",
        "        df = df.fillna({\"question\": \"\", \"octave_code\": \"\", \"problem_type\": \"Unknown\"})\n",
        "        for _, row in df.iterrows():\n",
        "            problem_type = str(row[\"problem_type\"]).strip() or \"Unknown\"\n",
        "            question = str(row[\"question\"]).strip()\n",
        "            code = str(row[\"octave_code\"]).strip()\n",
        "            if question and code:\n",
        "                self.examples_by_type[problem_type].append((question, code))\n",
        "\n",
        "        total_types = len(self.examples_by_type)\n",
        "        total_items = sum(len(v) for v in self.examples_by_type.values())\n",
        "        print(f\"Indexados {total_items} ejemplos en {total_types} categor√≠as (problem_type).\")\n",
        "\n",
        "    def retrieve(self, problem_type: str, k: int = 3, exclude_question: Optional[str] = None) -> str:\n",
        "        \"\"\"\n",
        "        Recupera k ejemplos del mismo 'problem_type' y devuelve un string con bloques \"Problem/Code\".\n",
        "        Si 'exclude_question' se provee, evita devolver exactamente ese enunciado (√∫til si el ejemplo actual viene del mismo CSV).\n",
        "        \"\"\"\n",
        "        candidates = self.examples_by_type.get(problem_type, [])\n",
        "        if not candidates:\n",
        "            return \"\"\n",
        "\n",
        "        pool = candidates\n",
        "        if exclude_question:\n",
        "            eq = exclude_question.strip()\n",
        "            pool = [(q, c) for (q, c) in candidates if q.strip() != eq]\n",
        "\n",
        "        if not pool:\n",
        "            return \"\"\n",
        "\n",
        "        sampled = random.sample(pool, min(k, len(pool)))\n",
        "        retrieved = [f\"Problem: {q}\\nCode: {c}\" for q, c in sampled]\n",
        "        return \"\\n\".join(retrieved)\n",
        "\n",
        "    # Helpers opcionales\n",
        "    def categories(self) -> List[str]:\n",
        "        \"\"\"Lista de problem_type disponibles.\"\"\"\n",
        "        return sorted(self.examples_by_type.keys())\n",
        "\n",
        "    def stats(self) -> pd.DataFrame:\n",
        "        \"\"\"Tabla con conteo por problem_type.\"\"\"\n",
        "        return pd.DataFrame(\n",
        "            [(t, len(v)) for t, v in self.examples_by_type.items()],\n",
        "            columns=[\"problem_type\", \"count\"]\n",
        "        ).sort_values(\"count\", ascending=False).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGH8ojMjDS-I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pickle\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# ==================== Modelo ====================\n",
        "\n",
        "class BertMultiTaskClassifier(nn.Module):\n",
        "    def __init__(self, dropout: float, num_problem_types: int, num_question_types: int):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "        # Congelar encoder (como ten√≠as)\n",
        "        for p in self.bert.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "        hidden = self.bert.config.hidden_size  # 768 para bert-base\n",
        "        self.classifier_problem = nn.Linear(hidden, num_problem_types)\n",
        "        self.classifier_question = nn.Linear(hidden, num_question_types)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # return_dict=True para nombres claros; fallback a pooler o CLS\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        if outputs.pooler_output is not None:\n",
        "            rep = outputs.pooler_output\n",
        "        else:\n",
        "            rep = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
        "        x = self.dropout(self.activation(rep))\n",
        "        logits_problem = self.classifier_problem(x)\n",
        "        logits_question = self.classifier_question(x)\n",
        "        return logits_problem, logits_question\n",
        "\n",
        "# ==================== Preprocesamiento ====================\n",
        "\n",
        "def preprocess_function(example: Dict[str, str], tokenizer, seq_len: int) -> Dict[str, Any]:\n",
        "    text = example[\"problem\"].lower()  # ‚ö†Ô∏è Mantengo lower() por consistencia con el entrenamiento\n",
        "    # Conservar operadores y puntuaci√≥n matem√°tica relevante\n",
        "    text = re.sub(r\"[^\\w\\d\\s\\+\\-\\*/=^‚àö%.,()]\", \" \", text)\n",
        "    # Normalizar repeticiones de 'x' largas\n",
        "    text = re.sub(r\"\\b[x]{2,}\\b\", \"x\", text)\n",
        "    text = re.sub(\" +\", \" \", text).strip()\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        max_length=seq_len,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"  # ya devuelve tensores\n",
        "    )\n",
        "    return tokens\n",
        "\n",
        "# ==================== Utils pickle ====================\n",
        "\n",
        "def save_file(name, obj):\n",
        "    with open(name, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def load_file(name):\n",
        "    with open(name, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# ==================== Pipeline de Inferencia ====================\n",
        "\n",
        "class MultiTaskInferencePipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        encoder_problem_path: str,\n",
        "        encoder_question_path: str,\n",
        "        dropout: float = 0.5,\n",
        "        seq_len: int = 512\n",
        "    ):\n",
        "        self.preprocess_function = preprocess_function\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # LabelEncoders\n",
        "        with open(encoder_problem_path, \"rb\") as f:\n",
        "            self.le_problem = pickle.load(f)\n",
        "        with open(encoder_question_path, \"rb\") as f:\n",
        "            self.le_question = pickle.load(f)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        # Modelo\n",
        "        self.model = BertMultiTaskClassifier(\n",
        "            dropout=dropout,\n",
        "            num_problem_types=len(self.le_problem.classes_),\n",
        "            num_question_types=len(self.le_question.classes_)\n",
        "        )\n",
        "        state = torch.load(model_path, map_location=\"cpu\")\n",
        "        self.model.load_state_dict(state, strict=True)  # si .pt viene de DataParallel, usar strict=False\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Devuelve las etiquetas predichas.\n",
        "        \"\"\"\n",
        "        tokens = self.preprocess_function({\"problem\": text}, self.tokenizer, seq_len=self.seq_len)\n",
        "        input_ids = tokens[\"input_ids\"].to(self.device)\n",
        "        attention_mask = tokens[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        logits_problem, logits_question = self.model(input_ids, attention_mask)\n",
        "        pred_problem = torch.argmax(logits_problem, dim=1).item()\n",
        "        pred_question = torch.argmax(logits_question, dim=1).item()\n",
        "\n",
        "        label_problem = self.le_problem.inverse_transform([pred_problem])[0]\n",
        "        label_question = self.le_question.inverse_transform([pred_question])[0]\n",
        "\n",
        "        return {\"problem_type\": label_problem, \"question_type\": label_question}\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict_with_probs(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Devuelve etiquetas y probabilidades (softmax).\n",
        "        √ötil para umbrales/inspecci√≥n.\n",
        "        \"\"\"\n",
        "        tokens = self.preprocess_function({\"problem\": text}, self.tokenizer, seq_len=self.seq_len)\n",
        "        input_ids = tokens[\"input_ids\"].to(self.device)\n",
        "        attention_mask = tokens[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        logits_problem, logits_question = self.model(input_ids, attention_mask)\n",
        "        probs_problem = F.softmax(logits_problem, dim=1).squeeze(0).cpu().numpy()\n",
        "        probs_question = F.softmax(logits_question, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        pred_problem = int(probs_problem.argmax())\n",
        "        pred_question = int(probs_question.argmax())\n",
        "\n",
        "        label_problem = self.le_problem.inverse_transform([pred_problem])[0]\n",
        "        label_question = self.le_question.inverse_transform([pred_question])[0]\n",
        "\n",
        "        # Mapa {label: prob} ordenado\n",
        "        pp_map = {lbl: float(probs_problem[i]) for i, lbl in enumerate(self.le_problem.classes_)}\n",
        "        pq_map = {lbl: float(probs_question[i]) for i, lbl in enumerate(self.le_question.classes_)}\n",
        "\n",
        "        return {\n",
        "            \"problem_type\": label_problem,\n",
        "            \"question_type\": label_question,\n",
        "            \"problem_type_probs\": dict(sorted(pp_map.items(), key=lambda x: x[1], reverse=True)),\n",
        "            \"question_type_probs\": dict(sorted(pq_map.items(), key=lambda x: x[1], reverse=True)),\n",
        "        }\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def batch_predict(self, texts: List[str], return_probs: bool = False, batch_size: int = 32) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Inferencia por lotes. Mucho m√°s eficiente para evaluar datasets completos.\n",
        "        \"\"\"\n",
        "        results: List[Dict[str, Any]] = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            chunk = texts[i:i + batch_size]\n",
        "            # Tokenizar en lote\n",
        "            proc = [self.preprocess_function({\"problem\": t}, self.tokenizer, seq_len=self.seq_len) for t in chunk]\n",
        "            input_ids = torch.cat([p[\"input_ids\"] for p in proc], dim=0).to(self.device)\n",
        "            attention_mask = torch.cat([p[\"attention_mask\"] for p in proc], dim=0).to(self.device)\n",
        "\n",
        "            logits_problem, logits_question = self.model(input_ids, attention_mask)\n",
        "\n",
        "            if return_probs:\n",
        "                probs_problem = F.softmax(logits_problem, dim=1).cpu()\n",
        "                probs_question = F.softmax(logits_question, dim=1).cpu()\n",
        "\n",
        "            preds_prob = torch.argmax(logits_problem, dim=1).cpu().tolist()\n",
        "            preds_ques = torch.argmax(logits_question, dim=1).cpu().tolist()\n",
        "\n",
        "            for idx, (pp, qq) in enumerate(zip(preds_prob, preds_ques)):\n",
        "                lp = self.le_problem.inverse_transform([pp])[0]\n",
        "                lq = self.le_question.inverse_transform([qq])[0]\n",
        "                item = {\"problem_type\": lp, \"question_type\": lq}\n",
        "                if return_probs:\n",
        "                    pp_map = {lbl: float(probs_problem[idx, j]) for j, lbl in enumerate(self.le_problem.classes_)}\n",
        "                    pq_map = {lbl: float(probs_question[idx, j]) for j, lbl in enumerate(self.le_question.classes_)}\n",
        "                    item[\"problem_type_probs\"] = dict(sorted(pp_map.items(), key=lambda x: x[1], reverse=True))\n",
        "                    item[\"question_type_probs\"] = dict(sorted(pq_map.items(), key=lambda x: x[1], reverse=True))\n",
        "                results.append(item)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rv1WP4VZjeU"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Dict, List, Any\n",
        "from abc import ABC, import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pickle\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# ==================== Modelo ====================\n",
        "\n",
        "class BertMultiTaskClassifier(nn.Module):\n",
        "    def __init__(self, dropout: float, num_problem_types: int, num_question_types: int):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "        # Congelar encoder (como ten√≠as)\n",
        "        for p in self.bert.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "        hidden = self.bert.config.hidden_size  # 768 para bert-base\n",
        "        self.classifier_problem = nn.Linear(hidden, num_problem_types)\n",
        "        self.classifier_question = nn.Linear(hidden, num_question_types)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # return_dict=True para nombres claros; fallback a pooler o CLS\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        if outputs.pooler_output is not None:\n",
        "            rep = outputs.pooler_output\n",
        "        else:\n",
        "            rep = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
        "        x = self.dropout(self.activation(rep))\n",
        "        logits_problem = self.classifier_problem(x)\n",
        "        logits_question = self.classifier_question(x)\n",
        "        return logits_problem, logits_question\n",
        "\n",
        "# ==================== Preprocesamiento ====================\n",
        "\n",
        "def preprocess_function(example: Dict[str, str], tokenizer, seq_len: int) -> Dict[str, Any]:\n",
        "    text = example[\"problem\"].lower()  # ‚ö†Ô∏è Mantengo lower() por consistencia con el entrenamiento\n",
        "    # Conservar operadores y puntuaci√≥n matem√°tica relevante\n",
        "    text = re.sub(r\"[^\\w\\d\\s\\+\\-\\*/=^‚àö%.,()]\", \" \", text)\n",
        "    # Normalizar repeticiones de 'x' largas\n",
        "    text = re.sub(r\"\\b[x]{2,}\\b\", \"x\", text)\n",
        "    text = re.sub(\" +\", \" \", text).strip()\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        max_length=seq_len,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"  # ya devuelve tensores\n",
        "    )\n",
        "    return tokens\n",
        "\n",
        "# ==================== Utils pickle ====================\n",
        "\n",
        "def save_file(name, obj):\n",
        "    with open(name, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def load_file(name):\n",
        "    with open(name, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# ==================== Pipeline de Inferencia ====================\n",
        "\n",
        "class MultiTaskInferencePipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        encoder_problem_path: str,\n",
        "        encoder_question_path: str,\n",
        "        dropout: float = 0.5,\n",
        "        seq_len: int = 512\n",
        "    ):\n",
        "        self.preprocess_function = preprocess_function\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # LabelEncoders\n",
        "        with open(encoder_problem_path, \"rb\") as f:\n",
        "            self.le_problem = pickle.load(f)\n",
        "        with open(encoder_question_path, \"rb\") as f:\n",
        "            self.le_question = pickle.load(f)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "        # Modelo\n",
        "        self.model = BertMultiTaskClassifier(\n",
        "            dropout=dropout,\n",
        "            num_problem_types=len(self.le_problem.classes_),\n",
        "            num_question_types=len(self.le_question.classes_)\n",
        "        )\n",
        "        state = torch.load(model_path, map_location=\"cpu\")\n",
        "        self.model.load_state_dict(state, strict=True)  # si .pt viene de DataParallel, usar strict=False\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Devuelve las etiquetas predichas.\n",
        "        \"\"\"\n",
        "        tokens = self.preprocess_function({\"problem\": text}, self.tokenizer, seq_len=self.seq_len)\n",
        "        input_ids = tokens[\"input_ids\"].to(self.device)\n",
        "        attention_mask = tokens[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        logits_problem, logits_question = self.model(input_ids, attention_mask)\n",
        "        pred_problem = torch.argmax(logits_problem, dim=1).item()\n",
        "        pred_question = torch.argmax(logits_question, dim=1).item()\n",
        "\n",
        "        label_problem = self.le_problem.inverse_transform([pred_problem])[0]\n",
        "        label_question = self.le_question.inverse_transform([pred_question])[0]\n",
        "\n",
        "        return {\"problem_type\": label_problem, \"question_type\": label_question}\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def predict_with_probs(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Devuelve etiquetas y probabilidades (softmax).\n",
        "        √ötil para umbrales/inspecci√≥n.\n",
        "        \"\"\"\n",
        "        tokens = self.preprocess_function({\"problem\": text}, self.tokenizer, seq_len=self.seq_len)\n",
        "        input_ids = tokens[\"input_ids\"].to(self.device)\n",
        "        attention_mask = tokens[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        logits_problem, logits_question = self.model(input_ids, attention_mask)\n",
        "        probs_problem = F.softmax(logits_problem, dim=1).squeeze(0).cpu().numpy()\n",
        "        probs_question = F.softmax(logits_question, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        pred_problem = int(probs_problem.argmax())\n",
        "        pred_question = int(probs_question.argmax())\n",
        "\n",
        "        label_problem = self.le_problem.inverse_transform([pred_problem])[0]\n",
        "        label_question = self.le_question.inverse_transform([pred_question])[0]\n",
        "\n",
        "        # Mapa {label: prob} ordenado\n",
        "        pp_map = {lbl: float(probs_problem[i]) for i, lbl in enumerate(self.le_problem.classes_)}\n",
        "        pq_map = {lbl: float(probs_question[i]) for i, lbl in enumerate(self.le_question.classes_)}\n",
        "\n",
        "        return {\n",
        "            \"problem_type\": label_problem,\n",
        "            \"question_type\": label_question,\n",
        "            \"problem_type_probs\": dict(sorted(pp_map.items(), key=lambda x: x[1], reverse=True)),\n",
        "            \"question_type_probs\": dict(sorted(pq_map.items(), key=lambda x: x[1], reverse=True)),\n",
        "        }\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def batch_predict(self, texts: List[str], return_probs: bool = False, batch_size: int = 32) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Inferencia por lotes. Mucho m√°s eficiente para evaluar datasets completos.\n",
        "        \"\"\"\n",
        "        results: List[Dict[str, Any]] = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            chunk = texts[i:i + batch_size]\n",
        "            # Tokenizar en lote\n",
        "            proc = [self.preprocess_function({\"problem\": t}, self.tokenizer, seq_len=self.seq_len) for t in chunk]\n",
        "            input_ids = torch.cat([p[\"input_ids\"] for p in proc], dim=0).to(self.device)\n",
        "            attention_mask = torch.cat([p[\"attention_mask\"] for p in proc], dim=0).to(self.device)\n",
        "\n",
        "            logits_problem, logits_question = self.model(input_ids, attention_mask)\n",
        "\n",
        "            if return_probs:\n",
        "                probs_problem = F.softmax(logits_problem, dim=1).cpu()\n",
        "                probs_question = F.softmax(logits_question, dim=1).cpu()\n",
        "\n",
        "            preds_prob = torch.argmax(logits_problem, dim=1).cpu().tolist()\n",
        "            preds_ques = torch.argmax(logits_question, dim=1).cpu().tolist()\n",
        "\n",
        "            for idx, (pp, qq) in enumerate(zip(preds_prob, preds_ques)):\n",
        "                lp = self.le_problem.inverse_transform([pp])[0]\n",
        "                lq = self.le_question.inverse_transform([qq])[0]\n",
        "                item = {\"problem_type\": lp, \"question_type\": lq}\n",
        "                if return_probs:\n",
        "                    pp_map = {lbl: float(probs_problem[idx, j]) for j, lbl in enumerate(self.le_problem.classes_)}\n",
        "                    pq_map = {lbl: float(probs_question[idx, j]) for j, lbl in enumerate(self.le_question.classes_)}\n",
        "                    item[\"problem_type_probs\"] = dict(sorted(pp_map.items(), key=lambda x: x[1], reverse=True))\n",
        "                    item[\"question_type_probs\"] = dict(sorted(pq_map.items(), key=lambda x: x[1], reverse=True))\n",
        "                results.append(item)\n",
        "        return results\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from pymilvus import MilvusClient\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "pattern_true = re.compile(r\"\\btrue\\b\", re.IGNORECASE)\n",
        "pattern_false = re.compile(r\"\\bfalse\\b\", re.IGNORECASE)\n",
        "\n",
        "# Instrucci√≥n fuerte para el √∫ltimo turno (evita prosa/markdown)\n",
        "# Pol√≠tica final elegida globalmente\n",
        "CODE_ONLY_MSG = (\n",
        "    \"Output ONLY a valid GNU Octave script (no prose/markdown). \"\n",
        "    \"When executed, print ONLY the final answer (numeric or single letter) using \"\n",
        "    \"printf('%.15g\\\\n', value) or printf('%c\\\\n', letter). No extra text.\"\n",
        ")\n",
        "\n",
        "def parse_resolvability(resp: str):\n",
        "    \"\"\"\n",
        "    Devuelve True/False/None seg√∫n la PRIMERA ocurrencia inequ√≠voca de 'true' o 'false'.\n",
        "    Evita casos como 'untrue' o apariciones posteriores contradictorias.\n",
        "    \"\"\"\n",
        "    resp_low = resp.lower()\n",
        "    t = pattern_true.search(resp_low)\n",
        "    f = pattern_false.search(resp_low)\n",
        "    if t and (not f or t.start() < f.start()):\n",
        "        return True\n",
        "    if f and (not t or f.start() < t.start()):\n",
        "        return False\n",
        "    return None\n",
        "\n",
        "class ConversationalPromptStrategy(ABC):\n",
        "    \"\"\"\n",
        "    Clase base abstracta para estrategias conversacionales.\n",
        "\n",
        "    Cada subclase debe implementar un flujo completo de conversaci√≥n\n",
        "    con el modelo, incluyendo resolubilidad, extracci√≥n de caracter√≠sticas\n",
        "    (si aplica) y generaci√≥n de c√≥digo.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        \"\"\"\n",
        "        Ejecuta una conversaci√≥n completa con el modelo para un problema dado.\n",
        "\n",
        "        Args:\n",
        "            row (dict): Una fila del dataset (de un CSV).\n",
        "            generator: Instancia del generador (e.g., VLLMGenerator).\n",
        "\n",
        "        Returns:\n",
        "            dict: Un diccionario con al menos:\n",
        "                - 'resolvability_response'\n",
        "                - 'is_octave_resolvable' (bool)\n",
        "                - 'problem_features' (opcional)\n",
        "                - 'model_output' (c√≥digo o razonamiento final)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class NonConversationalZeroShotStrategy(ConversationalPromptStrategy):\n",
        "    \"\"\"\n",
        "    Estrategia NO conversacional (single-shot).\n",
        "    Solo entrega el problema y exige c√≥digo GNU Octave que imprima la respuesta final.\n",
        "    \"\"\"\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        # Prompt single-shot (sin conversaci√≥n)\n",
        "        prompt = (\n",
        "            \"You are given a math problem.\\n\"\n",
        "            \"Solve it programmatically using GNU Octave and return ONLY the executable script.\\n\\n\"\n",
        "            \"Problem:\\n\"\n",
        "            f\"{question}\\n\\n\"\n",
        "            # Pol√≠tica fuerte elegida globalmente\n",
        "            f\"{CODE_ONLY_MSG}\\n\"\n",
        "        )\n",
        "\n",
        "        # Llamada single-shot al modelo\n",
        "        # Usamos generate() para evitar cualquier estructura de chat.\n",
        "        out_list = generator.generate([prompt])\n",
        "        model_output = out_list[0] if out_list else \"\"\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": None,\n",
        "            \"resolvability_response\": None,\n",
        "            \"is_octave_resolvable\": None,\n",
        "            \"problem_features\": None,\n",
        "            \"prompt\": prompt,\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "\n",
        "class NonConversationalReasonedCodeStrategy(ConversationalPromptStrategy):\n",
        "    \"\"\"\n",
        "    Estrategia NO conversacional (single-shot).\n",
        "    Induce razonamiento interno (sin que lo imprima) y exige SOLO el script GNU Octave.\n",
        "    No extrae resolubilidad ni caracter√≠sticas.\n",
        "    \"\"\"\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        # Prompt single-shot: pensar en silencio, devolver solo el script\n",
        "        prompt = (\n",
        "            \"You are given a math problem.\\n\"\n",
        "            \"Determine if the math problem can be solved programmatically using GNU Octave.Answer with 'True' or 'False'.\"\n",
        "            \"In case of 'False', try to generate the solution to the math problem in the most suitable way:\\n\"\n",
        "            \"In case of 'True', proceed as follows:\\n\"\n",
        "            \"Analyze the problem and extract its key data and the following characteristics:\\n\"\n",
        "                    \"'given_data', 'unknowns', 'constraints','applicable methods',\"\n",
        "                    \"'If a unit of measurement is used, specify which one'.\\n\"\n",
        "            \"With this information:\\n\\n\"\n",
        "            \"Problem:\\n\"\n",
        "            f\"{question}\\n\\n\"\n",
        "            f\"{CODE_ONLY_MSG}\\n\"\n",
        "        )\n",
        "\n",
        "        out_list = generator.generate([prompt])\n",
        "        model_output = out_list[0] if out_list else \"\"\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": None,\n",
        "            \"resolvability_response\": None,\n",
        "            \"is_octave_resolvable\": None,\n",
        "            \"problem_features\": None,\n",
        "            \"prompt\": prompt,\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "class ZeroShotConversationalStrategy(ConversationalPromptStrategy):\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        messages = []\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": \"OK, I've received the problem.\"})\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Determine if the math problem can be solved programmatically using GNU Octave. \"\n",
        "                \"Answer with 'True' or 'False' and justify your choice.\"\n",
        "            )\n",
        "        })\n",
        "        resolvability_response = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": resolvability_response})\n",
        "\n",
        "        is_octave_resolvable = parse_resolvability(resolvability_response)\n",
        "\n",
        "        problem_features = None\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Analyze the problem and extract its key data and the following characteristics:\\n\"\n",
        "                    \"'given_data', 'unknowns', 'constraints','applicable methods',\"\n",
        "                    \"'If a unit of measurement is used, specify which one'.\\n\"\n",
        "                )\n",
        "            })\n",
        "            problem_features = generator.chat(messages)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": problem_features})\n",
        "\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{CODE_ONLY_MSG}\"\n",
        "            })\n",
        "        else:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Provide the solution to the math problem in the most suitable way.\"\n",
        "            })\n",
        "\n",
        "        model_output = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": model_output})\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": messages[0][\"content\"],\n",
        "            \"resolvability_response\": resolvability_response,\n",
        "            \"is_octave_resolvable\": is_octave_resolvable,\n",
        "            \"problem_features\": problem_features,\n",
        "            \"prompt\": json.dumps(messages),\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "\n",
        "class ChainOfThoughtConversationalStrategy(ConversationalPromptStrategy):\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        rationale = str(row.get(\"rationale\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        messages = []\n",
        "        if rationale:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}.\\nRationale: {rationale}\\n\"})\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}.\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": \"OK, I've received the problem.\"})\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Determine if the math problem can be solved programmatically using GNU Octave. \"\n",
        "                \"Answer with 'True' or 'False' and justify your choice.\"\n",
        "            )\n",
        "        })\n",
        "        resolvability_response = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": resolvability_response})\n",
        "        is_octave_resolvable = parse_resolvability(resolvability_response)\n",
        "\n",
        "        problem_features = None\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Analyze the problem and extract its key data and the following characteristics:\\n\"\n",
        "                    \"'given_data', 'unknowns', 'constraints','applicable methods',\"\n",
        "                    \"'If a unit of measurement is used, specify which one'.\\n\"\n",
        "                )\n",
        "            })\n",
        "            problem_features = generator.chat(messages)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": problem_features})\n",
        "\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"{CODE_ONLY_MSG}\"})\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": \"Provide the solution to the math problem in the most suitable way.\"})\n",
        "\n",
        "        model_output = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": model_output})\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": messages[0][\"content\"],\n",
        "            \"resolvability_response\": resolvability_response,\n",
        "            \"is_octave_resolvable\": is_octave_resolvable,\n",
        "            \"problem_features\": problem_features,\n",
        "            \"prompt\": json.dumps(messages),\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "\n",
        "class ChainOfThoughtReasoningConversationalStrategy(ConversationalPromptStrategy):\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        # generator.generate devuelve lista\n",
        "        rationale_prompt = \"Think step by step to solve the following math problem:\\n\" + question + \"\\n\"\n",
        "        rationale_list = generator.generate([rationale_prompt])\n",
        "        rationale = rationale_list[0] if rationale_list else \"\"\n",
        "\n",
        "        messages = []\n",
        "        if rationale:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}.\\nRationale: {rationale}\\n\"})\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}.\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": \"OK, I've received the problem.\"})\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Determine if the math problem can be solved programmatically using GNU Octave. \"\n",
        "                \"Answer with 'True' or 'False' and justify your choice.\"\n",
        "            )\n",
        "        })\n",
        "        resolvability_response = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": resolvability_response})\n",
        "        is_octave_resolvable = parse_resolvability(resolvability_response)\n",
        "\n",
        "        problem_features = None\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Analyze the problem and extract its key data and the following characteristics:\\n\"\n",
        "                    \"'given_data', 'unknowns', 'constraints','applicable methods',\"\n",
        "                    \"'If a unit of measurement is used, specify which one'.\\n\"\n",
        "                )\n",
        "            })\n",
        "            problem_features = generator.chat(messages)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": problem_features})\n",
        "\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"{CODE_ONLY_MSG}\"})\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": \"Provide the solution to the math problem in the most suitable way.\"})\n",
        "\n",
        "        model_output = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": model_output})\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": messages[0][\"content\"],\n",
        "            \"resolvability_response\": resolvability_response,\n",
        "            \"is_octave_resolvable\": is_octave_resolvable,\n",
        "            \"problem_features\": problem_features,\n",
        "            \"prompt\": json.dumps(messages),\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "\n",
        "class RAGConversationalStrategy(ConversationalPromptStrategy):\n",
        "    def __init__(self, retriever):\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        messages = []\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": \"OK, I've received the problem.\"})\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Determine if the math problem can be solved programmatically using GNU Octave. \"\n",
        "                \"Answer with 'True' or 'False' and justify your choice.\"\n",
        "            )\n",
        "        })\n",
        "        resolvability_response = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": resolvability_response})\n",
        "        is_octave_resolvable = parse_resolvability(resolvability_response)\n",
        "\n",
        "        problem_features = None\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Analyze the problem and extract its key data and the following characteristics:\\n\"\n",
        "                    \"'given_data', 'unknowns', 'constraints','applicable methods',\"\n",
        "                    \"'If a unit of measurement is used, specify which one'.\\n\"\n",
        "                )\n",
        "            })\n",
        "            problem_features = generator.chat(messages)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": problem_features})\n",
        "\n",
        "        if is_octave_resolvable:\n",
        "            retrieved_context = self.retriever.retrieve(question)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"You are given related examples:\\n{retrieved_context}\\n\\n\"\n",
        "                    f\"Problem features: {problem_features}\\n\\n\"\n",
        "                    f\"Problem: {question}\\n\\n\"\n",
        "                    f\"{CODE_ONLY_MSG}\"\n",
        "                )\n",
        "            })\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Problem: {question}\\n\\nProvide the solution to the math problem in the most suitable way.\"})\n",
        "\n",
        "        model_output = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": model_output})\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": messages[0][\"content\"],\n",
        "            \"resolvability_response\": resolvability_response,\n",
        "            \"is_octave_resolvable\": is_octave_resolvable,\n",
        "            \"problem_features\": problem_features,\n",
        "            \"prompt\": json.dumps(messages),\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "\n",
        "class FewShotConversationalStrategy(ConversationalPromptStrategy):\n",
        "    def __init__(self, retriever, classifier=None, k=3):\n",
        "        self.retriever = retriever\n",
        "        self.classifier = classifier\n",
        "        self.k = k\n",
        "\n",
        "    def run_conversation(self, row: dict, generator) -> dict:\n",
        "        start_time = time.time()\n",
        "        question = str(row.get(\"question\", \"\")).strip()\n",
        "        if not question:\n",
        "            return {\"inference_time\": 0.0}\n",
        "\n",
        "        messages = []\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"Here is a math problem:\\n\\n{question}\"})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": \"OK, I've received the problem.\"})\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Determine if the math problem can be solved programmatically using GNU Octave. \"\n",
        "                \"Answer with 'True' or 'False' and justify your choice.\"\n",
        "            )\n",
        "        })\n",
        "        resolvability_response = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": resolvability_response})\n",
        "        is_octave_resolvable = parse_resolvability(resolvability_response)\n",
        "\n",
        "        problem_features = None\n",
        "        if is_octave_resolvable:\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Analyze the problem and extract its key data and the following characteristics:\\n\"\n",
        "                    \"'given_data', 'unknowns', 'constraints','applicable methods',\"\n",
        "                    \"'If a unit of measurement is used, specify which one'.\\n\"\n",
        "                )\n",
        "            })\n",
        "            problem_features = generator.chat(messages)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": problem_features})\n",
        "\n",
        "        if is_octave_resolvable:\n",
        "            # definir siempre retrieved_context y usar firma correcta del retriever\n",
        "            retrieved_context = \"\"\n",
        "            question_type = None\n",
        "            if self.classifier:\n",
        "                try:\n",
        "                    preds = self.classifier.predict(question)\n",
        "                    question_type = preds.get(\"problem_type\", None)\n",
        "                except Exception:\n",
        "                    question_type = None\n",
        "\n",
        "            if question_type:\n",
        "                # tu FewShotRetriever espera (problem_type, k)\n",
        "                retrieved_context = self.retriever.retrieve(problem_type=question_type, k=self.k)\n",
        "            else:\n",
        "                # si no hay tipo, tener un m√©todo general; si no, se queda vac√≠o\n",
        "                try:\n",
        "                    # default; si no, ignora\n",
        "                    retrieved_context = self.retriever.retrieve(problem_type=\"Unknown\", k=self.k)\n",
        "                except Exception:\n",
        "                    retrieved_context = \"\"\n",
        "\n",
        "            if retrieved_context:\n",
        "                messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"You are given related examples:\\n{retrieved_context}\\n\\n\"\n",
        "                        f\"Problem features: {problem_features}\\n\\n\"\n",
        "                        f\"Problem: {question}\\n\\n\"\n",
        "                        f\"{CODE_ONLY_MSG}\"\n",
        "                    )\n",
        "                })\n",
        "            else:\n",
        "                messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"Problem features: {problem_features}\\n\\n\"\n",
        "                        f\"Problem: {question}\\n\\n\"\n",
        "                        f\"{CODE_ONLY_MSG}\"\n",
        "                    )\n",
        "                })\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"Problem: {question}\\n\\nProvide the solution to the math problem in the most suitable way.\"})\n",
        "\n",
        "        model_output = generator.chat(messages)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": model_output})\n",
        "\n",
        "        return {\n",
        "            \"resolvability_prompt\": messages[0][\"content\"],\n",
        "            \"resolvability_response\": resolvability_response,\n",
        "            \"is_octave_resolvable\": is_octave_resolvable,\n",
        "            \"problem_features\": problem_features,\n",
        "            \"prompt\": json.dumps(messages),\n",
        "            \"model_output\": model_output,\n",
        "            \"inference_time\": time.time() - start_time\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEeE0QX7FuOu"
      },
      "outputs": [],
      "source": [
        "# Rutas fijas en Google Drive para cada dataset\n",
        "DATASET_PATHS = {\n",
        "    \"aqua\": \"/content/drive/MyDrive/tesis/datasets/AQUA/test.json\",\n",
        "    \"gsm8k\": \"/content/drive/MyDrive/tesis/datasets/GSM-8K/test.jsonl\",\n",
        "    \"math_data\": \"/content/drive/MyDrive/tesis/datasets/MATH/math_data.jsonl\",\n",
        "    \"math_shuffled\": \"/content/drive/MyDrive/tesis/datasets/MATH/shuffled_math.jsonl\",\n",
        "    \"mmlu\": \"/content/drive/MyDrive/tesis/datasets/MMLU/MMLU_test.jsonl\",\n",
        "    \"benchmark\": \"/content/drive/MyDrive/tesis/datasets/benchmarks/benchmark_math_gsm8k_3x300.jsonl\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ-gu4GCJLdV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, List, Optional\n",
        "import sympy as sp\n",
        "from sympy.parsing.latex import parse_latex as latex2sympy\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def boxed_to_value_string(boxed_expr: str, precision: int = 15) -> str:\n",
        "    \"\"\"\n",
        "    Convierte el contenido del \\\\boxed a string para tu dataset:\n",
        "      - Si es num√©rico real, devuelve el valor como string flotante (%.{precision}g).\n",
        "      - Si no es num√©rico (letra u otra expresi√≥n), devuelve la cadena tal cual.\n",
        "      - Siempre retorna str.\n",
        "    \"\"\"\n",
        "    s = (boxed_expr or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    try:\n",
        "        sym = latex2sympy(s)\n",
        "        # Intentar evaluar num√©ricamente con algo m√°s de precisi√≥n\n",
        "        val = sp.N(sym, precision + 5)\n",
        "        # Si es real (o convertible a float), lo formateamos; si no, devolvemos la cadena original\n",
        "        if val.is_real is False:\n",
        "            return s\n",
        "        # A veces is_real es None pero s√≠ es convertible:\n",
        "        f = float(val)\n",
        "        return f\"{f:.{precision}g}\"\n",
        "    except Exception:\n",
        "        # No se pudo parsear / evaluar: devolver la cadena original\n",
        "        return s\n",
        "\n",
        "def extract_boxed_solution(latex: str) -> str:\n",
        "    \"\"\"\n",
        "    Busca el primer \\\\boxed{...}, extrae lo que hay dentro.\n",
        "    Si no hay, devuelve cadena vac√≠a.\n",
        "    \"\"\"\n",
        "    if latex is None:\n",
        "        return \"\"\n",
        "    s = str(latex)\n",
        "    idx = s.find(r\"\\boxed{\")\n",
        "    if idx < 0:\n",
        "        return \"\"\n",
        "    start = idx + len(r\"\\boxed{\")\n",
        "    depth = 0\n",
        "    i = start\n",
        "    while i < len(s):\n",
        "        if s[i] == \"{\":\n",
        "            depth += 1\n",
        "        elif s[i] == \"}\":\n",
        "            if depth == 0:\n",
        "                return s[start:i].strip()\n",
        "            depth -= 1\n",
        "        i += 1\n",
        "    # si no encuentra cierre:\n",
        "    return s[start:].strip()\n",
        "\n",
        "\n",
        "\n",
        "class DatasetCSVBuilder:\n",
        "    \"\"\"\n",
        "    Transforma datasets JSON/JSONL a CSV estandarizado para el pipeline experimental,\n",
        "    con soporte para procesar solo los primeros `max_items` registros si se desea.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_paths: Dict[str, str]):\n",
        "        self.dataset_paths = dataset_paths\n",
        "\n",
        "        # Unidades comunes; ampliarlo si quieres\n",
        "        self.unit_pattern = re.compile(r'[$‚Ç¨¬£%]|(?<![a-zA-Z])m\\b|cm\\b|kg\\b|km\\b')\n",
        "        self.choice_pattern = re.compile(r'^\\s*([A-Za-z])\\)\\s*(.*)$')\n",
        "        self.boxed_pattern = re.compile(r'\\\\boxed\\{([^}]*)\\}')\n",
        "        self.simple_tex_pattern = re.compile(r'\\\\text\\{([^}]*)\\}')\n",
        "\n",
        "        # Columnas adicionales necesarias para el pipeline experimental completo\n",
        "        self.experiment_columns = [\n",
        "            \"strategy\",\n",
        "            \"model\",\n",
        "            \"dataset\",\n",
        "            \"resolvability_prompt\",\n",
        "            \"resolvability_response\",\n",
        "            \"is_octave_resolvable\",\n",
        "            \"problem_features\",\n",
        "            \"prompt\",\n",
        "            \"model_output\",\n",
        "            \"inference_time\",\n",
        "            \"octave_code\",\n",
        "            \"execution_output\",\n",
        "            \"execution_error\",\n",
        "            \"is_correct\",\n",
        "        ]\n",
        "\n",
        "    # --------------------------------------------------------------------- #\n",
        "    # API p√∫blica\n",
        "    # --------------------------------------------------------------------- #\n",
        "    def create_dataset(\n",
        "        self,\n",
        "        key: str,\n",
        "        max_items: Optional[int] = None,\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Lee el dataset indicado por `key`, procesa hasta `max_items`\n",
        "        registros (o todos si es None) y guarda el CSV resultante.\n",
        "        \"\"\"\n",
        "        path = self.dataset_paths[key]\n",
        "        records = self._load_records(path, max_items=max_items)\n",
        "        df = pd.DataFrame(records)\n",
        "        df = self._transform_dataframe(key, df)\n",
        "        df = self._finalize_schema(df, dataset_name=key)\n",
        "        self._save_as_csv(df, path, max_items=max_items)\n",
        "        return df\n",
        "\n",
        "    # --------------------------------------------------------------------- #\n",
        "    # Funciones internas\n",
        "    # --------------------------------------------------------------------- #\n",
        "    def _load_records(\n",
        "        self,\n",
        "        path: str,\n",
        "        *,\n",
        "        max_items: Optional[int] = None,\n",
        "    ) -> List[dict]:\n",
        "        \"\"\"\n",
        "        Devuelve una lista de diccionarios cargados desde un .json o .jsonl.\n",
        "        Si `max_items` est√° definido, corta la lista a ese tama√±o.\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            try:\n",
        "                if path.endswith(\".jsonl\"):\n",
        "                    for line in f:\n",
        "                        if not line.strip():\n",
        "                            continue\n",
        "                        data.append(json.loads(line.strip()))\n",
        "                        if max_items and len(data) >= max_items:\n",
        "                            break\n",
        "                else:  # .json (lista o objeto √∫nico)\n",
        "                    content = json.load(f)\n",
        "                    if isinstance(content, list):\n",
        "                        data.extend(content[:max_items] if max_items else content)\n",
        "                    else:\n",
        "                        data.append(content)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error al leer {path}: {e}\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _transform_dataframe(self, name: str, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Normaliza columnas a un esquema com√∫n:\n",
        "        - question, answer, rationale (strings)\n",
        "        - otras columnas espec√≠ficas por dataset (options, level, type, etc.)\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Asegurar existencia de columnas base (se completan luego)\n",
        "        for c in [\"question\", \"answer\", \"rationale\"]:\n",
        "            if c not in df.columns:\n",
        "                df[c] = None\n",
        "\n",
        "        if name == \"aqua\":\n",
        "            # AQUA-RAT: {question, options (list), correct (letra), rationale?}\n",
        "            if \"correct\" in df.columns:\n",
        "                df = df.rename(columns={\"correct\": \"answer\"})\n",
        "            # Asegurar question\n",
        "            if \"question\" not in df.columns and \"Problem\" in df.columns:\n",
        "                df = df.rename(columns={\"Problem\": \"question\"})\n",
        "\n",
        "            # Normalizar options a string \";\"-separado\n",
        "            if \"options\" in df.columns:\n",
        "                df[\"options\"] = df[\"options\"].apply(\n",
        "                    lambda x: ';'.join(x) if isinstance(x, list) else (x if isinstance(x, str) else \"\")\n",
        "                )\n",
        "\n",
        "            # Letra a may√∫sculas y sin espacios\n",
        "            df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
        "\n",
        "            # Mapear letra ‚Üí texto de opci√≥n; limpiar unidades si hay\n",
        "            df[\"answer_alternative\"] = df.apply(self._map_if_unit, axis=1)\n",
        "\n",
        "        elif name == \"gsm8k\":\n",
        "            # GSM8K: {\"question\", \"answer\"} con \"rationale #### final\"\n",
        "            if \"question\" not in df.columns and \"Question\" in df.columns:\n",
        "                df = df.rename(columns={\"Question\": \"question\"})\n",
        "            if \"answer\" in df.columns:\n",
        "                parts = df[\"answer\"].astype(str).str.split(\"####\", n=1, expand=True)\n",
        "                if parts.shape[1] == 2:\n",
        "                    df[\"rationale\"] = parts[0].str.strip()\n",
        "                    df[\"answer\"] = parts[1].str.strip()\n",
        "                else:\n",
        "                    # No hay '####': dejar answer tal cual y rationale vac√≠o\n",
        "                    df[\"rationale\"] = \"\"\n",
        "                    df[\"answer\"] = parts[0].str.strip()\n",
        "\n",
        "        elif name in (\"math_data\", \"math_shuffled\"):\n",
        "          # 1) question := problem\n",
        "          if \"problem\" in df.columns:\n",
        "              df[\"question\"] = df[\"problem\"].astype(str)\n",
        "          else:\n",
        "              # si faltara 'problem', conserva 'question' si existe; si no, vac√≠o\n",
        "              df[\"question\"] = df.get(\"question\", \"\").astype(str)\n",
        "\n",
        "          # 2) rationale := solution (solo para posibles estrategias que la usen)\n",
        "          if \"solution\" in df.columns:\n",
        "              df[\"rationale\"] = df[\"solution\"].astype(str)\n",
        "          else:\n",
        "              df[\"rationale\"] = df.get(\"rationale\", \"\").astype(str)\n",
        "\n",
        "          # 3) answer := contenido de \\boxed{...} convertido a string num√©rico si es evaluable; si no, tal cual\n",
        "          df[\"answer\"] = df[\"rationale\"].apply(\n",
        "              lambda s: extract_boxed_solution(s))\n",
        "\n",
        "\n",
        "        elif name == \"mmlu\":\n",
        "            # MMLU: nombres var√≠an; soportar \"question\"/\"Question\", \"choices\"/\"options\"\n",
        "            if \"question\" not in df.columns and \"Question\" in df.columns:\n",
        "                df = df.rename(columns={\"Question\": \"question\"})\n",
        "            if \"choices\" not in df.columns:\n",
        "                # construir 'choices' desde columnas \"Option*\"\n",
        "                option_cols = [c for c in df.columns if c.lower().startswith(\"option\")]\n",
        "                if option_cols:\n",
        "                    df[\"choices\"] = df[option_cols].apply(\n",
        "                        lambda row: ';'.join(row.values.astype(str)), axis=1\n",
        "                    )\n",
        "            # answer suele venir ya como letra o texto; lo dejamos tal cual\n",
        "        elif name == \"benchmark\":\n",
        "          # El benchmark ya debe venir con question/answer (y opcionalmente rationale, dataset).\n",
        "          # Normalizamos nombres y tipos sin re-interpretar contenido.\n",
        "          # Acepta tambi√©n 'problem'/'solution' como fallback por si alg√∫n bloque viene crudo.\n",
        "\n",
        "          # question\n",
        "          if \"question\" in df.columns:\n",
        "              df[\"question\"] = df[\"question\"].astype(str)\n",
        "          elif \"problem\" in df.columns:\n",
        "              df[\"question\"] = df[\"problem\"].astype(str)\n",
        "          else:\n",
        "              df[\"question\"] = \"\"\n",
        "\n",
        "          # answer\n",
        "          if \"answer\" in df.columns:\n",
        "              df[\"answer\"] = df[\"answer\"].astype(str)\n",
        "          elif \"solution\" in df.columns:\n",
        "              # Si por error entra crudo, lo dejamos tal cual (sin extraer \\boxed{})\n",
        "              df[\"answer\"] = df[\"solution\"].astype(str)\n",
        "          else:\n",
        "              df[\"answer\"] = \"\"\n",
        "\n",
        "          # rationale (opcional)\n",
        "          if \"rationale\" in df.columns:\n",
        "              df[\"rationale\"] = df[\"rationale\"].astype(str)\n",
        "          elif \"solution\" in df.columns:\n",
        "              df[\"rationale\"] = df[\"solution\"].astype(str)\n",
        "          else:\n",
        "              df[\"rationale\"] = \"\"\n",
        "\n",
        "          # dataset (si no viene, asignar 'benchmark')\n",
        "          if \"dataset\" not in df.columns:\n",
        "              df[\"dataset\"] = \"benchmark\"\n",
        "          else:\n",
        "              df[\"dataset\"] = df[\"dataset\"].astype(str).replace(\"\", \"benchmark\")\n",
        "\n",
        "\n",
        "        # Asegurar tipos string y strip b√°sico\n",
        "        for c in [\"question\", \"answer\", \"rationale\"]:\n",
        "            df[c] = df[c].astype(str).fillna(\"\").str.strip()\n",
        "\n",
        "        # Agregar id incremental si no existe\n",
        "        if \"id\" not in df.columns:\n",
        "            df.insert(0, \"id\", range(len(df)))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _finalize_schema(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
        "        # A√±adir columnas experimentales si faltan\n",
        "        for col in self.experiment_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = dataset_name if col == \"dataset\" else None\n",
        "\n",
        "        # Asegurar columnas m√≠nimas\n",
        "        for c in [\"question\", \"answer\", \"rationale\", \"dataset\"]:\n",
        "            if c not in df.columns:\n",
        "                df[c] = \"\" if c != \"dataset\" else dataset_name\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _map_if_unit(self, row):\n",
        "        \"\"\"\n",
        "        Para AQUA:\n",
        "        - row['answer'] es una letra (A/B/C/...)\n",
        "        - row['options'] es 'A) ...;B) ...;...'\n",
        "        Devuelve el texto de opci√≥n mapeado; si incluye unidades, las quita.\n",
        "        \"\"\"\n",
        "        letter = str(row.get(\"answer\", \"\")).strip().upper()\n",
        "        options = row.get(\"options\", \"\")\n",
        "        if not isinstance(options, str) or not options:\n",
        "            return None\n",
        "\n",
        "        # Construir mapping letra ‚Üí texto\n",
        "        mapping = {}\n",
        "        for raw in [p for p in options.split(';') if p.strip()]:\n",
        "            m = self.choice_pattern.match(raw)\n",
        "            if m:\n",
        "                k, v = m.group(1).upper(), m.group(2).strip()\n",
        "                mapping[k] = v\n",
        "            else:\n",
        "                # Si no hay \"A)\" expl√≠cito, mapear por orden A,B,C,D...\n",
        "                pass\n",
        "\n",
        "        if not mapping:\n",
        "            parts = [p.strip() for p in options.split(';') if p.strip()]\n",
        "            abc = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "            mapping = {abc[i]: parts[i] for i in range(min(len(parts), len(abc)))}\n",
        "\n",
        "        value = mapping.get(letter, None)\n",
        "        if not value:\n",
        "            return None\n",
        "\n",
        "        # Quitar unidades comunes si aparecen pegadas\n",
        "        cleaned = self.unit_pattern.sub('', value).strip()\n",
        "        return cleaned if cleaned else value\n",
        "\n",
        "    def _save_as_csv(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        original_path: str,\n",
        "        *,\n",
        "        max_items: Optional[int] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Guarda el CSV con un sufijo que indica si es un recorte parcial.\n",
        "        \"\"\"\n",
        "        base_dir = os.path.dirname(original_path)\n",
        "        base_name = os.path.splitext(os.path.basename(original_path))[0]\n",
        "        suffix = f\"_experiment_{max_items}\" if max_items else \"_experiment\"\n",
        "        output_path = os.path.join(base_dir, f\"{base_name}{suffix}.csv\")\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"‚úÖ CSV experimental guardado en: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKddwF89fWwU"
      },
      "source": [
        "# EXPERIMENTOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHx58GQEfe2X"
      },
      "source": [
        "# 1) Configuraci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8BzvyOVfjuK"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# CONFIG GLOBAL\n",
        "# =========================\n",
        "import os, re, json, random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "# LLM / ejecuci√≥n\n",
        "MAX_TOKENS       = 1024\n",
        "MAX_FIX_ATTEMPTS = 1\n",
        "EXEC_TIMEOUT     = 10\n",
        "\n",
        "# Salidas\n",
        "OUT_DIR = \"/content/drive/MyDrive/tesis/experiments_final\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- RAG CONFIG ---\n",
        "RAG_CSV_PATH = \"/content/drive/MyDrive/tesis/datasets/NuminaMath-1.5_rag_corpus/NuminaMath-1.5_rag_corpus_final.csv\"\n",
        "RAG_DB_PATH = \"./milvus_data.db\" #\"/content/drive/MyDrive/tesis/milvus_lite/milvus.db\"  # Milvus Lite\n",
        "RAG_COLLECTION = \"numinamath_rag_v1\"  # cambiar si regeneras el √≠ndice\n",
        "RAG_ENCODER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dims (coincide con MilvusRetriever)\n",
        "\n",
        "def results_path(dataset_key: str, model_key: str, strategy_name: str) -> str:\n",
        "    return os.path.join(OUT_DIR, f\"results_{dataset_key}_{strategy_name}_{model_key}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdJnHqZ0fqrK"
      },
      "source": [
        "# 2) Validaci√≥n de stdout y verificaci√≥n (n√∫mero **o** letra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNm4vaCLf8Ln"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# VALIDACI√ìN STRICTA & VERIFICACI√ìN\n",
        "# =========================\n",
        "import re\n",
        "from math_verify import parse, verify\n",
        "\n",
        "_NUMERIC_PATTERN = r\"[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?(?:[ij])?\"\n",
        "_LETTER_PATTERN  = r\"[A-Za-z]\"\n",
        "_VALID_OUTPUT = re.compile(rf\"^\\s*(?:{_NUMERIC_PATTERN}|{_LETTER_PATTERN})\\s*$\")\n",
        "\n",
        "def is_valid_stdout(s: str) -> bool:\n",
        "    \"\"\"True si stdout es SOLO un n√∫mero v√°lido o UNA letra (sin texto extra).\"\"\"\n",
        "    return bool(_VALID_OUTPUT.match(s or \"\"))\n",
        "\n",
        "def verify_output(gold: str, out: str) -> bool:\n",
        "    \"\"\"Compara gold vs stdout: letra directa o n√∫mero v√≠a math_verify.\"\"\"\n",
        "    gold = (\"\" if gold is None else str(gold)).strip()\n",
        "    out  = (\"\" if out  is None else str(out )).strip()\n",
        "    if not is_valid_stdout(out): return False\n",
        "    if len(out) == 1 and out.isalpha():\n",
        "        return gold.upper() == out.upper()\n",
        "    try:\n",
        "        return verify(parse(gold), parse(out))\n",
        "    except Exception:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elphqB3Ef-WM"
      },
      "source": [
        "# 3) Preprocesamiento del c√≥digo generado por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eHZySlPgDGD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Detecta bloques con fences markdown ```octave / ```m / ```matlab o gen√©ricos ```\n",
        "_CODE_FENCE_RE = re.compile(\n",
        "    r\"```(?:octave|matlab|m)?\\s*(.*?)```\",\n",
        "    re.DOTALL | re.IGNORECASE\n",
        ")\n",
        "\n",
        "# Alternativa: bloques HTML <code>...</code>\n",
        "_HTML_CODE_RE = re.compile(\n",
        "    r\"<code[^>]*>(.*?)</code>\",\n",
        "    re.DOTALL | re.IGNORECASE\n",
        ")\n",
        "\n",
        "def extract_octave_code(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extrae el c√≥digo Octave de una respuesta del modelo.\n",
        "    Prioriza bloques en fences Markdown o <code>...</code>.\n",
        "    Si no encuentra, devuelve el texto tal cual (strip).\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    s = str(text)\n",
        "\n",
        "    # 1) Fences Markdown con/ sin lenguaje\n",
        "    m = _CODE_FENCE_RE.search(s)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "\n",
        "    # 2) Bloques HTML <code>...</code>\n",
        "    m = _HTML_CODE_RE.search(s)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "\n",
        "    # 3) Fallback: devolver el texto \"limpio\" (sin fences) tal cual\n",
        "    #    (por si el modelo respet√≥ la policy y ya devolvi√≥ solo c√≥digo)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvF3lA0ugIrq"
      },
      "source": [
        "# 4) Builders (modelo, estrategia, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9MljfkXgNM4"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# BUILDERS\n",
        "# =========================\n",
        "\n",
        "# ============ RAG retriever builder (Milvus Lite) ============\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "\n",
        "_RAG_CACHE = {\"retriever\": None}\n",
        "\n",
        "def build_rag_retriever():\n",
        "    if _RAG_CACHE[\"retriever\"] is not None:\n",
        "        return _RAG_CACHE[\"retriever\"]\n",
        "\n",
        "    # Cargar corpus con embeddings precomputados\n",
        "    rag_df = pd.read_csv(RAG_CSV_PATH)\n",
        "\n",
        "    # Encoder para embebidos de consultas (384 dims; coincide con MilvusRetriever.dimension=384)\n",
        "    encoder = SentenceTransformer(RAG_ENCODER_NAME)\n",
        "\n",
        "    # Inicializar MilvusRetriever (usa Milvus Lite en un archivo local)\n",
        "    retriever = MilvusRetriever(\n",
        "        df=rag_df,\n",
        "        encoder=encoder,\n",
        "        db_path=RAG_DB_PATH,\n",
        "        collection_name=RAG_COLLECTION\n",
        "    )\n",
        "    _RAG_CACHE[\"retriever\"] = retriever\n",
        "    return retriever\n",
        "\n",
        "def build_generator(model_key: str, max_tokens: int = MAX_TOKENS):\n",
        "    return VLLMGenerator(\n",
        "        model_name=ModelRegistry.get_model_repo(model_key),\n",
        "        download=True,\n",
        "        temperature=0.0,   # estable para comparar Model/Policy\n",
        "        top_p=1.0,\n",
        "        max_tokens=max_tokens,\n",
        "        log_prompts=False\n",
        "    )\n",
        "\n",
        "def build_strategy(name: str):\n",
        "    n = name.lower()\n",
        "\n",
        "    if n == \"nonconv_zeroshot\": return NonConversationalZeroShotStrategy()\n",
        "    if n == \"nonconv_packed\": return NonConversationalReasonedCodeStrategy()\n",
        "\n",
        "    if n == \"few_shots\":\n",
        "        retr = FewShotRetriever(\n",
        "            csv_path=\"/content/drive/MyDrive/tesis/datasets/NuminaMath-1.5_rag_corpus/NuminaMath-1.5_rag_corpus_final.csv\"\n",
        "        )\n",
        "        clf = MultiTaskInferencePipeline(\n",
        "            model_path=\"/content/drive/MyDrive/tesis/categorizacion/bert_pre_trained_math_f1.pth\",\n",
        "            encoder_problem_path=\"/content/drive/MyDrive/tesis/categorizacion/problem_type_encoder.pkl\",\n",
        "            encoder_question_path=\"/content/drive/MyDrive/tesis/categorizacion/question_type_encoder.pkl\"\n",
        "        )\n",
        "        return FewShotConversationalStrategy(retriever=retr, classifier=clf, k=3)\n",
        "    if n == \"zero_shot\":        return ZeroShotConversationalStrategy()\n",
        "    if n == \"cot\":              return ChainOfThoughtConversationalStrategy()\n",
        "    if n == \"cot_reasoning\":    return ChainOfThoughtReasoningConversationalStrategy()\n",
        "\n",
        "    if n == \"rag\":\n",
        "        rag_retriever = build_rag_retriever()\n",
        "        return RAGConversationalStrategy(retriever=rag_retriever)\n",
        "\n",
        "    raise ValueError(f\"Estrategia no soportada: {name}\")\n",
        "\n",
        "\"\"\"def load_subset_df(dataset_key: str,\n",
        "                   start_index: int, # = START_INDEX,\n",
        "                   limit: int, # = LIMIT_PER_DATASET,\n",
        "                   n_samples: int = N_SAMPLES) -> pd.DataFrame:\"\"\"\n",
        "\n",
        "def load_subset_df(dataset_key: str,\n",
        "                   start_index: int,\n",
        "                   limit: int,\n",
        "                   n_samples: int ) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Devuelve los primeros n_samples a partir de start_index (sin aleatoriedad),\n",
        "    dentro de la ventana [start_index : start_index + limit).\n",
        "\n",
        "    Retorna columnas: row_id, question, answer\n",
        "    \"\"\"\n",
        "    builder = DatasetCSVBuilder(dataset_paths=DATASET_PATHS)\n",
        "    df_all = builder.create_dataset(dataset_key)\n",
        "\n",
        "    # Ventana fija del dataset\n",
        "    df_sub = df_all.iloc[start_index : start_index + limit].copy()\n",
        "\n",
        "    # Tomar en orden los primeros n_samples (si hay menos, devuelve los que haya)\n",
        "    if len(df_sub) > n_samples:\n",
        "        df_sub = df_sub.head(n_samples).copy()\n",
        "\n",
        "    # row_id estable por dataset+√≠ndice original del slice\n",
        "    df_sub.insert(0, \"row_id\", [f\"{dataset_key}:{i}\" for i in df_sub.index])\n",
        "\n",
        "    # Asegurar columnas m√≠nimas\n",
        "    for c in (\"question\", \"answer\"):\n",
        "        if c not in df_sub:\n",
        "            df_sub[c] = \"\"\n",
        "\n",
        "    return df_sub[[\"row_id\", \"question\", \"answer\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E-OHLCBgcw1"
      },
      "source": [
        "# 5) N√∫cleo de ejecuci√≥n por ejemplo (con pol√≠tica)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVbKAfA_gj6G"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# =========================\n",
        "# CORE: ejecutar 1 ejemplo con auto-fix y verificaci√≥n\n",
        "# =========================\n",
        "def run_single_example(row: dict, strategy, generator, executor,\n",
        "                       max_fix_attempts: int = MAX_FIX_ATTEMPTS) -> dict:\n",
        "\n",
        "    # inicio cron√≥metro local\n",
        "    _t0 = time.time()\n",
        "\n",
        "    question = str(row.get(\"question\", \"\")).strip()\n",
        "    answer   = str(row.get(\"answer\", \"\")).strip()\n",
        "    row_id   = row.get(\"row_id\")\n",
        "\n",
        "    # 1) Conversaci√≥n base (la estrategia ya decide c√≥mo pedir el c√≥digo)\n",
        "    base = strategy.run_conversation(row, generator)\n",
        "\n",
        "    # 2) Tomar el output del modelo y EXTRAER el c√≥digo (sin inyectar pol√≠tica)\n",
        "    raw_output = base.get(\"model_output\", \"\")\n",
        "    code = extract_octave_code(raw_output)\n",
        "\n",
        "    # 3) Ejecutar + auto-reparar si hay error o salida inv√°lida\n",
        "    attempts = 0\n",
        "    stdout = None\n",
        "    stderr = None\n",
        "    last_code = code\n",
        "\n",
        "    while attempts <= max_fix_attempts:\n",
        "        try:\n",
        "            stdout, stderr = executor.execute_with_timeout(last_code)\n",
        "        except Exception as e:\n",
        "            stdout, stderr = None, str(e)\n",
        "\n",
        "        if (stderr is None) and is_valid_stdout(stdout):\n",
        "            break\n",
        "\n",
        "        attempts += 1\n",
        "        if attempts > max_fix_attempts:\n",
        "            break\n",
        "\n",
        "        repair_messages = [\n",
        "            {\"role\": \"user\", \"content\":\n",
        "                \"The following GNU Octave code failed or produced an invalid output.\\n\"\n",
        "                \"Please fix it and return ONLY the corrected Octave script (no prose/markdown).\\n\\n\"\n",
        "                \"Previous code:\\n```octave\\n\" + last_code + \"\\n```\\n\"\n",
        "                \"stderr:\\n\" + (stderr or \"(none)\") + \"\\n\"\n",
        "                \"stdout:\\n\" + (stdout or \"(none)\") + \"\\n\"\n",
        "            }\n",
        "        ]\n",
        "        repaired = generator.chat(repair_messages)\n",
        "        last_code = extract_octave_code(repaired)\n",
        "\n",
        "    # 4) Verificaci√≥n estricta del stdout (n√∫mero o letra) y correcci√≥n\n",
        "    is_correct = (stderr is None) and is_valid_stdout(stdout) and verify_output(answer, stdout)\n",
        "\n",
        "    # sumar el tiempo local al de base.get(\"inference_time\")\n",
        "    _elapsed = time.time() - _t0\n",
        "    _base_time = base.get(\"inference_time\") or 0.0\n",
        "    _total_time = _base_time + _elapsed\n",
        "\n",
        "    return {\n",
        "        \"row_id\": row_id,\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"octave_code\": last_code,\n",
        "        \"execution_output\": stdout,\n",
        "        \"execution_error\": stderr,\n",
        "        \"is_correct\": is_correct,\n",
        "        # extras √∫tiles para auditor√≠a\n",
        "        \"resolvability_response\": base.get(\"resolvability_response\"),\n",
        "        \"is_octave_resolvable\": base.get(\"is_octave_resolvable\"),\n",
        "        \"problem_features\": base.get(\"problem_features\"),\n",
        "        \"inference_time\": _total_time,  # tiempo total\n",
        "        \"prompt_base\": base.get(\"prompt\"),\n",
        "        \"model_output_raw\": raw_output,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOAAPkzPWYg"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import display\n",
        "\n",
        "def expected_row_ids(dataset_key: str, start_index: int, limit: int, n_samples: int) -> list[str]:\n",
        "    \"\"\"\n",
        "    Deriva los row_id esperados para 'dataset_key' seg√∫n tu funci√≥n load_subset_df\n",
        "    (primeros N_SAMPLES desde START_INDEX).\n",
        "    \"\"\"\n",
        "    df = load_subset_df(dataset_key, start_index, limit, n_samples)\n",
        "    return df[\"row_id\"].astype(str).tolist()\n",
        "\n",
        "def progress_table(models, datasets, strategy_name, out_dir=OUT_DIR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Construye una tabla con el progreso por (dataset, model, strategy):\n",
        "        - total_target (N_SAMPLES)\n",
        "        - done (cu√°ntos row_id ya est√°n en CSV)\n",
        "        - pending (= total_target - done)\n",
        "        - out_csv (ruta del csv donde se escribe)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for model_key in models:\n",
        "        for dataset_key in datasets:\n",
        "            out_csv = results_path(dataset_key, model_key, strategy_name)\n",
        "            exp_ids = set(expected_row_ids(dataset_key))\n",
        "            done_ids = set()\n",
        "            if os.path.exists(out_csv):\n",
        "                try:\n",
        "                    tmp = pd.read_csv(out_csv, usecols=[\"row_id\"])\n",
        "                    done_ids = set(tmp[\"row_id\"].astype(str).tolist())\n",
        "                except Exception:\n",
        "                    done_ids = set()\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_key,\n",
        "                \"model\": model_key,\n",
        "                \"strategy\": strategy_name,\n",
        "                \"total_target\": len(exp_ids),\n",
        "                \"done\": len(exp_ids & done_ids),\n",
        "                \"pending\": len(exp_ids - done_ids),\n",
        "                \"out_csv\": out_csv\n",
        "            })\n",
        "    dfp = pd.DataFrame(rows).sort_values([\"dataset\",\"model\"]).reset_index(drop=True)\n",
        "    display(dfp)\n",
        "    return dfp\n",
        "\n",
        "def list_pending_ids(dataset_key: str, model_key: str, strategy_name: str, start_index: int, limit: int, n_samples: int) -> list[str]:\n",
        "    out_csv = results_path(dataset_key, model_key, strategy_name)\n",
        "    exp = set(expected_row_ids(dataset_key, start_index, limit, n_samples))\n",
        "    done = set()\n",
        "    if os.path.exists(out_csv):\n",
        "        try:\n",
        "            done = set(pd.read_csv(out_csv, usecols=[\"row_id\"])[\"row_id\"].astype(str))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return sorted(exp - done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41KLgTdmgrCP"
      },
      "source": [
        "# 6) Reanudaci√≥n (checkpoint) y orquestador multi-modelo/dataset/pol√≠tica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74H-TnFgvN5"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# REANUDACI√ìN & ORQUESTADOR\n",
        "# =========================\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def run_experiments_progress(models, datasets, strategy_name, start_index, limit, n_samples):\n",
        "    for model_key in models:\n",
        "        print(f\"\\n================= MODEL: {model_key} =================\")\n",
        "        generator = build_generator(model_key)\n",
        "        strategy  = build_strategy(strategy_name)\n",
        "        executor  = OctaveCodeExecutor(timeout=EXEC_TIMEOUT)\n",
        "\n",
        "        for dataset_key in datasets:\n",
        "            print(f\"-- dataset: {dataset_key}\")\n",
        "            # ids esperados y pendientes\n",
        "            all_ids = expected_row_ids(dataset_key, start_index, limit, n_samples)\n",
        "            out_csv = results_path(dataset_key, model_key, strategy_name)\n",
        "            done_ids = set()\n",
        "            if os.path.exists(out_csv):\n",
        "                try:\n",
        "                    done_ids = set(pd.read_csv(out_csv, usecols=[\"row_id\"])[\"row_id\"].astype(str))\n",
        "                except Exception:\n",
        "                    done_ids = set()\n",
        "            pend_ids = [rid for rid in all_ids if rid not in done_ids]\n",
        "\n",
        "            # si no hay pendientes, contin√∫a\n",
        "            if not pend_ids:\n",
        "                print(\"No hay pendientes. Saltando.\")\n",
        "                continue\n",
        "\n",
        "            # cargar subset completo una sola vez y filtrar por pending ids\n",
        "            df = load_subset_df(dataset_key, start_index, limit, n_samples)\n",
        "            df = df[df[\"row_id\"].isin(pend_ids)].copy()\n",
        "\n",
        "            write_header = not os.path.exists(out_csv)\n",
        "            with open(out_csv, \"a\", encoding=\"utf-8\") as fout:\n",
        "                for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"{dataset_key} ({len(pend_ids)} pendientes)\"):\n",
        "                    rowd = row.to_dict()\n",
        "                    try:\n",
        "                        out = run_single_example(\n",
        "                            row=rowd,\n",
        "                            strategy=strategy,\n",
        "                            generator=generator,\n",
        "                            executor=executor,\n",
        "                            max_fix_attempts=MAX_FIX_ATTEMPTS\n",
        "                        )\n",
        "                        out.update({\n",
        "                            \"dataset\": dataset_key,\n",
        "                            \"model\": model_key,\n",
        "                            \"strategy\": strategy_name,\n",
        "                            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\")\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        out = {\n",
        "                            \"row_id\": rowd.get(\"row_id\"),\n",
        "                            \"question\": rowd.get(\"question\"),\n",
        "                            \"answer\": rowd.get(\"answer\"),\n",
        "                            \"octave_code\": None,\n",
        "                            \"execution_output\": None,\n",
        "                            \"execution_error\": f\"PIPELINE_ERROR: {e}\",\n",
        "                            \"is_correct\": False,\n",
        "                            \"dataset\": dataset_key,\n",
        "                            \"model\": model_key,\n",
        "                            \"strategy\": strategy_name,\n",
        "                            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\")\n",
        "                        }\n",
        "                    pd.DataFrame([out]).to_csv(fout, header=write_header, index=False)\n",
        "                    write_header = False\n",
        "\n",
        "            # peque√±o resumen por combo al terminar\n",
        "            new_done = len(set(expected_row_ids(dataset_key, start_index, limit, n_samples)) & set(pd.read_csv(out_csv, usecols=[\"row_id\"])[\"row_id\"].astype(str)))\n",
        "            print(f\"  ‚Üí Progreso {dataset_key}/{model_key}: {new_done}/{len(all_ids)} completados\")\n",
        "\n",
        "    print(\"Experimentos finalizados / checkpoints actualizados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2EaQn6xg0_m"
      },
      "source": [
        "# 7) Lanzar los experimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4470c520841c4f11b401e08eaa6f99e4",
            "a12036e657a640d5925bc645ccf77922",
            "40d7109feda54e839b2b2617438c06c0",
            "27f5d9c762c846d19ce835e54d3591ba",
            "142883a7ed0740909c419a0d9d422a82",
            "ab99de1ce8564259bc1bf7ac9227fc39",
            "9ea73396f29d41abbbbd35d4aa89a3e7",
            "6695c903878141b7ab34e5dc1c1ee7d5",
            "c8928c86e7e5425c916149b4c9fe7a6e",
            "c27df96da9244898900d78b688b3ca33",
            "61bd33bb31cb4e75bdcaa1cd573a5b9a",
            "eb89d424d93849449b4445cc3b261994",
            "ef3cd4ef3bba4189b9da8a160107b61d",
            "9130bf8144d34b6b931e94c6f4c2760d",
            "343ba10c117d496d8c08b210d08c4d38",
            "5e1bc93a703e4c9d80b49042ca67e40a",
            "d72b2d65b2fd41178493453ee1a1d68e",
            "e23503dd0b9c42d0bab65ddf0769959e",
            "4b524aafdbd6409c80edf591c88ff068",
            "b47a6508da1a4da9ba0802fc23f27a89",
            "bc28576ae5fe4028bb1673548913c610",
            "e141693ba4194fe6b2049a464c941f95",
            "7f726160e6414694b6b265b476db7aac",
            "41d2b317d64340e88c07128511741ec3",
            "64c213339d954921b91f9ab252d4be7e",
            "a96f078cb8c141369fc74ca1329ff45c",
            "3383aab391ca4f9d9ff281f217795fa5",
            "5d09be0becde42d5b8e9a3f9d88c8ff6",
            "2b58bc0754cd487e8b57767c28e5524c",
            "2c1db08e3d4d41088da502e03bbba6ce",
            "f3fce9a7e1874a889a45ac5abe84735a",
            "4fafee8c1a42465b9360fd017eb44cc7",
            "7c9e2ccc195e45a18497ab189b533bc9",
            "239b885e263d428296af794004283b39",
            "acd677ebc829491abb66d60ae25f069c",
            "e751e61216c14b819d2052073b33889c",
            "bc305dda9db24179bae18c8ba317dfd3",
            "114f2708ced44fdabf0ff51dd8f3dc03",
            "6ba8fa6e3ba14465bbf4a29adc0aac11",
            "28557f0f3aef44e6b3061c49db419ea7",
            "e785db7117484a5fa9e351ca1a9096eb",
            "89ea13f87f01400e8897c68818dd57c1",
            "a53a7d2637bd4ff5819986e21e8d7026",
            "63741ecf1de64c63a471007678bc45ea",
            "bfc742aa581841048ec39dd4a7ccdb59",
            "6232623c943a4dd69c155db7d8f0adeb",
            "942a8ff5d7ef440d9cb74a02d78d13a8",
            "96a855917d4c4064bdc0d98360e8ee4a",
            "3cdccdf791be4d9aaf32f3e452215e88",
            "4d2f9c693afc415a836a85a2c9996573",
            "bd9aeee92cc74b3691a2c8810ba1fffd",
            "eb50be6a64524245b712cd78687cd8f7",
            "edbad2a771a84f54b2e04cb741b7ef97",
            "8a093daee5c941be825d5ce2f5a6aaa3",
            "f358b05b88474b0fa57edadcacc32220",
            "754c5e344ee84331a53e3770b1e97d4d",
            "5bc35e7ed9a044398cc7b30d9555af84",
            "e00f250101d4418e996c173330b1c56c",
            "b880a18e94e24d39ac80219e6d50c458",
            "246a2b6d6a2c4781a6a0cf0609c04fc2",
            "185274f7413e4ee6abf281bfb7f9ace7",
            "53d7760b9d184aacb70639b4eaa2e5cf",
            "70bd5e72c69440019cec34c8792aac26",
            "92a520031675470fb5eb5a763a967a48",
            "174304e888304d69b822613796304ed0",
            "90c9135bc54745c8ad284454757cd906",
            "d7de65df1a164e75a3c08bf7363a8bea",
            "1ee3944651af4a0c955ceee3ab19d4d7",
            "60bf3ab1764c46cd9206f77beab561cf",
            "c75b7544e71c4d1898dd3e9c00b68796",
            "41fb45f87c1848f8b4756e87787ab00e",
            "dd9acc2369484d0d9c4755ca125912fd",
            "6f3347ae1df74cf19a2d2ccfa3462270",
            "7552d84e6ea34958b642e817bd72bc2e",
            "d095602a2e39425d9c68def7bdf3febc",
            "8fb301f3b1da46a091b8a4bca284a86b",
            "b77dd04e4a064376aedbea13d372fed9",
            "bd45523fb0954bba8132463cedb799ee",
            "bc044d9b3e7e4f4d957baca8f26801ab",
            "ea7c3f362e33461395adefdc4d056fb0",
            "207a53ca98444f50ad339871aa1f449e",
            "1f5f8671c82e4ec38046f5326b20a829",
            "0eed6f54efb94477aea11c91b5f0a683",
            "041f42a6446843fba16095917254f01c",
            "691f5a1b2e344e118b5a7fad4974fe86",
            "d874d17e5c354109882b686f901addc7",
            "1d943d13d6d14c47bb6b8a5be325a715",
            "fcc072a04e394abb9bfcf9059f826535",
            "b36068e2a5a14d14887d0c6a54a54a4b",
            "cecbc8e21f9e4293a822f8bcabe33515",
            "64f7375d9d4a41d780424dbbed990442",
            "86530f10792d41b596ef21de6c8a4035",
            "508045b8697f41ae865f3a96dcef5327",
            "6944042150b74bed86671f0c3bb64af5",
            "6ddaa48f8b5a4cb19a4542a66917cc5e",
            "3a96c92fc5b54931b4f51633fa85dc12",
            "203c74e46ada498c91815de17322f7ca",
            "2aea09316613428e9b480eba2b365793",
            "1d8cad43f7d543c286c67ba05076d49f",
            "262ab786dd754ac2b84b62fc78ef69ac",
            "54833a9af9824dedbac65571846b6b21",
            "9f78f306323a4a7b9f0ce0c60e364ec7",
            "418a86013da24ea59523b37e6a9a2f86",
            "0f7d6b23c24347f1a823f4df29aa70ba",
            "039d91620b94464ea8ef00a9b7d0247d",
            "f79e5d51f06d4c01a716b1cb38ef9f27",
            "ec4099180e1445b9ae7a4bde896f971c",
            "48d8e999f2034392ba6611f62861ceee",
            "1c49c169f67b45da94afec4c5c1693e6",
            "56d395cd612d400d823430ccefede778",
            "309048674d974dd380dfd6dae6846b1c",
            "e2a82bf4db914a91b76d6f58caa76595",
            "1661cb5b003d42268c1c52feed1a55c9",
            "5502a09de5f34572b7ae0a59e0da19f5",
            "df9c512ca16d420e9c0f28cbd0df8e61",
            "29d48719faba4b9eb62c7ff7c20711ec",
            "2d535b0da66e4381a2269652143ea69e",
            "89ce1ddfd48b4572bd03b9ff17b52dd2",
            "c43c6cad52924975919ff814aae650aa",
            "628a848ccc79463a85d34a176db69a5f",
            "b2e0008d4d90438a8f3706e166baa854"
          ]
        },
        "id": "VYjIrFOHQxNH",
        "outputId": "ee9a5543-120c-41fc-9a5e-5b5fa3b193d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================= MODEL: deepseek-math-7b =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4470c520841c4f11b401e08eaa6f99e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb89d424d93849449b4445cc3b261994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f726160e6414694b6b265b476db7aac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "239b885e263d428296af794004283b39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a9d4f501-4393-4763-9295-75638d5ea02d)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/LICENSE\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a9d4f501-4393-4763-9295-75638d5ea02d)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/LICENSE\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 098e2ca6-1afb-42d4-b5f1-fc958dfcb1cb)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/pytorch_model-00001-of-00002.bin\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 098e2ca6-1afb-42d4-b5f1-fc958dfcb1cb)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/pytorch_model-00001-of-00002.bin\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c75c1333-c742-4d9b-bc23-395429acf5d6)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/generation_config.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c75c1333-c742-4d9b-bc23-395429acf5d6)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/generation_config.json\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 21f05e04-d8d2-4a2c-8a3e-22e42eebfead)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/README.md\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 21f05e04-d8d2-4a2c-8a3e-22e42eebfead)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/README.md\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 65513a30-4856-4498-b69e-c61ab2cdb42a)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/.gitattributes\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 65513a30-4856-4498-b69e-c61ab2cdb42a)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/.gitattributes\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 47d5d15b-b4ce-4207-ad30-c54fc000224e)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/pytorch_model-00002-of-00002.bin\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 47d5d15b-b4ce-4207-ad30-c54fc000224e)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/pytorch_model-00002-of-00002.bin\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9c151ff9-11d5-49c3-a63e-3efc98a205cc)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/config.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9c151ff9-11d5-49c3-a63e-3efc98a205cc)')' thrown while requesting HEAD https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/resolve/0a5828f800a36df0fd7f0ed581b983246c0677ff/config.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfc742aa581841048ec39dd4a7ccdb59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "754c5e344ee84331a53e3770b1e97d4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7de65df1a164e75a3c08bf7363a8bea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "LICENSE: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd45523fb0954bba8132463cedb799ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b36068e2a5a14d14887d0c6a54a54a4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "262ab786dd754ac2b84b62fc78ef69ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "309048674d974dd380dfd6dae6846b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-02 17:10:45 [utils.py:233] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.92, 'disable_log_stats': True, 'model': '/content/deepseek-math-7b-instruct'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-02 17:10:45 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-02 17:10:45 [model.py:1510] Using max model len 4096\n",
            "INFO 11-02 17:10:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-02 17:10:47 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "INFO 11-02 17:12:12 [llm.py:306] Supported_tasks: ['generate']\n",
            "Indexados 1994 ejemplos en 6 categor√≠as (problem_type).\n",
            "-- dataset: benchmark\n",
            "‚úÖ CSV experimental guardado en: /content/drive/MyDrive/tesis/datasets/benchmarks/benchmark_math_gsm8k_3x300_experiment.csv\n",
            "No hay pendientes. Saltando.\n",
            "Experimentos finalizados / checkpoints actualizados.\n"
          ]
        }
      ],
      "source": [
        "# Tama√±os\n",
        "N_SAMPLES          = 900      # ejercicios por dataset para iteraci√≥n r√°pida\n",
        "START_INDEX        = 0       # desde qu√© √≠ndice muestrear\n",
        "LIMIT_PER_DATASET  = 900     # recortar a los primeros 400 √≠tems\n",
        "\n",
        "# Datasets / Modelos a evaluar\n",
        "DATASETS_TO_RUN = [\"benchmark\"] #[\"gsm8k\", \"math_data\", \"math_shuffled\", \"benchmark\"]\n",
        "MODELS_TO_RUN   = [\"deepseek-math-7b\"] #[\"deepseek-math-7b\", \"mathstral-7b\", \"mistral-7b-instruct\", \"qwen2-math-7b-instruct\"]\n",
        "# Estrategia por defecto (\"nonconv_zeroshot\", \"nonconv_packed\", \"zero_shot\", \"few_shots\", \"cot_reasoning\", \"rag\")\n",
        "STRATEGY_NAME   = \"few_shots\"\n",
        "\n",
        "run_experiments_progress(\n",
        "    models=MODELS_TO_RUN,\n",
        "    datasets=DATASETS_TO_RUN,\n",
        "    strategy_name=STRATEGY_NAME,\n",
        "    start_index=START_INDEX,\n",
        "    limit=LIMIT_PER_DATASET,\n",
        "    n_samples=N_SAMPLES\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumen & Leaderboard"
      ],
      "metadata": {
        "id": "pNfUh0pqLQ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# RESUMEN & RANKING (penaliza no cobertura) + TIEMPOS\n",
        "# =========================\n",
        "import os, glob, math\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/tesis/experiments_final\"\n",
        "\n",
        "# 1) Cargar todos los CSV de resultados\n",
        "csv_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"results_*.csv\")))\n",
        "if not csv_paths:\n",
        "    raise FileNotFoundError(f\"No se encontraron CSVs de resultados en {OUT_DIR}\")\n",
        "\n",
        "dfs = []\n",
        "for p in csv_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(p)\n",
        "        # columnas m√≠nimas\n",
        "        for c in [\n",
        "            \"row_id\",\"dataset\",\"model\",\"strategy\",\"is_correct\",\n",
        "            \"octave_code\",\"execution_output\",\"execution_error\",\n",
        "            \"inference_time\"\n",
        "        ]:\n",
        "            if c not in df.columns:\n",
        "                df[c] = None\n",
        "        df[\"__source_file\"] = os.path.basename(p)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error leyendo {p}: {e}\")\n",
        "\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"No se pudo cargar ning√∫n CSV v√°lido.\")\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# 2) Normalizaci√≥n de tipos\n",
        "# - is_correct: booleana (NaN -> False)\n",
        "all_df[\"is_correct\"] = all_df[\"is_correct\"].fillna(False).astype(bool)\n",
        "\n",
        "# Asegura la columna is_octave_resolvable\n",
        "if \"is_octave_resolvable\" not in all_df.columns:\n",
        "    all_df[\"is_octave_resolvable\"] = False\n",
        "all_df[\"is_octave_resolvable\"] = all_df[\"is_octave_resolvable\"].fillna(False).astype(bool)\n",
        "\n",
        "# Define qu√© estrategias cuentan como conversacionales\n",
        "CONV_STRATS = {\n",
        "    \"zero_shot\", \"few_shots\", \"cot\", \"cot_reasoning\", \"rag\"\n",
        "}\n",
        "\n",
        "# Marca de intento para no conversacionales (rastro de ejecuci√≥n)\n",
        "attempted_nonconv = (\n",
        "    all_df[\"octave_code\"].notna() |\n",
        "    all_df[\"execution_output\"].notna() |\n",
        "    all_df[\"execution_error\"].notna()\n",
        ")\n",
        "\n",
        "# Calcula attempted seg√∫n el tipo de estrategia\n",
        "all_df[\"attempted\"] = np.where(\n",
        "    all_df[\"strategy\"].astype(str).isin(CONV_STRATS),\n",
        "    all_df[\"is_octave_resolvable\"],      # conversacionales: solo si resoluble\n",
        "    attempted_nonconv                    # no conversacionales: rastro de ejecuci√≥n\n",
        ").astype(bool)\n",
        "\n",
        "# - inference_time: num√©rico (NaN si no disponible)\n",
        "all_df[\"inference_time\"] = pd.to_numeric(all_df[\"inference_time\"], errors=\"coerce\")\n",
        "\n",
        "# 3) Total de √≠tems por dataset (n√∫mero de problemas distintos en el dataset)\n",
        "total_by_dataset = (\n",
        "    all_df.groupby(\"dataset\")[\"row_id\"]\n",
        "    .nunique()\n",
        "    .rename(\"total_items\")\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# 4) Agregaci√≥n por (dataset, modelo, estrategia)\n",
        "agg = (\n",
        "    all_df.groupby([\"dataset\",\"model\",\"strategy\"], dropna=False)\n",
        "    .agg(\n",
        "        attempted=(\"attempted\", \"sum\"),                                # cu√°ntos problemas intent√≥\n",
        "        correct=(\"is_correct\", lambda s: int(pd.Series(s).sum())),     # cu√°ntos correctos\n",
        "        total_rows=(\"row_id\",\"count\"),                                  # filas logueadas (referencia)\n",
        "        inf_time_sum=(\"inference_time\", \"sum\"),                         # suma de tiempos (ignora NaN)\n",
        "        inf_time_mean=(\"inference_time\", \"mean\"),                       # media simple (ignora NaN)\n",
        "        inf_time_median=(\"inference_time\", \"median\")                    # mediana (robusta)\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# 5) Unir el total de √≠tems reales del dataset\n",
        "agg = agg.merge(total_by_dataset, on=\"dataset\", how=\"left\")\n",
        "\n",
        "# 6) M√©tricas\n",
        "# - coverage = attempted / total_items (qu√© fracci√≥n del dataset intent√≥)\n",
        "# - attempt_success = correct / attempted (precisi√≥n sobre lo intentado)\n",
        "# - overall_success = correct / total_items (penaliza no cobertura)\n",
        "agg[\"coverage\"] = agg.apply(lambda r: (r[\"attempted\"]/r[\"total_items\"]) if r[\"total_items\"]>0 else 0.0, axis=1)\n",
        "agg[\"attempt_success\"] = agg.apply(lambda r: (r[\"correct\"]/r[\"attempted\"]) if r[\"attempted\"]>0 else 0.0, axis=1)\n",
        "agg[\"overall_success\"] = agg.apply(lambda r: (r[\"correct\"]/r[\"total_items\"]) if r[\"total_items\"]>0 else 0.0, axis=1)\n",
        "\n",
        "# Tiempo medio por intento (promedia mejor cuando hay distinta cobertura)\n",
        "agg[\"inf_time_per_attempt\"] = agg.apply(\n",
        "    lambda r: (r[\"inf_time_sum\"]/r[\"attempted\"]) if r[\"attempted\"]>0 else float(\"nan\"), axis=1\n",
        ")\n",
        "\n",
        "# 7) Rankings\n",
        "# 7.1 Ranking por dataset (ordenado por overall_success desc; desempate: attempt_success desc, coverage desc)\n",
        "rank_per_dataset = (\n",
        "    agg.sort_values(\n",
        "        [\"dataset\",\"overall_success\",\"attempt_success\",\"coverage\",\"correct\"],\n",
        "        ascending=[True, False, False, False, False]\n",
        "    )\n",
        ")\n",
        "\n",
        "# 7.2 Ranking global sumando across datasets\n",
        "global_score = (\n",
        "    agg.groupby([\"model\",\"strategy\"], dropna=False)\n",
        "       .agg(\n",
        "           total_correct=(\"correct\",\"sum\"),\n",
        "           total_attempted=(\"attempted\",\"sum\"),\n",
        "           total_items=(\"total_items\",\"sum\"),\n",
        "           total_inf_time=(\"inf_time_sum\",\"sum\"),\n",
        "           mean_inf_time=(\"inf_time_mean\",\"mean\"),          # media de medias (informativa)\n",
        "           median_inf_time=(\"inf_time_median\",\"median\")     # mediana de medianas (robusta)\n",
        "       )\n",
        "       .reset_index()\n",
        ")\n",
        "\n",
        "# M√©tricas globales\n",
        "global_score[\"global_overall_success\"] = global_score.apply(\n",
        "    lambda r: (r[\"total_correct\"]/r[\"total_items\"]) if r[\"total_items\"]>0 else 0.0, axis=1\n",
        ")\n",
        "global_score[\"global_attempt_success\"] = global_score.apply(\n",
        "    lambda r: (r[\"total_correct\"]/r[\"total_attempted\"]) if r[\"total_attempted\"]>0 else 0.0, axis=1\n",
        ")\n",
        "global_score[\"global_coverage\"] = global_score.apply(\n",
        "    lambda r: (r[\"total_attempted\"]/r[\"total_items\"]) if r[\"total_items\"]>0 else 0.0, axis=1\n",
        ")\n",
        "# Tiempo medio global por intento (ponderado por intentos)\n",
        "global_score[\"global_inf_time_per_attempt\"] = global_score.apply(\n",
        "    lambda r: (r[\"total_inf_time\"]/r[\"total_attempted\"]) if r[\"total_attempted\"]>0 else float(\"nan\"), axis=1\n",
        ")\n",
        "\n",
        "global_rank = global_score.sort_values(\n",
        "    [\"global_overall_success\",\"global_attempt_success\",\"global_coverage\",\"total_correct\"],\n",
        "    ascending=[False, False, False, False]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 8) Guardar y mostrar\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "agg_path = os.path.join(OUT_DIR, f\"agg_overall_{ts}.csv\")\n",
        "rank_ds_path = os.path.join(OUT_DIR, f\"rank_per_dataset_{ts}.csv\")\n",
        "global_rank_path = os.path.join(OUT_DIR, f\"global_rank_{ts}.csv\")\n",
        "\n",
        "agg.to_csv(agg_path, index=False)\n",
        "rank_per_dataset.to_csv(rank_ds_path, index=False)\n",
        "global_rank.to_csv(global_rank_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Guardados:\")\n",
        "print(\"  -\", agg_path)\n",
        "print(\"  -\", rank_ds_path)\n",
        "print(\"  -\", global_rank_path)\n",
        "\n",
        "print(\"\\n=== üèÅ Ranking GLOBAL (mejor ‚Üí peor) ===\")\n",
        "display(rank_per_dataset[[\n",
        "    \"dataset\",\"model\",\"strategy\",\n",
        "    \"correct\",\"attempted\",\"total_items\",\n",
        "    \"overall_success\",\"attempt_success\",\"coverage\",\n",
        "    \"inf_time_per_attempt\",\"inf_time_mean\",\"inf_time_median\",\"inf_time_sum\"\n",
        "]])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XhZi2sG5QRO1",
        "outputId": "92fc0f0b-ca34-49ba-9f6b-61d311501204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Guardados:\n",
            "  - /content/drive/MyDrive/tesis/experiments_final/agg_overall_20251030_210027.csv\n",
            "  - /content/drive/MyDrive/tesis/experiments_final/rank_per_dataset_20251030_210027.csv\n",
            "  - /content/drive/MyDrive/tesis/experiments_final/global_rank_20251030_210027.csv\n",
            "\n",
            "=== üèÅ Ranking GLOBAL (mejor ‚Üí peor) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2856398056.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  all_df[\"is_octave_resolvable\"] = all_df[\"is_octave_resolvable\"].fillna(False).astype(bool)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      dataset                   model          strategy  correct  attempted  \\\n",
              "6   benchmark            mathstral-7b     cot_reasoning      422        900   \n",
              "3   benchmark        deepseek-math-7b  nonconv_zeroshot      379        811   \n",
              "18  benchmark  qwen2-math-7b-instruct     cot_reasoning      379        868   \n",
              "10  benchmark            mathstral-7b               rag      377        900   \n",
              "2   benchmark        deepseek-math-7b    nonconv_packed      376        827   \n",
              "11  benchmark            mathstral-7b         zero_shot      365        900   \n",
              "7   benchmark            mathstral-7b         few_shots      364        900   \n",
              "9   benchmark            mathstral-7b  nonconv_zeroshot      355        900   \n",
              "23  benchmark  qwen2-math-7b-instruct         zero_shot      352        777   \n",
              "19  benchmark  qwen2-math-7b-instruct         few_shots      347        782   \n",
              "22  benchmark  qwen2-math-7b-instruct               rag      346        779   \n",
              "1   benchmark        deepseek-math-7b         few_shots      335        711   \n",
              "4   benchmark        deepseek-math-7b               rag      328        711   \n",
              "20  benchmark  qwen2-math-7b-instruct    nonconv_packed      325        900   \n",
              "0   benchmark        deepseek-math-7b     cot_reasoning      301        750   \n",
              "21  benchmark  qwen2-math-7b-instruct  nonconv_zeroshot      281        900   \n",
              "8   benchmark            mathstral-7b    nonconv_packed      272        900   \n",
              "5   benchmark        deepseek-math-7b         zero_shot      263        711   \n",
              "12  benchmark     mistral-7b-instruct     cot_reasoning      261        898   \n",
              "16  benchmark     mistral-7b-instruct               rag      259        894   \n",
              "13  benchmark     mistral-7b-instruct         few_shots      250        894   \n",
              "15  benchmark     mistral-7b-instruct  nonconv_zeroshot      243        900   \n",
              "17  benchmark     mistral-7b-instruct         zero_shot      233        894   \n",
              "14  benchmark     mistral-7b-instruct    nonconv_packed      215        900   \n",
              "\n",
              "    total_items  overall_success  attempt_success  coverage  \\\n",
              "6           900         0.468889         0.468889  1.000000   \n",
              "3           900         0.421111         0.467324  0.901111   \n",
              "18          900         0.421111         0.436636  0.964444   \n",
              "10          900         0.418889         0.418889  1.000000   \n",
              "2           900         0.417778         0.454655  0.918889   \n",
              "11          900         0.405556         0.405556  1.000000   \n",
              "7           900         0.404444         0.404444  1.000000   \n",
              "9           900         0.394444         0.394444  1.000000   \n",
              "23          900         0.391111         0.453024  0.863333   \n",
              "19          900         0.385556         0.443734  0.868889   \n",
              "22          900         0.384444         0.444159  0.865556   \n",
              "1           900         0.372222         0.471167  0.790000   \n",
              "4           900         0.364444         0.461322  0.790000   \n",
              "20          900         0.361111         0.361111  1.000000   \n",
              "0           900         0.334444         0.401333  0.833333   \n",
              "21          900         0.312222         0.312222  1.000000   \n",
              "8           900         0.302222         0.302222  1.000000   \n",
              "5           900         0.292222         0.369902  0.790000   \n",
              "12          900         0.290000         0.290646  0.997778   \n",
              "16          900         0.287778         0.289709  0.993333   \n",
              "13          900         0.277778         0.279642  0.993333   \n",
              "15          900         0.270000         0.270000  1.000000   \n",
              "17          900         0.258889         0.260626  0.993333   \n",
              "14          900         0.238889         0.238889  1.000000   \n",
              "\n",
              "    inf_time_per_attempt  inf_time_mean  inf_time_median   inf_time_sum  \n",
              "6             101.477798     101.477798        97.070483   91330.018030  \n",
              "3              26.336648      23.732246        18.867350   21359.021514  \n",
              "18            170.554202     164.490053       151.672980  148041.047634  \n",
              "10             60.705703      60.705703        56.027802   54635.132286  \n",
              "2              25.215631      23.170363        19.114528   20853.326453  \n",
              "11             63.291097      63.291097        59.663431   56961.987339  \n",
              "7              61.473006      61.473006        57.597412   55325.705601  \n",
              "9              24.845617      24.845617        21.301638   22361.055521  \n",
              "23            137.642868     118.831676       107.985078  106948.508755  \n",
              "19            140.296614     121.766873       110.719337  109711.952215  \n",
              "22            138.814449     120.151617       109.153378  108136.455578  \n",
              "1              77.030258      60.853904        52.114822   54768.513441  \n",
              "4              78.941828      62.364044        52.649830   56127.639489  \n",
              "20             39.339613      39.339613        32.792867   35405.651644  \n",
              "0              86.230075      71.858396        63.971494   64672.556501  \n",
              "21             17.476885      17.476885        10.317768   15729.196182  \n",
              "8              30.826952      30.826952        26.458811   27744.256730  \n",
              "5              74.329520      58.720321        50.989352   52848.289075  \n",
              "12            124.749717     124.472495       108.378811  112025.245633  \n",
              "16             76.515856      76.005750        69.587656   68405.174878  \n",
              "13             76.520630      76.010492        69.379429   68409.443116  \n",
              "15             17.905902      17.905902        13.280128   16115.312016  \n",
              "17             78.418835      77.896043        71.100954   70106.438343  \n",
              "14             21.257168      21.257168        15.216154   19131.451360  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce3f27b8-ff48-443e-b3af-30ab924c7074\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>model</th>\n",
              "      <th>strategy</th>\n",
              "      <th>correct</th>\n",
              "      <th>attempted</th>\n",
              "      <th>total_items</th>\n",
              "      <th>overall_success</th>\n",
              "      <th>attempt_success</th>\n",
              "      <th>coverage</th>\n",
              "      <th>inf_time_per_attempt</th>\n",
              "      <th>inf_time_mean</th>\n",
              "      <th>inf_time_median</th>\n",
              "      <th>inf_time_sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>422</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.468889</td>\n",
              "      <td>0.468889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>101.477798</td>\n",
              "      <td>101.477798</td>\n",
              "      <td>97.070483</td>\n",
              "      <td>91330.018030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>379</td>\n",
              "      <td>811</td>\n",
              "      <td>900</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>0.467324</td>\n",
              "      <td>0.901111</td>\n",
              "      <td>26.336648</td>\n",
              "      <td>23.732246</td>\n",
              "      <td>18.867350</td>\n",
              "      <td>21359.021514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>379</td>\n",
              "      <td>868</td>\n",
              "      <td>900</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>0.436636</td>\n",
              "      <td>0.964444</td>\n",
              "      <td>170.554202</td>\n",
              "      <td>164.490053</td>\n",
              "      <td>151.672980</td>\n",
              "      <td>148041.047634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>rag</td>\n",
              "      <td>377</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.418889</td>\n",
              "      <td>0.418889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>60.705703</td>\n",
              "      <td>60.705703</td>\n",
              "      <td>56.027802</td>\n",
              "      <td>54635.132286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>376</td>\n",
              "      <td>827</td>\n",
              "      <td>900</td>\n",
              "      <td>0.417778</td>\n",
              "      <td>0.454655</td>\n",
              "      <td>0.918889</td>\n",
              "      <td>25.215631</td>\n",
              "      <td>23.170363</td>\n",
              "      <td>19.114528</td>\n",
              "      <td>20853.326453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>365</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.405556</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.291097</td>\n",
              "      <td>63.291097</td>\n",
              "      <td>59.663431</td>\n",
              "      <td>56961.987339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>364</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.404444</td>\n",
              "      <td>0.404444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>61.473006</td>\n",
              "      <td>61.473006</td>\n",
              "      <td>57.597412</td>\n",
              "      <td>55325.705601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>355</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.845617</td>\n",
              "      <td>24.845617</td>\n",
              "      <td>21.301638</td>\n",
              "      <td>22361.055521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>352</td>\n",
              "      <td>777</td>\n",
              "      <td>900</td>\n",
              "      <td>0.391111</td>\n",
              "      <td>0.453024</td>\n",
              "      <td>0.863333</td>\n",
              "      <td>137.642868</td>\n",
              "      <td>118.831676</td>\n",
              "      <td>107.985078</td>\n",
              "      <td>106948.508755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>347</td>\n",
              "      <td>782</td>\n",
              "      <td>900</td>\n",
              "      <td>0.385556</td>\n",
              "      <td>0.443734</td>\n",
              "      <td>0.868889</td>\n",
              "      <td>140.296614</td>\n",
              "      <td>121.766873</td>\n",
              "      <td>110.719337</td>\n",
              "      <td>109711.952215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>rag</td>\n",
              "      <td>346</td>\n",
              "      <td>779</td>\n",
              "      <td>900</td>\n",
              "      <td>0.384444</td>\n",
              "      <td>0.444159</td>\n",
              "      <td>0.865556</td>\n",
              "      <td>138.814449</td>\n",
              "      <td>120.151617</td>\n",
              "      <td>109.153378</td>\n",
              "      <td>108136.455578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>335</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.372222</td>\n",
              "      <td>0.471167</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>77.030258</td>\n",
              "      <td>60.853904</td>\n",
              "      <td>52.114822</td>\n",
              "      <td>54768.513441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>rag</td>\n",
              "      <td>328</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.364444</td>\n",
              "      <td>0.461322</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>78.941828</td>\n",
              "      <td>62.364044</td>\n",
              "      <td>52.649830</td>\n",
              "      <td>56127.639489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>325</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>39.339613</td>\n",
              "      <td>39.339613</td>\n",
              "      <td>32.792867</td>\n",
              "      <td>35405.651644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>301</td>\n",
              "      <td>750</td>\n",
              "      <td>900</td>\n",
              "      <td>0.334444</td>\n",
              "      <td>0.401333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>86.230075</td>\n",
              "      <td>71.858396</td>\n",
              "      <td>63.971494</td>\n",
              "      <td>64672.556501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>281</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.476885</td>\n",
              "      <td>17.476885</td>\n",
              "      <td>10.317768</td>\n",
              "      <td>15729.196182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>272</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.302222</td>\n",
              "      <td>0.302222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30.826952</td>\n",
              "      <td>30.826952</td>\n",
              "      <td>26.458811</td>\n",
              "      <td>27744.256730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>263</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.292222</td>\n",
              "      <td>0.369902</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>74.329520</td>\n",
              "      <td>58.720321</td>\n",
              "      <td>50.989352</td>\n",
              "      <td>52848.289075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>261</td>\n",
              "      <td>898</td>\n",
              "      <td>900</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.290646</td>\n",
              "      <td>0.997778</td>\n",
              "      <td>124.749717</td>\n",
              "      <td>124.472495</td>\n",
              "      <td>108.378811</td>\n",
              "      <td>112025.245633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>rag</td>\n",
              "      <td>259</td>\n",
              "      <td>894</td>\n",
              "      <td>900</td>\n",
              "      <td>0.287778</td>\n",
              "      <td>0.289709</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>76.515856</td>\n",
              "      <td>76.005750</td>\n",
              "      <td>69.587656</td>\n",
              "      <td>68405.174878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>250</td>\n",
              "      <td>894</td>\n",
              "      <td>900</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.279642</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>76.520630</td>\n",
              "      <td>76.010492</td>\n",
              "      <td>69.379429</td>\n",
              "      <td>68409.443116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>243</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.905902</td>\n",
              "      <td>17.905902</td>\n",
              "      <td>13.280128</td>\n",
              "      <td>16115.312016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>233</td>\n",
              "      <td>894</td>\n",
              "      <td>900</td>\n",
              "      <td>0.258889</td>\n",
              "      <td>0.260626</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>78.418835</td>\n",
              "      <td>77.896043</td>\n",
              "      <td>71.100954</td>\n",
              "      <td>70106.438343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>215</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.238889</td>\n",
              "      <td>0.238889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.257168</td>\n",
              "      <td>21.257168</td>\n",
              "      <td>15.216154</td>\n",
              "      <td>19131.451360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce3f27b8-ff48-443e-b3af-30ab924c7074')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce3f27b8-ff48-443e-b3af-30ab924c7074 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce3f27b8-ff48-443e-b3af-30ab924c7074');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-185d788f-3c4c-4b47-90bc-bca1974efece\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-185d788f-3c4c-4b47-90bc-bca1974efece')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-185d788f-3c4c-4b47-90bc-bca1974efece button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"]])\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"benchmark\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"deepseek-math-7b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"cot_reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 215,\n        \"max\": 422,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          272\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attempted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 71,\n        \"min\": 711,\n        \"max\": 900,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          782\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_items\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 900,\n        \"max\": 900,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          900\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overall_success\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06371372051719705,\n        \"min\": 0.2388888888888889,\n        \"max\": 0.4688888888888889,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.3022222222222222\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attempt_success\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07794290785357628,\n        \"min\": 0.2388888888888889,\n        \"max\": 0.4711673699015471,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.45302445302445304\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coverage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07961347343872811,\n        \"min\": 0.79,\n        \"max\": 1.0,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.8688888888888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_per_attempt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44.326986301286,\n        \"min\": 17.47688464668062,\n        \"max\": 170.55420234373636,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          137.64286841051305\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40.44181201527468,\n        \"min\": 17.47688464668062,\n        \"max\": 164.4900529270702,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          118.8316763944096\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.067136968540325,\n        \"min\": 10.317768096923828,\n        \"max\": 151.67298030853271,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          107.98507785797119\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36404.74851535524,\n        \"min\": 15729.196182012558,\n        \"max\": 148041.04763436317,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          106948.50875496864\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(global_rank[[\n",
        "    \"model\",\"strategy\",\n",
        "    \"total_correct\",\"total_attempted\",\"total_items\",\n",
        "    \"global_overall_success\",\"global_attempt_success\",\"global_coverage\",\n",
        "    \"global_inf_time_per_attempt\",\"total_inf_time\",\"mean_inf_time\",\"median_inf_time\"\n",
        "]].head(20))\n",
        "\n",
        "print(\"\\n=== üß© Ranking por DATASET (ordenado por overall_success) ===\")\n",
        "display(rank_per_dataset[[\n",
        "    \"dataset\",\"model\",\"strategy\",\n",
        "    \"correct\",\"attempted\",\"total_items\",\n",
        "    \"overall_success\",\"attempt_success\",\"coverage\",\n",
        "    \"inf_time_per_attempt\",\"inf_time_mean\",\"inf_time_median\",\"inf_time_sum\"\n",
        "]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3jOrHE7aTZKI",
        "outputId": "ec0bb82f-2f14-4aa9-a96f-e2fa3b8b8099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                     model          strategy  total_correct  total_attempted  \\\n",
              "0             mathstral-7b     cot_reasoning            422              900   \n",
              "1         deepseek-math-7b  nonconv_zeroshot            379              811   \n",
              "2   qwen2-math-7b-instruct     cot_reasoning            379              868   \n",
              "3             mathstral-7b               rag            377              900   \n",
              "4         deepseek-math-7b    nonconv_packed            376              827   \n",
              "5             mathstral-7b         zero_shot            365              900   \n",
              "6             mathstral-7b         few_shots            364              900   \n",
              "7             mathstral-7b  nonconv_zeroshot            355              900   \n",
              "8   qwen2-math-7b-instruct         zero_shot            352              777   \n",
              "9   qwen2-math-7b-instruct         few_shots            347              782   \n",
              "10  qwen2-math-7b-instruct               rag            346              779   \n",
              "11        deepseek-math-7b         few_shots            335              711   \n",
              "12        deepseek-math-7b               rag            328              711   \n",
              "13  qwen2-math-7b-instruct    nonconv_packed            325              900   \n",
              "14        deepseek-math-7b     cot_reasoning            301              750   \n",
              "15  qwen2-math-7b-instruct  nonconv_zeroshot            281              900   \n",
              "16            mathstral-7b    nonconv_packed            272              900   \n",
              "17        deepseek-math-7b         zero_shot            263              711   \n",
              "18     mistral-7b-instruct     cot_reasoning            261              898   \n",
              "19     mistral-7b-instruct               rag            259              894   \n",
              "\n",
              "    total_items  global_overall_success  global_attempt_success  \\\n",
              "0           900                0.468889                0.468889   \n",
              "1           900                0.421111                0.467324   \n",
              "2           900                0.421111                0.436636   \n",
              "3           900                0.418889                0.418889   \n",
              "4           900                0.417778                0.454655   \n",
              "5           900                0.405556                0.405556   \n",
              "6           900                0.404444                0.404444   \n",
              "7           900                0.394444                0.394444   \n",
              "8           900                0.391111                0.453024   \n",
              "9           900                0.385556                0.443734   \n",
              "10          900                0.384444                0.444159   \n",
              "11          900                0.372222                0.471167   \n",
              "12          900                0.364444                0.461322   \n",
              "13          900                0.361111                0.361111   \n",
              "14          900                0.334444                0.401333   \n",
              "15          900                0.312222                0.312222   \n",
              "16          900                0.302222                0.302222   \n",
              "17          900                0.292222                0.369902   \n",
              "18          900                0.290000                0.290646   \n",
              "19          900                0.287778                0.289709   \n",
              "\n",
              "    global_coverage  global_inf_time_per_attempt  total_inf_time  \\\n",
              "0          1.000000                   101.477798    91330.018030   \n",
              "1          0.901111                    26.336648    21359.021514   \n",
              "2          0.964444                   170.554202   148041.047634   \n",
              "3          1.000000                    60.705703    54635.132286   \n",
              "4          0.918889                    25.215631    20853.326453   \n",
              "5          1.000000                    63.291097    56961.987339   \n",
              "6          1.000000                    61.473006    55325.705601   \n",
              "7          1.000000                    24.845617    22361.055521   \n",
              "8          0.863333                   137.642868   106948.508755   \n",
              "9          0.868889                   140.296614   109711.952215   \n",
              "10         0.865556                   138.814449   108136.455578   \n",
              "11         0.790000                    77.030258    54768.513441   \n",
              "12         0.790000                    78.941828    56127.639489   \n",
              "13         1.000000                    39.339613    35405.651644   \n",
              "14         0.833333                    86.230075    64672.556501   \n",
              "15         1.000000                    17.476885    15729.196182   \n",
              "16         1.000000                    30.826952    27744.256730   \n",
              "17         0.790000                    74.329520    52848.289075   \n",
              "18         0.997778                   124.749717   112025.245633   \n",
              "19         0.993333                    76.515856    68405.174878   \n",
              "\n",
              "    mean_inf_time  median_inf_time  \n",
              "0      101.477798        97.070483  \n",
              "1       23.732246        18.867350  \n",
              "2      164.490053       151.672980  \n",
              "3       60.705703        56.027802  \n",
              "4       23.170363        19.114528  \n",
              "5       63.291097        59.663431  \n",
              "6       61.473006        57.597412  \n",
              "7       24.845617        21.301638  \n",
              "8      118.831676       107.985078  \n",
              "9      121.766873       110.719337  \n",
              "10     120.151617       109.153378  \n",
              "11      60.853904        52.114822  \n",
              "12      62.364044        52.649830  \n",
              "13      39.339613        32.792867  \n",
              "14      71.858396        63.971494  \n",
              "15      17.476885        10.317768  \n",
              "16      30.826952        26.458811  \n",
              "17      58.720321        50.989352  \n",
              "18     124.472495       108.378811  \n",
              "19      76.005750        69.587656  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee181778-f1a0-473e-b82b-e32d4307fd7f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>strategy</th>\n",
              "      <th>total_correct</th>\n",
              "      <th>total_attempted</th>\n",
              "      <th>total_items</th>\n",
              "      <th>global_overall_success</th>\n",
              "      <th>global_attempt_success</th>\n",
              "      <th>global_coverage</th>\n",
              "      <th>global_inf_time_per_attempt</th>\n",
              "      <th>total_inf_time</th>\n",
              "      <th>mean_inf_time</th>\n",
              "      <th>median_inf_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>422</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.468889</td>\n",
              "      <td>0.468889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>101.477798</td>\n",
              "      <td>91330.018030</td>\n",
              "      <td>101.477798</td>\n",
              "      <td>97.070483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>379</td>\n",
              "      <td>811</td>\n",
              "      <td>900</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>0.467324</td>\n",
              "      <td>0.901111</td>\n",
              "      <td>26.336648</td>\n",
              "      <td>21359.021514</td>\n",
              "      <td>23.732246</td>\n",
              "      <td>18.867350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>379</td>\n",
              "      <td>868</td>\n",
              "      <td>900</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>0.436636</td>\n",
              "      <td>0.964444</td>\n",
              "      <td>170.554202</td>\n",
              "      <td>148041.047634</td>\n",
              "      <td>164.490053</td>\n",
              "      <td>151.672980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>rag</td>\n",
              "      <td>377</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.418889</td>\n",
              "      <td>0.418889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>60.705703</td>\n",
              "      <td>54635.132286</td>\n",
              "      <td>60.705703</td>\n",
              "      <td>56.027802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>376</td>\n",
              "      <td>827</td>\n",
              "      <td>900</td>\n",
              "      <td>0.417778</td>\n",
              "      <td>0.454655</td>\n",
              "      <td>0.918889</td>\n",
              "      <td>25.215631</td>\n",
              "      <td>20853.326453</td>\n",
              "      <td>23.170363</td>\n",
              "      <td>19.114528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>365</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.405556</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.291097</td>\n",
              "      <td>56961.987339</td>\n",
              "      <td>63.291097</td>\n",
              "      <td>59.663431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>364</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.404444</td>\n",
              "      <td>0.404444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>61.473006</td>\n",
              "      <td>55325.705601</td>\n",
              "      <td>61.473006</td>\n",
              "      <td>57.597412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>355</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.845617</td>\n",
              "      <td>22361.055521</td>\n",
              "      <td>24.845617</td>\n",
              "      <td>21.301638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>352</td>\n",
              "      <td>777</td>\n",
              "      <td>900</td>\n",
              "      <td>0.391111</td>\n",
              "      <td>0.453024</td>\n",
              "      <td>0.863333</td>\n",
              "      <td>137.642868</td>\n",
              "      <td>106948.508755</td>\n",
              "      <td>118.831676</td>\n",
              "      <td>107.985078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>347</td>\n",
              "      <td>782</td>\n",
              "      <td>900</td>\n",
              "      <td>0.385556</td>\n",
              "      <td>0.443734</td>\n",
              "      <td>0.868889</td>\n",
              "      <td>140.296614</td>\n",
              "      <td>109711.952215</td>\n",
              "      <td>121.766873</td>\n",
              "      <td>110.719337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>rag</td>\n",
              "      <td>346</td>\n",
              "      <td>779</td>\n",
              "      <td>900</td>\n",
              "      <td>0.384444</td>\n",
              "      <td>0.444159</td>\n",
              "      <td>0.865556</td>\n",
              "      <td>138.814449</td>\n",
              "      <td>108136.455578</td>\n",
              "      <td>120.151617</td>\n",
              "      <td>109.153378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>335</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.372222</td>\n",
              "      <td>0.471167</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>77.030258</td>\n",
              "      <td>54768.513441</td>\n",
              "      <td>60.853904</td>\n",
              "      <td>52.114822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>rag</td>\n",
              "      <td>328</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.364444</td>\n",
              "      <td>0.461322</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>78.941828</td>\n",
              "      <td>56127.639489</td>\n",
              "      <td>62.364044</td>\n",
              "      <td>52.649830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>325</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>39.339613</td>\n",
              "      <td>35405.651644</td>\n",
              "      <td>39.339613</td>\n",
              "      <td>32.792867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>301</td>\n",
              "      <td>750</td>\n",
              "      <td>900</td>\n",
              "      <td>0.334444</td>\n",
              "      <td>0.401333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>86.230075</td>\n",
              "      <td>64672.556501</td>\n",
              "      <td>71.858396</td>\n",
              "      <td>63.971494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>281</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.476885</td>\n",
              "      <td>15729.196182</td>\n",
              "      <td>17.476885</td>\n",
              "      <td>10.317768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>272</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.302222</td>\n",
              "      <td>0.302222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30.826952</td>\n",
              "      <td>27744.256730</td>\n",
              "      <td>30.826952</td>\n",
              "      <td>26.458811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>263</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.292222</td>\n",
              "      <td>0.369902</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>74.329520</td>\n",
              "      <td>52848.289075</td>\n",
              "      <td>58.720321</td>\n",
              "      <td>50.989352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>261</td>\n",
              "      <td>898</td>\n",
              "      <td>900</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.290646</td>\n",
              "      <td>0.997778</td>\n",
              "      <td>124.749717</td>\n",
              "      <td>112025.245633</td>\n",
              "      <td>124.472495</td>\n",
              "      <td>108.378811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>rag</td>\n",
              "      <td>259</td>\n",
              "      <td>894</td>\n",
              "      <td>900</td>\n",
              "      <td>0.287778</td>\n",
              "      <td>0.289709</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>76.515856</td>\n",
              "      <td>68405.174878</td>\n",
              "      <td>76.005750</td>\n",
              "      <td>69.587656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee181778-f1a0-473e-b82b-e32d4307fd7f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee181778-f1a0-473e-b82b-e32d4307fd7f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee181778-f1a0-473e-b82b-e32d4307fd7f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-091ecc3d-f578-4334-89c2-3d4cba7cbba5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-091ecc3d-f578-4334-89c2-3d4cba7cbba5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-091ecc3d-f578-4334-89c2-3d4cba7cbba5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"]])\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"deepseek-math-7b\",\n          \"mistral-7b-instruct\",\n          \"mathstral-7b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"cot_reasoning\",\n          \"nonconv_zeroshot\",\n          \"few_shots\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47,\n        \"min\": 259,\n        \"max\": 422,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          422,\n          364,\n          328\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_attempted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74,\n        \"min\": 711,\n        \"max\": 900,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          782,\n          900,\n          898\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_items\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 900,\n        \"max\": 900,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          900\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_overall_success\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05236359488113754,\n        \"min\": 0.2877777777777778,\n        \"max\": 0.4688888888888889,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          0.4688888888888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_attempt_success\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.062078562195023314,\n        \"min\": 0.2897091722595078,\n        \"max\": 0.4711673699015471,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.4688888888888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_coverage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08284376662499106,\n        \"min\": 0.79,\n        \"max\": 1.0,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.8688888888888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_inf_time_per_attempt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 45.29219171236732,\n        \"min\": 17.47688464668062,\n        \"max\": 170.55420234373636,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          101.47779781103134\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_inf_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37259.8469500573,\n        \"min\": 15729.196182012558,\n        \"max\": 148041.04763436317,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          91330.01802992821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_inf_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41.391136302964746,\n        \"min\": 17.47688464668062,\n        \"max\": 164.4900529270702,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          101.47779781103134\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_inf_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.82417921853697,\n        \"min\": 10.317768096923828,\n        \"max\": 151.67298030853271,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          97.07048308849335\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== üß© Ranking por DATASET (ordenado por overall_success) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      dataset                   model          strategy  correct  attempted  \\\n",
              "6   benchmark            mathstral-7b     cot_reasoning      422        900   \n",
              "3   benchmark        deepseek-math-7b  nonconv_zeroshot      379        811   \n",
              "18  benchmark  qwen2-math-7b-instruct     cot_reasoning      379        868   \n",
              "10  benchmark            mathstral-7b               rag      377        900   \n",
              "2   benchmark        deepseek-math-7b    nonconv_packed      376        827   \n",
              "11  benchmark            mathstral-7b         zero_shot      365        900   \n",
              "7   benchmark            mathstral-7b         few_shots      364        900   \n",
              "9   benchmark            mathstral-7b  nonconv_zeroshot      355        900   \n",
              "23  benchmark  qwen2-math-7b-instruct         zero_shot      352        777   \n",
              "19  benchmark  qwen2-math-7b-instruct         few_shots      347        782   \n",
              "22  benchmark  qwen2-math-7b-instruct               rag      346        779   \n",
              "1   benchmark        deepseek-math-7b         few_shots      335        711   \n",
              "4   benchmark        deepseek-math-7b               rag      328        711   \n",
              "20  benchmark  qwen2-math-7b-instruct    nonconv_packed      325        900   \n",
              "0   benchmark        deepseek-math-7b     cot_reasoning      301        750   \n",
              "21  benchmark  qwen2-math-7b-instruct  nonconv_zeroshot      281        900   \n",
              "8   benchmark            mathstral-7b    nonconv_packed      272        900   \n",
              "5   benchmark        deepseek-math-7b         zero_shot      263        711   \n",
              "12  benchmark     mistral-7b-instruct     cot_reasoning      261        898   \n",
              "16  benchmark     mistral-7b-instruct               rag      259        894   \n",
              "15  benchmark     mistral-7b-instruct  nonconv_zeroshot      243        900   \n",
              "17  benchmark     mistral-7b-instruct         zero_shot      233        894   \n",
              "14  benchmark     mistral-7b-instruct    nonconv_packed      215        900   \n",
              "13  benchmark     mistral-7b-instruct         few_shots      187        685   \n",
              "\n",
              "    total_items  overall_success  attempt_success  coverage  \\\n",
              "6           900         0.468889         0.468889  1.000000   \n",
              "3           900         0.421111         0.467324  0.901111   \n",
              "18          900         0.421111         0.436636  0.964444   \n",
              "10          900         0.418889         0.418889  1.000000   \n",
              "2           900         0.417778         0.454655  0.918889   \n",
              "11          900         0.405556         0.405556  1.000000   \n",
              "7           900         0.404444         0.404444  1.000000   \n",
              "9           900         0.394444         0.394444  1.000000   \n",
              "23          900         0.391111         0.453024  0.863333   \n",
              "19          900         0.385556         0.443734  0.868889   \n",
              "22          900         0.384444         0.444159  0.865556   \n",
              "1           900         0.372222         0.471167  0.790000   \n",
              "4           900         0.364444         0.461322  0.790000   \n",
              "20          900         0.361111         0.361111  1.000000   \n",
              "0           900         0.334444         0.401333  0.833333   \n",
              "21          900         0.312222         0.312222  1.000000   \n",
              "8           900         0.302222         0.302222  1.000000   \n",
              "5           900         0.292222         0.369902  0.790000   \n",
              "12          900         0.290000         0.290646  0.997778   \n",
              "16          900         0.287778         0.289709  0.993333   \n",
              "15          900         0.270000         0.270000  1.000000   \n",
              "17          900         0.258889         0.260626  0.993333   \n",
              "14          900         0.238889         0.238889  1.000000   \n",
              "13          900         0.207778         0.272993  0.761111   \n",
              "\n",
              "    inf_time_per_attempt  inf_time_mean  inf_time_median   inf_time_sum  \n",
              "6             101.477798     101.477798        97.070483   91330.018030  \n",
              "3              26.336648      23.732246        18.867350   21359.021514  \n",
              "18            170.554202     164.490053       151.672980  148041.047634  \n",
              "10             60.705703      60.705703        56.027802   54635.132286  \n",
              "2              25.215631      23.170363        19.114528   20853.326453  \n",
              "11             63.291097      63.291097        59.663431   56961.987339  \n",
              "7              61.473006      61.473006        57.597412   55325.705601  \n",
              "9              24.845617      24.845617        21.301638   22361.055521  \n",
              "23            137.642868     118.831676       107.985078  106948.508755  \n",
              "19            140.296614     121.766873       110.719337  109711.952215  \n",
              "22            138.814449     120.151617       109.153378  108136.455578  \n",
              "1              77.030258      60.853904        52.114822   54768.513441  \n",
              "4              78.941828      62.364044        52.649830   56127.639489  \n",
              "20             39.339613      39.339613        32.792867   35405.651644  \n",
              "0              86.230075      71.858396        63.971494   64672.556501  \n",
              "21             17.476885      17.476885        10.317768   15729.196182  \n",
              "8              30.826952      30.826952        26.458811   27744.256730  \n",
              "5              74.329520      58.720321        50.989352   52848.289075  \n",
              "12            124.749717     124.472495       108.378811  112025.245633  \n",
              "16             76.515856      76.005750        69.587656   68405.174878  \n",
              "15             17.905902      17.905902        13.280128   16115.312016  \n",
              "17             78.418835      77.896043        71.100954   70106.438343  \n",
              "14             21.257168      21.257168        15.216154   19131.451360  \n",
              "13             76.891666      76.334480        69.443840   52670.791081  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95327afc-7d31-424e-b8a9-a93ddfdcf1ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>model</th>\n",
              "      <th>strategy</th>\n",
              "      <th>correct</th>\n",
              "      <th>attempted</th>\n",
              "      <th>total_items</th>\n",
              "      <th>overall_success</th>\n",
              "      <th>attempt_success</th>\n",
              "      <th>coverage</th>\n",
              "      <th>inf_time_per_attempt</th>\n",
              "      <th>inf_time_mean</th>\n",
              "      <th>inf_time_median</th>\n",
              "      <th>inf_time_sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>422</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.468889</td>\n",
              "      <td>0.468889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>101.477798</td>\n",
              "      <td>101.477798</td>\n",
              "      <td>97.070483</td>\n",
              "      <td>91330.018030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>379</td>\n",
              "      <td>811</td>\n",
              "      <td>900</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>0.467324</td>\n",
              "      <td>0.901111</td>\n",
              "      <td>26.336648</td>\n",
              "      <td>23.732246</td>\n",
              "      <td>18.867350</td>\n",
              "      <td>21359.021514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>379</td>\n",
              "      <td>868</td>\n",
              "      <td>900</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>0.436636</td>\n",
              "      <td>0.964444</td>\n",
              "      <td>170.554202</td>\n",
              "      <td>164.490053</td>\n",
              "      <td>151.672980</td>\n",
              "      <td>148041.047634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>rag</td>\n",
              "      <td>377</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.418889</td>\n",
              "      <td>0.418889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>60.705703</td>\n",
              "      <td>60.705703</td>\n",
              "      <td>56.027802</td>\n",
              "      <td>54635.132286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>376</td>\n",
              "      <td>827</td>\n",
              "      <td>900</td>\n",
              "      <td>0.417778</td>\n",
              "      <td>0.454655</td>\n",
              "      <td>0.918889</td>\n",
              "      <td>25.215631</td>\n",
              "      <td>23.170363</td>\n",
              "      <td>19.114528</td>\n",
              "      <td>20853.326453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>365</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.405556</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.291097</td>\n",
              "      <td>63.291097</td>\n",
              "      <td>59.663431</td>\n",
              "      <td>56961.987339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>364</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.404444</td>\n",
              "      <td>0.404444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>61.473006</td>\n",
              "      <td>61.473006</td>\n",
              "      <td>57.597412</td>\n",
              "      <td>55325.705601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>355</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.845617</td>\n",
              "      <td>24.845617</td>\n",
              "      <td>21.301638</td>\n",
              "      <td>22361.055521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>352</td>\n",
              "      <td>777</td>\n",
              "      <td>900</td>\n",
              "      <td>0.391111</td>\n",
              "      <td>0.453024</td>\n",
              "      <td>0.863333</td>\n",
              "      <td>137.642868</td>\n",
              "      <td>118.831676</td>\n",
              "      <td>107.985078</td>\n",
              "      <td>106948.508755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>347</td>\n",
              "      <td>782</td>\n",
              "      <td>900</td>\n",
              "      <td>0.385556</td>\n",
              "      <td>0.443734</td>\n",
              "      <td>0.868889</td>\n",
              "      <td>140.296614</td>\n",
              "      <td>121.766873</td>\n",
              "      <td>110.719337</td>\n",
              "      <td>109711.952215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>rag</td>\n",
              "      <td>346</td>\n",
              "      <td>779</td>\n",
              "      <td>900</td>\n",
              "      <td>0.384444</td>\n",
              "      <td>0.444159</td>\n",
              "      <td>0.865556</td>\n",
              "      <td>138.814449</td>\n",
              "      <td>120.151617</td>\n",
              "      <td>109.153378</td>\n",
              "      <td>108136.455578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>335</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.372222</td>\n",
              "      <td>0.471167</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>77.030258</td>\n",
              "      <td>60.853904</td>\n",
              "      <td>52.114822</td>\n",
              "      <td>54768.513441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>rag</td>\n",
              "      <td>328</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.364444</td>\n",
              "      <td>0.461322</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>78.941828</td>\n",
              "      <td>62.364044</td>\n",
              "      <td>52.649830</td>\n",
              "      <td>56127.639489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>325</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>39.339613</td>\n",
              "      <td>39.339613</td>\n",
              "      <td>32.792867</td>\n",
              "      <td>35405.651644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>301</td>\n",
              "      <td>750</td>\n",
              "      <td>900</td>\n",
              "      <td>0.334444</td>\n",
              "      <td>0.401333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>86.230075</td>\n",
              "      <td>71.858396</td>\n",
              "      <td>63.971494</td>\n",
              "      <td>64672.556501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>qwen2-math-7b-instruct</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>281</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.476885</td>\n",
              "      <td>17.476885</td>\n",
              "      <td>10.317768</td>\n",
              "      <td>15729.196182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mathstral-7b</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>272</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.302222</td>\n",
              "      <td>0.302222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30.826952</td>\n",
              "      <td>30.826952</td>\n",
              "      <td>26.458811</td>\n",
              "      <td>27744.256730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>deepseek-math-7b</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>263</td>\n",
              "      <td>711</td>\n",
              "      <td>900</td>\n",
              "      <td>0.292222</td>\n",
              "      <td>0.369902</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>74.329520</td>\n",
              "      <td>58.720321</td>\n",
              "      <td>50.989352</td>\n",
              "      <td>52848.289075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>cot_reasoning</td>\n",
              "      <td>261</td>\n",
              "      <td>898</td>\n",
              "      <td>900</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.290646</td>\n",
              "      <td>0.997778</td>\n",
              "      <td>124.749717</td>\n",
              "      <td>124.472495</td>\n",
              "      <td>108.378811</td>\n",
              "      <td>112025.245633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>rag</td>\n",
              "      <td>259</td>\n",
              "      <td>894</td>\n",
              "      <td>900</td>\n",
              "      <td>0.287778</td>\n",
              "      <td>0.289709</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>76.515856</td>\n",
              "      <td>76.005750</td>\n",
              "      <td>69.587656</td>\n",
              "      <td>68405.174878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>nonconv_zeroshot</td>\n",
              "      <td>243</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.905902</td>\n",
              "      <td>17.905902</td>\n",
              "      <td>13.280128</td>\n",
              "      <td>16115.312016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>zero_shot</td>\n",
              "      <td>233</td>\n",
              "      <td>894</td>\n",
              "      <td>900</td>\n",
              "      <td>0.258889</td>\n",
              "      <td>0.260626</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>78.418835</td>\n",
              "      <td>77.896043</td>\n",
              "      <td>71.100954</td>\n",
              "      <td>70106.438343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>nonconv_packed</td>\n",
              "      <td>215</td>\n",
              "      <td>900</td>\n",
              "      <td>900</td>\n",
              "      <td>0.238889</td>\n",
              "      <td>0.238889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.257168</td>\n",
              "      <td>21.257168</td>\n",
              "      <td>15.216154</td>\n",
              "      <td>19131.451360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>mistral-7b-instruct</td>\n",
              "      <td>few_shots</td>\n",
              "      <td>187</td>\n",
              "      <td>685</td>\n",
              "      <td>900</td>\n",
              "      <td>0.207778</td>\n",
              "      <td>0.272993</td>\n",
              "      <td>0.761111</td>\n",
              "      <td>76.891666</td>\n",
              "      <td>76.334480</td>\n",
              "      <td>69.443840</td>\n",
              "      <td>52670.791081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95327afc-7d31-424e-b8a9-a93ddfdcf1ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95327afc-7d31-424e-b8a9-a93ddfdcf1ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95327afc-7d31-424e-b8a9-a93ddfdcf1ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-297aedb3-16fe-4e68-a6a6-3f5c51afbc3d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-297aedb3-16fe-4e68-a6a6-3f5c51afbc3d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-297aedb3-16fe-4e68-a6a6-3f5c51afbc3d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"]])\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"benchmark\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"deepseek-math-7b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"strategy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"cot_reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 61,\n        \"min\": 187,\n        \"max\": 422,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          272\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attempted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 77,\n        \"min\": 685,\n        \"max\": 900,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          894\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_items\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 900,\n        \"max\": 900,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          900\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overall_success\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0687195824804108,\n        \"min\": 0.20777777777777778,\n        \"max\": 0.4688888888888889,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.3022222222222222\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attempt_success\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07832303591717651,\n        \"min\": 0.2388888888888889,\n        \"max\": 0.4711673699015471,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.45302445302445304\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coverage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08666661505428085,\n        \"min\": 0.7611111111111111,\n        \"max\": 1.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.9933333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_per_attempt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44.32835959397262,\n        \"min\": 17.47688464668062,\n        \"max\": 170.55420234373636,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          137.64286841051305\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40.444845906605245,\n        \"min\": 17.47688464668062,\n        \"max\": 164.4900529270702,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          118.8316763944096\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38.06781266054024,\n        \"min\": 10.317768096923828,\n        \"max\": 151.67298030853271,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          107.98507785797119\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_time_sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36401.8672137949,\n        \"min\": 15729.196182012558,\n        \"max\": 148041.04763436317,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          106948.50875496864\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# AN√ÅLISIS ESTAD√çSTICO: COCHRAN'S Q y McNEMAR (penalizando no-cobertura)\n",
        "# =========================\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.contingency_tables import cochrans_q, mcnemar\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/tesis/experiments_final\"\n",
        "\n",
        "# 1) Cargar results_*.csv\n",
        "csv_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"results_*.csv\")))\n",
        "if not csv_paths:\n",
        "    raise FileNotFoundError(f\"No se encontraron CSVs de resultados en {OUT_DIR}\")\n",
        "\n",
        "dfs = []\n",
        "for p in csv_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(p)\n",
        "        df[\"__source_file\"] = os.path.basename(p)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error leyendo {p}: {e}\")\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"No se pudo cargar ning√∫n CSV v√°lido.\")\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# (opcional) limita al benchmark\n",
        "# all_df = all_df[all_df[\"dataset\"] == \"benchmark\"].copy()\n",
        "\n",
        "# 2) Campos m√≠nimos y tipado\n",
        "for c in [\"row_id\",\"dataset\",\"model\",\"strategy\",\"is_correct\"]:\n",
        "    if c not in all_df.columns:\n",
        "        all_df[c] = None\n",
        "# is_correct -> bool/int\n",
        "all_df[\"is_correct\"] = all_df[\"is_correct\"].fillna(False).astype(bool).astype(int)\n",
        "\n",
        "# 3) Construir \"pair\" y la grilla completa row_id √ó pair para penalizar no-cobertura\n",
        "all_df[\"pair\"] = all_df[\"model\"].astype(str) + \"_\" + all_df[\"strategy\"].astype(str)\n",
        "row_ids = all_df[\"row_id\"].astype(str).unique()\n",
        "pairs   = all_df[\"pair\"].unique()\n",
        "\n",
        "pivot = (all_df\n",
        "         .pivot_table(index=\"row_id\", columns=\"pair\", values=\"is_correct\", aggfunc=\"first\"))\n",
        "# reindex a la grilla completa y falta -> 0\n",
        "pivot = pivot.reindex(index=row_ids, columns=pairs).fillna(0).astype(int)\n",
        "\n",
        "print(f\"‚úÖ Matriz creada con {pivot.shape[0]} problemas y {pivot.shape[1]} combinaciones.\")\n",
        "\n",
        "# 4) Cochran‚Äôs Q (test global)\n",
        "print(\"\\n=== üßÆ COCHRAN'S Q TEST ===\")\n",
        "q_res = cochrans_q(pivot.to_numpy())\n",
        "print(f\"Q = {q_res.statistic:.4f}, p = {q_res.pvalue:.6f}\")\n",
        "if q_res.pvalue < 0.05:\n",
        "    print(\"‚Üí Hay diferencias significativas entre combinaciones (rechaza H0).\")\n",
        "else:\n",
        "    print(\"‚Üí No hay evidencia suficiente de diferencias globales (no se rechaza H0).\")\n",
        "\n",
        "# 5) McNemar pareado + correcci√≥n Holm\n",
        "print(\"\\n=== ‚öñÔ∏è MCNEMAR PAIRWISE TESTS ===\")\n",
        "pairs_list = list(pivot.columns)\n",
        "pw = []\n",
        "for i in range(len(pairs_list)):\n",
        "    for j in range(i+1, len(pairs_list)):\n",
        "        a, b = pairs_list[i], pairs_list[j]\n",
        "        tbl = pd.crosstab(pivot[a], pivot[b])  # 2x2\n",
        "        if tbl.shape == (2,2):\n",
        "            res = mcnemar(tbl, exact=False, correction=True)\n",
        "            pw.append({\"A\": a, \"B\": b, \"stat\": float(res.statistic), \"p\": float(res.pvalue)})\n",
        "\n",
        "pairwise_df = pd.DataFrame(pw).sort_values(\"p\").reset_index(drop=True)\n",
        "if not pairwise_df.empty:\n",
        "    # Holm‚ÄìBonferroni\n",
        "    rej, p_adj, *_ = multipletests(pairwise_df[\"p\"].values, method=\"holm\")\n",
        "    pairwise_df[\"p_holm\"] = p_adj\n",
        "    pairwise_df[\"signif_0.05\"] = rej\n",
        "\n",
        "    print(\"\\nParejas significativas (p_holm < 0.05):\")\n",
        "    display(pairwise_df[pairwise_df[\"p_holm\"] < 0.05])\n",
        "    print(\"\\nResumen completo (ordenado por p cruda):\")\n",
        "    display(pairwise_df)\n",
        "else:\n",
        "    print(\"No hay pares comparables (matriz vac√≠a o columnas insuficientes).\")\n",
        "\n",
        "# 6) ‚ÄúMejor combinaci√≥n‚Äù por tasa de acierto (promedio de la columna)\n",
        "acc_by_pair = pivot.mean(axis=0).sort_values(ascending=False)\n",
        "best_pair   = acc_by_pair.index[0]\n",
        "best_score  = acc_by_pair.iloc[0]\n",
        "print(\"\\n=== üèÜ MEJOR COMBINACI√ìN (por proporci√≥n de aciertos penalizando no-cobertura) ===\")\n",
        "print(f\"{best_pair}: accuracy = {best_score:.4f}\")\n",
        "\n",
        "# (Opcional) tabla ordenada\n",
        "rank_table = acc_by_pair.reset_index()\n",
        "rank_table.columns = [\"pair\", \"accuracy\"]\n",
        "display(rank_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QVCGy2bPQjA3",
        "outputId": "f3883776-a9e9-41a2-e7cd-c87952a93bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Matriz creada con 900 problemas y 24 combinaciones.\n",
            "\n",
            "=== üßÆ COCHRAN'S Q TEST ===\n",
            "Q = 678.7573, p = 0.000000\n",
            "‚Üí Hay diferencias significativas entre combinaciones (rechaza H0).\n",
            "\n",
            "=== ‚öñÔ∏è MCNEMAR PAIRWISE TESTS ===\n",
            "\n",
            "Parejas significativas (p_holm < 0.05):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                         A  \\\n",
              "0               mathstral-7b_cot_reasoning   \n",
              "1               mathstral-7b_cot_reasoning   \n",
              "2               mathstral-7b_cot_reasoning   \n",
              "3               mathstral-7b_cot_reasoning   \n",
              "4       mistral-7b-instruct_nonconv_packed   \n",
              "..                                     ...   \n",
              "138             deepseek-math-7b_few_shots   \n",
              "139            mathstral-7b_nonconv_packed   \n",
              "140  qwen2-math-7b-instruct_nonconv_packed   \n",
              "141         deepseek-math-7b_cot_reasoning   \n",
              "142      deepseek-math-7b_nonconv_zeroshot   \n",
              "\n",
              "                                        B        stat             p  \\\n",
              "0      mistral-7b-instruct_nonconv_packed  165.120623  8.609151e-38   \n",
              "1           mistral-7b-instruct_zero_shot  131.390335  2.034066e-30   \n",
              "2    mistral-7b-instruct_nonconv_zeroshot  129.322449  5.764936e-30   \n",
              "3           mistral-7b-instruct_few_shots  116.964000  2.923258e-27   \n",
              "4       deepseek-math-7b_nonconv_zeroshot  114.521552  1.001716e-26   \n",
              "..                                    ...         ...           ...   \n",
              "138           mathstral-7b_nonconv_packed   16.497854  4.870517e-05   \n",
              "139    mistral-7b-instruct_nonconv_packed   15.297561  9.183501e-05   \n",
              "140            deepseek-math-7b_zero_shot   15.250000  9.417676e-05   \n",
              "141  mistral-7b-instruct_nonconv_zeroshot   13.425620  2.482110e-04   \n",
              "142                  deepseek-math-7b_rag   12.953368  3.193465e-04   \n",
              "\n",
              "           p_holm  signif_0.05  \n",
              "0    2.376126e-35         True  \n",
              "1    5.593681e-28         True  \n",
              "2    1.579592e-27         True  \n",
              "3    7.980495e-25         True  \n",
              "4    2.724668e-24         True  \n",
              "..            ...          ...  \n",
              "138  6.721313e-03         True  \n",
              "139  1.258140e-02         True  \n",
              "140  1.280804e-02         True  \n",
              "141  3.350848e-02         True  \n",
              "142  4.279243e-02         True  \n",
              "\n",
              "[143 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-470cc64b-9145-403b-bd4d-3597690f11bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>stat</th>\n",
              "      <th>p</th>\n",
              "      <th>p_holm</th>\n",
              "      <th>signif_0.05</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_nonconv_packed</td>\n",
              "      <td>165.120623</td>\n",
              "      <td>8.609151e-38</td>\n",
              "      <td>2.376126e-35</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_zero_shot</td>\n",
              "      <td>131.390335</td>\n",
              "      <td>2.034066e-30</td>\n",
              "      <td>5.593681e-28</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_nonconv_zeroshot</td>\n",
              "      <td>129.322449</td>\n",
              "      <td>5.764936e-30</td>\n",
              "      <td>1.579592e-27</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_few_shots</td>\n",
              "      <td>116.964000</td>\n",
              "      <td>2.923258e-27</td>\n",
              "      <td>7.980495e-25</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mistral-7b-instruct_nonconv_packed</td>\n",
              "      <td>deepseek-math-7b_nonconv_zeroshot</td>\n",
              "      <td>114.521552</td>\n",
              "      <td>1.001716e-26</td>\n",
              "      <td>2.724668e-24</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>deepseek-math-7b_few_shots</td>\n",
              "      <td>mathstral-7b_nonconv_packed</td>\n",
              "      <td>16.497854</td>\n",
              "      <td>4.870517e-05</td>\n",
              "      <td>6.721313e-03</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>mathstral-7b_nonconv_packed</td>\n",
              "      <td>mistral-7b-instruct_nonconv_packed</td>\n",
              "      <td>15.297561</td>\n",
              "      <td>9.183501e-05</td>\n",
              "      <td>1.258140e-02</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>qwen2-math-7b-instruct_nonconv_packed</td>\n",
              "      <td>deepseek-math-7b_zero_shot</td>\n",
              "      <td>15.250000</td>\n",
              "      <td>9.417676e-05</td>\n",
              "      <td>1.280804e-02</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>deepseek-math-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_nonconv_zeroshot</td>\n",
              "      <td>13.425620</td>\n",
              "      <td>2.482110e-04</td>\n",
              "      <td>3.350848e-02</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>deepseek-math-7b_nonconv_zeroshot</td>\n",
              "      <td>deepseek-math-7b_rag</td>\n",
              "      <td>12.953368</td>\n",
              "      <td>3.193465e-04</td>\n",
              "      <td>4.279243e-02</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>143 rows √ó 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-470cc64b-9145-403b-bd4d-3597690f11bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-470cc64b-9145-403b-bd4d-3597690f11bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-470cc64b-9145-403b-bd4d-3597690f11bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5c27cfc4-8a21-4c29-aae7-b32d000fc969\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c27cfc4-8a21-4c29-aae7-b32d000fc969')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5c27cfc4-8a21-4c29-aae7-b32d000fc969 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(rank_table)\",\n  \"rows\": 143,\n  \"fields\": [\n    {\n      \"column\": \"A\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"mistral-7b-instruct_rag\",\n          \"mistral-7b-instruct_few_shots\",\n          \"mathstral-7b_cot_reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"qwen2-math-7b-instruct_rag\",\n          \"mathstral-7b_nonconv_packed\",\n          \"mistral-7b-instruct_nonconv_packed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.149839689711644,\n        \"min\": 12.953367875647668,\n        \"max\": 165.12062256809338,\n        \"num_unique_values\": 141,\n        \"samples\": [\n          44.062745098039215,\n          25.041984732824428,\n          67.43333333333334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.602232585701569e-05,\n        \"min\": 8.609150577497617e-38,\n        \"max\": 0.00031934647649309664,\n        \"num_unique_values\": 141,\n        \"samples\": [\n          3.180164235729878e-11,\n          5.609545249031417e-07,\n          2.1793302509126482e-16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_holm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004858543107117592,\n        \"min\": 2.3761255593893422e-35,\n        \"max\": 0.04279242785007495,\n        \"num_unique_values\": 138,\n        \"samples\": [\n          1.5858217460457538e-06,\n          9.311845113392152e-05,\n          7.43921909367362e-20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"signif_0.05\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumen completo (ordenado por p cruda):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                        A  \\\n",
              "0              mathstral-7b_cot_reasoning   \n",
              "1              mathstral-7b_cot_reasoning   \n",
              "2              mathstral-7b_cot_reasoning   \n",
              "3              mathstral-7b_cot_reasoning   \n",
              "4      mistral-7b-instruct_nonconv_packed   \n",
              "..                                    ...   \n",
              "271  qwen2-math-7b-instruct_cot_reasoning   \n",
              "272  qwen2-math-7b-instruct_cot_reasoning   \n",
              "273                mathstral-7b_few_shots   \n",
              "274       deepseek-math-7b_nonconv_packed   \n",
              "275      qwen2-math-7b-instruct_few_shots   \n",
              "\n",
              "                                        B        stat             p  \\\n",
              "0      mistral-7b-instruct_nonconv_packed  165.120623  8.609151e-38   \n",
              "1           mistral-7b-instruct_zero_shot  131.390335  2.034066e-30   \n",
              "2    mistral-7b-instruct_nonconv_zeroshot  129.322449  5.764936e-30   \n",
              "3           mistral-7b-instruct_few_shots  116.964000  2.923258e-27   \n",
              "4       deepseek-math-7b_nonconv_zeroshot  114.521552  1.001716e-26   \n",
              "..                                    ...         ...           ...   \n",
              "271                      mathstral-7b_rag    0.003906  9.501647e-01   \n",
              "272     deepseek-math-7b_nonconv_zeroshot    0.003817  9.507379e-01   \n",
              "273                mathstral-7b_zero_shot    0.000000  1.000000e+00   \n",
              "274                      mathstral-7b_rag    0.000000  1.000000e+00   \n",
              "275            qwen2-math-7b-instruct_rag    0.000000  1.000000e+00   \n",
              "\n",
              "           p_holm  signif_0.05  \n",
              "0    2.376126e-35         True  \n",
              "1    5.593681e-28         True  \n",
              "2    1.579592e-27         True  \n",
              "3    7.980495e-25         True  \n",
              "4    2.724668e-24         True  \n",
              "..            ...          ...  \n",
              "271  1.000000e+00        False  \n",
              "272  1.000000e+00        False  \n",
              "273  1.000000e+00        False  \n",
              "274  1.000000e+00        False  \n",
              "275  1.000000e+00        False  \n",
              "\n",
              "[276 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a751022-637f-472c-9179-dd017d6d86f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>stat</th>\n",
              "      <th>p</th>\n",
              "      <th>p_holm</th>\n",
              "      <th>signif_0.05</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_nonconv_packed</td>\n",
              "      <td>165.120623</td>\n",
              "      <td>8.609151e-38</td>\n",
              "      <td>2.376126e-35</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_zero_shot</td>\n",
              "      <td>131.390335</td>\n",
              "      <td>2.034066e-30</td>\n",
              "      <td>5.593681e-28</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_nonconv_zeroshot</td>\n",
              "      <td>129.322449</td>\n",
              "      <td>5.764936e-30</td>\n",
              "      <td>1.579592e-27</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>mistral-7b-instruct_few_shots</td>\n",
              "      <td>116.964000</td>\n",
              "      <td>2.923258e-27</td>\n",
              "      <td>7.980495e-25</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mistral-7b-instruct_nonconv_packed</td>\n",
              "      <td>deepseek-math-7b_nonconv_zeroshot</td>\n",
              "      <td>114.521552</td>\n",
              "      <td>1.001716e-26</td>\n",
              "      <td>2.724668e-24</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>qwen2-math-7b-instruct_cot_reasoning</td>\n",
              "      <td>mathstral-7b_rag</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>9.501647e-01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>qwen2-math-7b-instruct_cot_reasoning</td>\n",
              "      <td>deepseek-math-7b_nonconv_zeroshot</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>9.507379e-01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>mathstral-7b_few_shots</td>\n",
              "      <td>mathstral-7b_zero_shot</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>deepseek-math-7b_nonconv_packed</td>\n",
              "      <td>mathstral-7b_rag</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>qwen2-math-7b-instruct_few_shots</td>\n",
              "      <td>qwen2-math-7b-instruct_rag</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>276 rows √ó 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a751022-637f-472c-9179-dd017d6d86f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a751022-637f-472c-9179-dd017d6d86f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a751022-637f-472c-9179-dd017d6d86f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-99b934cd-1298-4393-b889-ea814b4309f8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99b934cd-1298-4393-b889-ea814b4309f8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-99b934cd-1298-4393-b889-ea814b4309f8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ceb7990a-c41f-45fc-82bc-d921cb7f46a5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pairwise_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ceb7990a-c41f-45fc-82bc-d921cb7f46a5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pairwise_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pairwise_df",
              "summary": "{\n  \"name\": \"pairwise_df\",\n  \"rows\": 276,\n  \"fields\": [\n    {\n      \"column\": \"A\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"mistral-7b-instruct_rag\",\n          \"mistral-7b-instruct_few_shots\",\n          \"mathstral-7b_cot_reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"qwen2-math-7b-instruct_rag\",\n          \"mathstral-7b_nonconv_packed\",\n          \"mistral-7b-instruct_nonconv_packed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31.257481296461414,\n        \"min\": 0.0,\n        \"max\": 165.12062256809338,\n        \"num_unique_values\": 269,\n        \"samples\": [\n          69.71904761904761,\n          22.629770992366414,\n          38.900862068965516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23926687585447257,\n        \"min\": 8.609150577497617e-38,\n        \"max\": 1.0,\n        \"num_unique_values\": 269,\n        \"samples\": [\n          6.838255080119876e-17,\n          1.9641734638718855e-06,\n          4.458836571548798e-10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_holm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.46922833597062474,\n        \"min\": 2.3761255593893422e-35,\n        \"max\": 1.0,\n        \"num_unique_values\": 169,\n        \"samples\": [\n          0.06311721536268372,\n          1.6753724946293696e-14,\n          0.0007051262834170545\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"signif_0.05\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== üèÜ MEJOR COMBINACI√ìN (por proporci√≥n de aciertos penalizando no-cobertura) ===\n",
            "mathstral-7b_cot_reasoning: accuracy = 0.4689\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       pair  accuracy\n",
              "0                mathstral-7b_cot_reasoning  0.468889\n",
              "1      qwen2-math-7b-instruct_cot_reasoning  0.421111\n",
              "2         deepseek-math-7b_nonconv_zeroshot  0.421111\n",
              "3                          mathstral-7b_rag  0.418889\n",
              "4           deepseek-math-7b_nonconv_packed  0.417778\n",
              "5                    mathstral-7b_zero_shot  0.405556\n",
              "6                    mathstral-7b_few_shots  0.404444\n",
              "7             mathstral-7b_nonconv_zeroshot  0.394444\n",
              "8          qwen2-math-7b-instruct_zero_shot  0.391111\n",
              "9          qwen2-math-7b-instruct_few_shots  0.385556\n",
              "10               qwen2-math-7b-instruct_rag  0.384444\n",
              "11               deepseek-math-7b_few_shots  0.372222\n",
              "12                     deepseek-math-7b_rag  0.364444\n",
              "13    qwen2-math-7b-instruct_nonconv_packed  0.361111\n",
              "14           deepseek-math-7b_cot_reasoning  0.334444\n",
              "15  qwen2-math-7b-instruct_nonconv_zeroshot  0.312222\n",
              "16              mathstral-7b_nonconv_packed  0.302222\n",
              "17               deepseek-math-7b_zero_shot  0.292222\n",
              "18        mistral-7b-instruct_cot_reasoning  0.290000\n",
              "19                  mistral-7b-instruct_rag  0.287778\n",
              "20            mistral-7b-instruct_few_shots  0.277778\n",
              "21     mistral-7b-instruct_nonconv_zeroshot  0.270000\n",
              "22            mistral-7b-instruct_zero_shot  0.258889\n",
              "23       mistral-7b-instruct_nonconv_packed  0.238889"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e11d108b-9563-4f6d-b0fa-21c5d79b93e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mathstral-7b_cot_reasoning</td>\n",
              "      <td>0.468889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qwen2-math-7b-instruct_cot_reasoning</td>\n",
              "      <td>0.421111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>deepseek-math-7b_nonconv_zeroshot</td>\n",
              "      <td>0.421111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mathstral-7b_rag</td>\n",
              "      <td>0.418889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>deepseek-math-7b_nonconv_packed</td>\n",
              "      <td>0.417778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mathstral-7b_zero_shot</td>\n",
              "      <td>0.405556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mathstral-7b_few_shots</td>\n",
              "      <td>0.404444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mathstral-7b_nonconv_zeroshot</td>\n",
              "      <td>0.394444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>qwen2-math-7b-instruct_zero_shot</td>\n",
              "      <td>0.391111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>qwen2-math-7b-instruct_few_shots</td>\n",
              "      <td>0.385556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>qwen2-math-7b-instruct_rag</td>\n",
              "      <td>0.384444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>deepseek-math-7b_few_shots</td>\n",
              "      <td>0.372222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>deepseek-math-7b_rag</td>\n",
              "      <td>0.364444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>qwen2-math-7b-instruct_nonconv_packed</td>\n",
              "      <td>0.361111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>deepseek-math-7b_cot_reasoning</td>\n",
              "      <td>0.334444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>qwen2-math-7b-instruct_nonconv_zeroshot</td>\n",
              "      <td>0.312222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>mathstral-7b_nonconv_packed</td>\n",
              "      <td>0.302222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>deepseek-math-7b_zero_shot</td>\n",
              "      <td>0.292222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mistral-7b-instruct_cot_reasoning</td>\n",
              "      <td>0.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mistral-7b-instruct_rag</td>\n",
              "      <td>0.287778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>mistral-7b-instruct_few_shots</td>\n",
              "      <td>0.277778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>mistral-7b-instruct_nonconv_zeroshot</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>mistral-7b-instruct_zero_shot</td>\n",
              "      <td>0.258889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>mistral-7b-instruct_nonconv_packed</td>\n",
              "      <td>0.238889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e11d108b-9563-4f6d-b0fa-21c5d79b93e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e11d108b-9563-4f6d-b0fa-21c5d79b93e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e11d108b-9563-4f6d-b0fa-21c5d79b93e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-35849884-8c47-402c-a2f8-7a50ea5d9a30\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35849884-8c47-402c-a2f8-7a50ea5d9a30')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-35849884-8c47-402c-a2f8-7a50ea5d9a30 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_7a7aaf61-301e-4102-b46b-ffe1045d0a72\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('rank_table')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7a7aaf61-301e-4102-b46b-ffe1045d0a72 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('rank_table');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "rank_table",
              "summary": "{\n  \"name\": \"rank_table\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"pair\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"qwen2-math-7b-instruct_zero_shot\",\n          \"mathstral-7b_nonconv_packed\",\n          \"mathstral-7b_cot_reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06371372051719705,\n        \"min\": 0.2388888888888889,\n        \"max\": 0.4688888888888889,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.3022222222222222,\n          0.3844444444444444,\n          0.4688888888888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Mejor combinaci√≥n (modelo + estrategia) penalizando no-cobertura\n",
        "# ===============================\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/tesis/experiments_final\"\n",
        "\n",
        "# 1) Cargar todos los CSV de resultados\n",
        "csv_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"results_*.csv\")))\n",
        "if not csv_paths:\n",
        "    raise FileNotFoundError(f\"No se encontraron CSVs en {OUT_DIR}\")\n",
        "\n",
        "dfs = []\n",
        "for p in csv_paths:\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            p,\n",
        "            usecols=[\"row_id\",\"dataset\",\"model\",\"strategy\",\"is_correct\"]\n",
        "        )\n",
        "        df[\"__source_file\"] = os.path.basename(p)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error leyendo {p}: {e}\")\n",
        "\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"No se pudo cargar ning√∫n CSV v√°lido.\")\n",
        "\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# 2) Tipado y columna combinada\n",
        "all_df[\"row_id\"]   = all_df[\"row_id\"].astype(str)\n",
        "all_df[\"dataset\"]  = all_df[\"dataset\"].astype(str)\n",
        "all_df[\"model\"]    = all_df[\"model\"].astype(str)\n",
        "all_df[\"strategy\"] = all_df[\"strategy\"].astype(str)\n",
        "all_df[\"pair\"]     = all_df[\"model\"] + \"_\" + all_df[\"strategy\"]\n",
        "\n",
        "# is_correct -> bool/int (NaN -> False)\n",
        "all_df[\"is_correct\"] = all_df[\"is_correct\"].fillna(False).astype(bool).astype(int)\n",
        "\n",
        "# 3) Ranking por dataset (penaliza no-cobertura construyendo la grilla completa)\n",
        "pairs_global = sorted(all_df[\"pair\"].unique())\n",
        "per_dataset_rows = []\n",
        "\n",
        "for ds, g in all_df.groupby(\"dataset\", dropna=False):\n",
        "    row_ids = sorted(g[\"row_id\"].unique())\n",
        "    # pivot de aciertos (0/1) y reindexar a grilla completa row_id √ó pair\n",
        "    piv = (g.pivot_table(index=\"row_id\", columns=\"pair\", values=\"is_correct\", aggfunc=\"first\")\n",
        "             .reindex(index=row_ids, columns=pairs_global)\n",
        "             .fillna(0).astype(int))\n",
        "\n",
        "    # m√©tricas por par dentro del dataset\n",
        "    correct_total = piv.sum(axis=0)                    # aciertos totales del par en este dataset\n",
        "    total_items   = len(row_ids)                       # n√∫mero de problemas del dataset\n",
        "    overall_succ  = correct_total / total_items        # proporci√≥n de aciertos penalizando no-cobertura\n",
        "\n",
        "    tmp = (pd.DataFrame({\n",
        "            \"dataset\": ds,\n",
        "            \"pair\": correct_total.index,\n",
        "            \"correct_total\": correct_total.values,\n",
        "            \"total_items\": total_items,\n",
        "            \"overall_success\": overall_succ.values\n",
        "          })\n",
        "          .sort_values([\"overall_success\",\"correct_total\"], ascending=[False, False])\n",
        "          .reset_index(drop=True))\n",
        "    per_dataset_rows.append(tmp)\n",
        "\n",
        "rank_per_dataset = pd.concat(per_dataset_rows, ignore_index=True)\n",
        "\n",
        "# Separar model / strategy para lectura c√≥moda\n",
        "rank_per_dataset[[\"model\",\"strategy\"]] = rank_per_dataset[\"pair\"].str.split(\"_\", n=1, expand=True)\n",
        "\n",
        "# 4) Ranking global (sumando a trav√©s de datasets)\n",
        "global_rank = (rank_per_dataset\n",
        "               .groupby([\"pair\",\"model\",\"strategy\"], dropna=False)\n",
        "               .agg(total_correct=(\"correct_total\",\"sum\"),\n",
        "                    total_items=(\"total_items\",\"sum\"))\n",
        "               .reset_index())\n",
        "\n",
        "global_rank[\"global_overall_success\"] = global_rank[\"total_correct\"] / global_rank[\"total_items\"]\n",
        "\n",
        "global_rank = (global_rank\n",
        "               .sort_values([\"global_overall_success\",\"total_correct\",\"total_items\"],\n",
        "                             ascending=[False, False, False])\n",
        "               .reset_index(drop=True))\n",
        "\n",
        "# 5) Mostrar ‚Äúmejor combinaci√≥n‚Äù y top 5\n",
        "print(\"‚úÖ Ranking GLOBAL (top 5 por proporci√≥n de aciertos penalizando no-cobertura):\")\n",
        "print(global_rank.head(5)[[\"pair\",\"model\",\"strategy\",\"total_correct\",\"total_items\",\"global_overall_success\"]])\n",
        "\n",
        "best = global_rank.iloc[0]\n",
        "print(\"\\nüéØ Mejor combinaci√≥n GLOBAL:\")\n",
        "print(f\"  Par: {best['pair']}  |  Modelo: {best['model']}  |  Estrategia: {best['strategy']}\")\n",
        "print(f\"  Aciertos/Total: {int(best['total_correct'])}/{int(best['total_items'])}  \"\n",
        "      f\"(global_overall_success = {best['global_overall_success']:.4f})\")\n",
        "\n",
        "# 6) Guardar rankings a CSV\n",
        "rank_ds_path     = os.path.join(OUT_DIR, \"ranking_model_strategy_per_dataset_penalized.csv\")\n",
        "global_rank_path = os.path.join(OUT_DIR, \"ranking_model_strategy_global_penalized.csv\")\n",
        "\n",
        "rank_per_dataset.to_csv(rank_ds_path, index=False)\n",
        "global_rank.to_csv(global_rank_path, index=False)\n",
        "\n",
        "print(\"\\nüíæ Guardados:\")\n",
        "print(\"  -\", rank_ds_path)\n",
        "print(\"  -\", global_rank_path)\n",
        "\n",
        "# (Opcional) Mostrar ranking por dataset ordenado\n",
        "print(\"\\n=== üß© Ranking por DATASET (mejor ‚Üí peor en cada dataset) ===\")\n",
        "disp_cols = [\"dataset\",\"pair\",\"model\",\"strategy\",\"correct_total\",\"total_items\",\"overall_success\"]\n",
        "for ds in rank_per_dataset[\"dataset\"].unique():\n",
        "    print(f\"\\n[Dataset: {ds}]\")\n",
        "    print(rank_per_dataset[rank_per_dataset[\"dataset\"]==ds][disp_cols]\n",
        "          .sort_values(\"overall_success\", ascending=False)\n",
        "          .head(10)\n",
        "          .to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmvmFWjQRD8_",
        "outputId": "69c63c6e-cf77-4998-92af-15f4a7ded483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ranking GLOBAL (top 5 por proporci√≥n de aciertos penalizando no-cobertura):\n",
            "                                   pair                   model  \\\n",
            "0            mathstral-7b_cot_reasoning            mathstral-7b   \n",
            "1     deepseek-math-7b_nonconv_zeroshot        deepseek-math-7b   \n",
            "2  qwen2-math-7b-instruct_cot_reasoning  qwen2-math-7b-instruct   \n",
            "3                      mathstral-7b_rag            mathstral-7b   \n",
            "4       deepseek-math-7b_nonconv_packed        deepseek-math-7b   \n",
            "\n",
            "           strategy  total_correct  total_items  global_overall_success  \n",
            "0     cot_reasoning            422          900                0.468889  \n",
            "1  nonconv_zeroshot            379          900                0.421111  \n",
            "2     cot_reasoning            379          900                0.421111  \n",
            "3               rag            377          900                0.418889  \n",
            "4    nonconv_packed            376          900                0.417778  \n",
            "\n",
            "üéØ Mejor combinaci√≥n GLOBAL:\n",
            "  Par: mathstral-7b_cot_reasoning  |  Modelo: mathstral-7b  |  Estrategia: cot_reasoning\n",
            "  Aciertos/Total: 422/900  (global_overall_success = 0.4689)\n",
            "\n",
            "üíæ Guardados:\n",
            "  - /content/drive/MyDrive/tesis/experiments_final/ranking_model_strategy_per_dataset_penalized.csv\n",
            "  - /content/drive/MyDrive/tesis/experiments_final/ranking_model_strategy_global_penalized.csv\n",
            "\n",
            "=== üß© Ranking por DATASET (mejor ‚Üí peor en cada dataset) ===\n",
            "\n",
            "[Dataset: benchmark]\n",
            "  dataset                                 pair                  model         strategy  correct_total  total_items  overall_success\n",
            "benchmark           mathstral-7b_cot_reasoning           mathstral-7b    cot_reasoning            422          900         0.468889\n",
            "benchmark    deepseek-math-7b_nonconv_zeroshot       deepseek-math-7b nonconv_zeroshot            379          900         0.421111\n",
            "benchmark qwen2-math-7b-instruct_cot_reasoning qwen2-math-7b-instruct    cot_reasoning            379          900         0.421111\n",
            "benchmark                     mathstral-7b_rag           mathstral-7b              rag            377          900         0.418889\n",
            "benchmark      deepseek-math-7b_nonconv_packed       deepseek-math-7b   nonconv_packed            376          900         0.417778\n",
            "benchmark               mathstral-7b_zero_shot           mathstral-7b        zero_shot            365          900         0.405556\n",
            "benchmark               mathstral-7b_few_shots           mathstral-7b        few_shots            364          900         0.404444\n",
            "benchmark        mathstral-7b_nonconv_zeroshot           mathstral-7b nonconv_zeroshot            355          900         0.394444\n",
            "benchmark     qwen2-math-7b-instruct_zero_shot qwen2-math-7b-instruct        zero_shot            352          900         0.391111\n",
            "benchmark     qwen2-math-7b-instruct_few_shots qwen2-math-7b-instruct        few_shots            347          900         0.385556\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4470c520841c4f11b401e08eaa6f99e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a12036e657a640d5925bc645ccf77922",
              "IPY_MODEL_40d7109feda54e839b2b2617438c06c0",
              "IPY_MODEL_27f5d9c762c846d19ce835e54d3591ba"
            ],
            "layout": "IPY_MODEL_142883a7ed0740909c419a0d9d422a82"
          }
        },
        "a12036e657a640d5925bc645ccf77922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab99de1ce8564259bc1bf7ac9227fc39",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ea73396f29d41abbbbd35d4aa89a3e7",
            "value": "Fetching‚Äá10‚Äáfiles:‚Äá100%"
          }
        },
        "40d7109feda54e839b2b2617438c06c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6695c903878141b7ab34e5dc1c1ee7d5",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8928c86e7e5425c916149b4c9fe7a6e",
            "value": 10
          }
        },
        "27f5d9c762c846d19ce835e54d3591ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27df96da9244898900d78b688b3ca33",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_61bd33bb31cb4e75bdcaa1cd573a5b9a",
            "value": "‚Äá10/10‚Äá[00:47&lt;00:00,‚Äá‚Äá7.68s/it]"
          }
        },
        "142883a7ed0740909c419a0d9d422a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab99de1ce8564259bc1bf7ac9227fc39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea73396f29d41abbbbd35d4aa89a3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6695c903878141b7ab34e5dc1c1ee7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8928c86e7e5425c916149b4c9fe7a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c27df96da9244898900d78b688b3ca33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61bd33bb31cb4e75bdcaa1cd573a5b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb89d424d93849449b4445cc3b261994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef3cd4ef3bba4189b9da8a160107b61d",
              "IPY_MODEL_9130bf8144d34b6b931e94c6f4c2760d",
              "IPY_MODEL_343ba10c117d496d8c08b210d08c4d38"
            ],
            "layout": "IPY_MODEL_5e1bc93a703e4c9d80b49042ca67e40a"
          }
        },
        "ef3cd4ef3bba4189b9da8a160107b61d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d72b2d65b2fd41178493453ee1a1d68e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e23503dd0b9c42d0bab65ddf0769959e",
            "value": "pytorch_model.bin.index.json:‚Äá"
          }
        },
        "9130bf8144d34b6b931e94c6f4c2760d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b524aafdbd6409c80edf591c88ff068",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b47a6508da1a4da9ba0802fc23f27a89",
            "value": 1
          }
        },
        "343ba10c117d496d8c08b210d08c4d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc28576ae5fe4028bb1673548913c610",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e141693ba4194fe6b2049a464c941f95",
            "value": "‚Äá22.5k/?‚Äá[00:00&lt;00:00,‚Äá2.66MB/s]"
          }
        },
        "5e1bc93a703e4c9d80b49042ca67e40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d72b2d65b2fd41178493453ee1a1d68e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e23503dd0b9c42d0bab65ddf0769959e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b524aafdbd6409c80edf591c88ff068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b47a6508da1a4da9ba0802fc23f27a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc28576ae5fe4028bb1673548913c610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e141693ba4194fe6b2049a464c941f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f726160e6414694b6b265b476db7aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41d2b317d64340e88c07128511741ec3",
              "IPY_MODEL_64c213339d954921b91f9ab252d4be7e",
              "IPY_MODEL_a96f078cb8c141369fc74ca1329ff45c"
            ],
            "layout": "IPY_MODEL_3383aab391ca4f9d9ff281f217795fa5"
          }
        },
        "41d2b317d64340e88c07128511741ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d09be0becde42d5b8e9a3f9d88c8ff6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2b58bc0754cd487e8b57767c28e5524c",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "64c213339d954921b91f9ab252d4be7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c1db08e3d4d41088da502e03bbba6ce",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3fce9a7e1874a889a45ac5abe84735a",
            "value": 1
          }
        },
        "a96f078cb8c141369fc74ca1329ff45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fafee8c1a42465b9360fd017eb44cc7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7c9e2ccc195e45a18497ab189b533bc9",
            "value": "‚Äá4.61M/?‚Äá[00:00&lt;00:00,‚Äá6.23MB/s]"
          }
        },
        "3383aab391ca4f9d9ff281f217795fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d09be0becde42d5b8e9a3f9d88c8ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b58bc0754cd487e8b57767c28e5524c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c1db08e3d4d41088da502e03bbba6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f3fce9a7e1874a889a45ac5abe84735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fafee8c1a42465b9360fd017eb44cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9e2ccc195e45a18497ab189b533bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "239b885e263d428296af794004283b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acd677ebc829491abb66d60ae25f069c",
              "IPY_MODEL_e751e61216c14b819d2052073b33889c",
              "IPY_MODEL_bc305dda9db24179bae18c8ba317dfd3"
            ],
            "layout": "IPY_MODEL_114f2708ced44fdabf0ff51dd8f3dc03"
          }
        },
        "acd677ebc829491abb66d60ae25f069c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ba8fa6e3ba14465bbf4a29adc0aac11",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_28557f0f3aef44e6b3061c49db419ea7",
            "value": "tokenizer_config.json:‚Äá"
          }
        },
        "e751e61216c14b819d2052073b33889c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e785db7117484a5fa9e351ca1a9096eb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89ea13f87f01400e8897c68818dd57c1",
            "value": 1
          }
        },
        "bc305dda9db24179bae18c8ba317dfd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a53a7d2637bd4ff5819986e21e8d7026",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_63741ecf1de64c63a471007678bc45ea",
            "value": "‚Äá1.14k/?‚Äá[00:00&lt;00:00,‚Äá136kB/s]"
          }
        },
        "114f2708ced44fdabf0ff51dd8f3dc03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba8fa6e3ba14465bbf4a29adc0aac11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28557f0f3aef44e6b3061c49db419ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e785db7117484a5fa9e351ca1a9096eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "89ea13f87f01400e8897c68818dd57c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a53a7d2637bd4ff5819986e21e8d7026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63741ecf1de64c63a471007678bc45ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfc742aa581841048ec39dd4a7ccdb59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6232623c943a4dd69c155db7d8f0adeb",
              "IPY_MODEL_942a8ff5d7ef440d9cb74a02d78d13a8",
              "IPY_MODEL_96a855917d4c4064bdc0d98360e8ee4a"
            ],
            "layout": "IPY_MODEL_3cdccdf791be4d9aaf32f3e452215e88"
          }
        },
        "6232623c943a4dd69c155db7d8f0adeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2f9c693afc415a836a85a2c9996573",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd9aeee92cc74b3691a2c8810ba1fffd",
            "value": "pytorch_model-00001-of-00002.bin:‚Äá100%"
          }
        },
        "942a8ff5d7ef440d9cb74a02d78d13a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb50be6a64524245b712cd78687cd8f7",
            "max": 9968194327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edbad2a771a84f54b2e04cb741b7ef97",
            "value": 9968194327
          }
        },
        "96a855917d4c4064bdc0d98360e8ee4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a093daee5c941be825d5ce2f5a6aaa3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f358b05b88474b0fa57edadcacc32220",
            "value": "‚Äá9.97G/9.97G‚Äá[00:36&lt;00:00,‚Äá405MB/s]"
          }
        },
        "3cdccdf791be4d9aaf32f3e452215e88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d2f9c693afc415a836a85a2c9996573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd9aeee92cc74b3691a2c8810ba1fffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb50be6a64524245b712cd78687cd8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edbad2a771a84f54b2e04cb741b7ef97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a093daee5c941be825d5ce2f5a6aaa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f358b05b88474b0fa57edadcacc32220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754c5e344ee84331a53e3770b1e97d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bc35e7ed9a044398cc7b30d9555af84",
              "IPY_MODEL_e00f250101d4418e996c173330b1c56c",
              "IPY_MODEL_b880a18e94e24d39ac80219e6d50c458"
            ],
            "layout": "IPY_MODEL_246a2b6d6a2c4781a6a0cf0609c04fc2"
          }
        },
        "5bc35e7ed9a044398cc7b30d9555af84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_185274f7413e4ee6abf281bfb7f9ace7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_53d7760b9d184aacb70639b4eaa2e5cf",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "e00f250101d4418e996c173330b1c56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70bd5e72c69440019cec34c8792aac26",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92a520031675470fb5eb5a763a967a48",
            "value": 121
          }
        },
        "b880a18e94e24d39ac80219e6d50c458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174304e888304d69b822613796304ed0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_90c9135bc54745c8ad284454757cd906",
            "value": "‚Äá121/121‚Äá[00:00&lt;00:00,‚Äá16.3kB/s]"
          }
        },
        "246a2b6d6a2c4781a6a0cf0609c04fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185274f7413e4ee6abf281bfb7f9ace7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d7760b9d184aacb70639b4eaa2e5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70bd5e72c69440019cec34c8792aac26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a520031675470fb5eb5a763a967a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "174304e888304d69b822613796304ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c9135bc54745c8ad284454757cd906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7de65df1a164e75a3c08bf7363a8bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ee3944651af4a0c955ceee3ab19d4d7",
              "IPY_MODEL_60bf3ab1764c46cd9206f77beab561cf",
              "IPY_MODEL_c75b7544e71c4d1898dd3e9c00b68796"
            ],
            "layout": "IPY_MODEL_41fb45f87c1848f8b4756e87787ab00e"
          }
        },
        "1ee3944651af4a0c955ceee3ab19d4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9acc2369484d0d9c4755ca125912fd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6f3347ae1df74cf19a2d2ccfa3462270",
            "value": "README.md:‚Äá"
          }
        },
        "60bf3ab1764c46cd9206f77beab561cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7552d84e6ea34958b642e817bd72bc2e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d095602a2e39425d9c68def7bdf3febc",
            "value": 1
          }
        },
        "c75b7544e71c4d1898dd3e9c00b68796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb301f3b1da46a091b8a4bca284a86b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b77dd04e4a064376aedbea13d372fed9",
            "value": "‚Äá3.23k/?‚Äá[00:00&lt;00:00,‚Äá316kB/s]"
          }
        },
        "41fb45f87c1848f8b4756e87787ab00e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9acc2369484d0d9c4755ca125912fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3347ae1df74cf19a2d2ccfa3462270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7552d84e6ea34958b642e817bd72bc2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d095602a2e39425d9c68def7bdf3febc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fb301f3b1da46a091b8a4bca284a86b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b77dd04e4a064376aedbea13d372fed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd45523fb0954bba8132463cedb799ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc044d9b3e7e4f4d957baca8f26801ab",
              "IPY_MODEL_ea7c3f362e33461395adefdc4d056fb0",
              "IPY_MODEL_207a53ca98444f50ad339871aa1f449e"
            ],
            "layout": "IPY_MODEL_1f5f8671c82e4ec38046f5326b20a829"
          }
        },
        "bc044d9b3e7e4f4d957baca8f26801ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eed6f54efb94477aea11c91b5f0a683",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_041f42a6446843fba16095917254f01c",
            "value": "LICENSE:‚Äá"
          }
        },
        "ea7c3f362e33461395adefdc4d056fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_691f5a1b2e344e118b5a7fad4974fe86",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d874d17e5c354109882b686f901addc7",
            "value": 1
          }
        },
        "207a53ca98444f50ad339871aa1f449e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d943d13d6d14c47bb6b8a5be325a715",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fcc072a04e394abb9bfcf9059f826535",
            "value": "‚Äá13.8k/?‚Äá[00:00&lt;00:00,‚Äá1.74MB/s]"
          }
        },
        "1f5f8671c82e4ec38046f5326b20a829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eed6f54efb94477aea11c91b5f0a683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "041f42a6446843fba16095917254f01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "691f5a1b2e344e118b5a7fad4974fe86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d874d17e5c354109882b686f901addc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d943d13d6d14c47bb6b8a5be325a715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc072a04e394abb9bfcf9059f826535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b36068e2a5a14d14887d0c6a54a54a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cecbc8e21f9e4293a822f8bcabe33515",
              "IPY_MODEL_64f7375d9d4a41d780424dbbed990442",
              "IPY_MODEL_86530f10792d41b596ef21de6c8a4035"
            ],
            "layout": "IPY_MODEL_508045b8697f41ae865f3a96dcef5327"
          }
        },
        "cecbc8e21f9e4293a822f8bcabe33515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6944042150b74bed86671f0c3bb64af5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6ddaa48f8b5a4cb19a4542a66917cc5e",
            "value": "config.json:‚Äá100%"
          }
        },
        "64f7375d9d4a41d780424dbbed990442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a96c92fc5b54931b4f51633fa85dc12",
            "max": 594,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_203c74e46ada498c91815de17322f7ca",
            "value": 594
          }
        },
        "86530f10792d41b596ef21de6c8a4035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aea09316613428e9b480eba2b365793",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d8cad43f7d543c286c67ba05076d49f",
            "value": "‚Äá594/594‚Äá[00:00&lt;00:00,‚Äá75.2kB/s]"
          }
        },
        "508045b8697f41ae865f3a96dcef5327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6944042150b74bed86671f0c3bb64af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ddaa48f8b5a4cb19a4542a66917cc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a96c92fc5b54931b4f51633fa85dc12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203c74e46ada498c91815de17322f7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aea09316613428e9b480eba2b365793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8cad43f7d543c286c67ba05076d49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "262ab786dd754ac2b84b62fc78ef69ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54833a9af9824dedbac65571846b6b21",
              "IPY_MODEL_9f78f306323a4a7b9f0ce0c60e364ec7",
              "IPY_MODEL_418a86013da24ea59523b37e6a9a2f86"
            ],
            "layout": "IPY_MODEL_0f7d6b23c24347f1a823f4df29aa70ba"
          }
        },
        "54833a9af9824dedbac65571846b6b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_039d91620b94464ea8ef00a9b7d0247d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f79e5d51f06d4c01a716b1cb38ef9f27",
            "value": "pytorch_model-00002-of-00002.bin:‚Äá100%"
          }
        },
        "9f78f306323a4a7b9f0ce0c60e364ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec4099180e1445b9ae7a4bde896f971c",
            "max": 3852630633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48d8e999f2034392ba6611f62861ceee",
            "value": 3852630633
          }
        },
        "418a86013da24ea59523b37e6a9a2f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c49c169f67b45da94afec4c5c1693e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_56d395cd612d400d823430ccefede778",
            "value": "‚Äá3.85G/3.85G‚Äá[00:26&lt;00:00,‚Äá43.5MB/s]"
          }
        },
        "0f7d6b23c24347f1a823f4df29aa70ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "039d91620b94464ea8ef00a9b7d0247d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79e5d51f06d4c01a716b1cb38ef9f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec4099180e1445b9ae7a4bde896f971c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d8e999f2034392ba6611f62861ceee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c49c169f67b45da94afec4c5c1693e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d395cd612d400d823430ccefede778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "309048674d974dd380dfd6dae6846b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2a82bf4db914a91b76d6f58caa76595",
              "IPY_MODEL_1661cb5b003d42268c1c52feed1a55c9",
              "IPY_MODEL_5502a09de5f34572b7ae0a59e0da19f5"
            ],
            "layout": "IPY_MODEL_df9c512ca16d420e9c0f28cbd0df8e61"
          }
        },
        "e2a82bf4db914a91b76d6f58caa76595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29d48719faba4b9eb62c7ff7c20711ec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2d535b0da66e4381a2269652143ea69e",
            "value": ".gitattributes:‚Äá"
          }
        },
        "1661cb5b003d42268c1c52feed1a55c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89ce1ddfd48b4572bd03b9ff17b52dd2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c43c6cad52924975919ff814aae650aa",
            "value": 1
          }
        },
        "5502a09de5f34572b7ae0a59e0da19f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628a848ccc79463a85d34a176db69a5f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2e0008d4d90438a8f3706e166baa854",
            "value": "‚Äá1.52k/?‚Äá[00:00&lt;00:00,‚Äá87.2kB/s]"
          }
        },
        "df9c512ca16d420e9c0f28cbd0df8e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d48719faba4b9eb62c7ff7c20711ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d535b0da66e4381a2269652143ea69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89ce1ddfd48b4572bd03b9ff17b52dd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c43c6cad52924975919ff814aae650aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "628a848ccc79463a85d34a176db69a5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e0008d4d90438a8f3706e166baa854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}